<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//unpkg.com/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//unpkg.com/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="前言记录一些自己整理的面经。">
<meta property="og:type" content="article">
<meta property="og:title" content="面经">
<meta property="og:url" content="http://example.com/2023/03/06/%E9%9D%A2%E7%BB%8F/index.html">
<meta property="og:site_name" content="木霈玖的博客">
<meta property="og:description" content="前言记录一些自己整理的面经。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231023271.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231029120.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231122873.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231355906.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231127826.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231137928.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308221330259.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202309052221203.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308261842588.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222347197.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222349037.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211132940.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211055521.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211059473.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211251919.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308101255449.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101955821.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102004442.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101950246.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102008358.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102010115.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102019772.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071555276.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071559636.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306191056073.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306191111890.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306191113911.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071107979.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304211122841.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304211122760.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071138731.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101613543.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101613198.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101650959.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101651061.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101656177.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101658315.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101704500.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101704670.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101716405.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101727004.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101743580.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101744292.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101744829.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101746573.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102112494.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102149708.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102152516.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102156269.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102208969.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102216271.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111538347.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111601603.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111619137.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071255957.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071300267.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20230106231925.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303212301932.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222128650.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222129537.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317819.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317983.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306151052109.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222332494.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071328562.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071332166.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071333924.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071335563.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071336003.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339814.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339457.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071340668.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071341013.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071342703.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071343619.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071356215.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101147762.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101148082.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101504342.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101504156.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101539655.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101545859.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101553212.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302227637.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302042116.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302053360.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302112845.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302133753.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302134012.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302137784.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302138098.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302139598.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302156146.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306151023478.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231436850.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310947858.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310948367.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310952548.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311001584.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311002558.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311016054.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311035374.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311039224.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311047793.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311102674.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101646509.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101836412.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101836048.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101738369.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101841213.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202307131454285.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202307131152042.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202307131153000.png">
<meta property="article:published_time" content="2023-03-06T14:04:43.000Z">
<meta property="article:modified_time" content="2024-11-17T08:48:36.847Z">
<meta property="article:author" content="木霈玖">
<meta property="article:tag" content="总结">
<meta property="article:tag" content="面试">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231023271.png">


<link rel="canonical" href="http://example.com/2023/03/06/%E9%9D%A2%E7%BB%8F/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>面经 | 木霈玖的博客</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">木霈玖的博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E6%A8%A1%E5%9E%8B%E4%B8%8D%E6%94%B6%E6%95%9B%EF%BC%8C%E6%98%AF%E5%90%A6%E8%AF%B4%E6%98%8E%E8%BF%99%E4%B8%AA%E6%A8%A1%E5%9E%8B%E6%97%A0%E6%95%88%EF%BC%8C%E8%87%B4%E6%A8%A1%E5%9E%8B%E4%B8%8D%E6%94%B6%E6%95%9B%E7%9A%84%E5%8E%9F%E5%9B%A0%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="nav-number">2.</span> <span class="nav-text">训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">3.</span> <span class="nav-text">特征归一化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8A%A0%E5%BF%AB%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%80%9F%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">4.</span> <span class="nav-text">加快模型训练速度的方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86%E3%80%81%E6%B5%8B%E8%AF%95%E9%9B%86%E3%80%81%E9%AA%8C%E8%AF%81%E9%9B%86%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="nav-number">5.</span> <span class="nav-text">训练集、测试集、验证集的作用</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%92%8C%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1"><span class="nav-number">6.</span> <span class="nav-text">极大似然估计和贝叶斯估计</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="nav-number">6.1.</span> <span class="nav-text">极大似然估计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1"><span class="nav-number">6.2.</span> <span class="nav-text">贝叶斯估计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1"><span class="nav-number">6.2.1.</span> <span class="nav-text">最大后验估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C"><span class="nav-number">6.2.2.</span> <span class="nav-text">共轭先验</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Zero-Shot-One-Shot-Few-Shot"><span class="nav-number">7.</span> <span class="nav-text">Zero-Shot,One-Shot,Few-Shot</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PyTorch%E5%92%8CTensorFlow%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">8.</span> <span class="nav-text">PyTorch和TensorFlow的区别</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%9B%BE%E4%B8%8E%E9%9D%99%E6%80%81%E5%9B%BE"><span class="nav-number">8.1.</span> <span class="nav-text">动态图与静态图</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2%E5%92%8C%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7"><span class="nav-number">8.2.</span> <span class="nav-text">部署和可扩展性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B5%84%E6%BA%90%E4%BC%98%E5%8C%96%E5%92%8C%E5%88%A9%E7%94%A8%E7%8E%87"><span class="nav-number">8.3.</span> <span class="nav-text">资源优化和利用率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E6%9C%AF%E7%A0%94%E7%A9%B6%E5%92%8C%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81"><span class="nav-number">8.4.</span> <span class="nav-text">学术研究和开源代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Pytorch%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86"><span class="nav-number">9.</span> <span class="nav-text">Pytorch底层原理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="nav-number">9.1.</span> <span class="nav-text">自动微分的实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Pytorch%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83"><span class="nav-number">10.</span> <span class="nav-text">Pytorch多卡训练</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DataLoader-DataSet-Sampler%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-number">11.</span> <span class="nav-text">DataLoader, DataSet, Sampler之间的关系</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83"><span class="nav-number">12.</span> <span class="nav-text">混合精度训练</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#FP16"><span class="nav-number">12.1.</span> <span class="nav-text">FP16</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8FP16"><span class="nav-number">12.2.</span> <span class="nav-text">为什么要用FP16</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AA%E7%94%A8FP16%E4%BC%9A%E6%9C%89%E9%97%AE%E9%A2%98"><span class="nav-number">12.3.</span> <span class="nav-text">为什么只用FP16会有问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95"><span class="nav-number">12.4.</span> <span class="nav-text">解决办法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">13.</span> <span class="nav-text">提升模型泛化能力的方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="nav-number">14.</span> <span class="nav-text">偏差和方差</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">15.</span> <span class="nav-text">过拟合和欠拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5"><span class="nav-number">15.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">15.2.</span> <span class="nav-text">过拟合的解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#L1%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8CL2%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">15.2.1.</span> <span class="nav-text">L1正则化和L2正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#L1%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">15.2.1.1.</span> <span class="nav-text">L1正则化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L2%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">15.2.1.2.</span> <span class="nav-text">L2正则化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">15.3.</span> <span class="nav-text">欠拟合的解决方案</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="nav-number">16.</span> <span class="nav-text">梯度消失和梯度爆炸</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5-1"><span class="nav-number">16.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">16.2.</span> <span class="nav-text">解决方案</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">17.</span> <span class="nav-text">解决样本不均衡的方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LabelSmoothing"><span class="nav-number">18.</span> <span class="nav-text">LabelSmoothing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%AD%E5%B9%B3%E6%BB%91%E5%92%8C%E9%94%90%E5%8C%96%E6%93%8D%E4%BD%9C%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">19.</span> <span class="nav-text">图像处理中平滑和锐化操作是什么？</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5-2"><span class="nav-number">19.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">19.2.</span> <span class="nav-text">使用场景</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E4%B8%AD%E7%9A%84%E4%BD%8E%E9%A2%91%E4%BF%A1%E5%8F%B7%E5%92%8C%E9%AB%98%E9%A2%91%E4%BF%A1%E5%8F%B7"><span class="nav-number">20.</span> <span class="nav-text">图像中的低频信号和高频信号</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#batchsize"><span class="nav-number">21.</span> <span class="nav-text">batchsize</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">22.</span> <span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Huber-Loss"><span class="nav-number">22.1.</span> <span class="nav-text">Huber Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Smooth-L1-Loss"><span class="nav-number">22.2.</span> <span class="nav-text">Smooth L1 Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cross-Entropy-Loss"><span class="nav-number">22.3.</span> <span class="nav-text">Cross Entropy Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Binary-Cross-Entropy-Loss"><span class="nav-number">22.4.</span> <span class="nav-text">Binary Cross Entropy Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Focal-loss"><span class="nav-number">22.5.</span> <span class="nav-text">Focal loss</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">23.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sigmoid"><span class="nav-number">23.1.</span> <span class="nav-text">Sigmoid</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tanh"><span class="nav-number">23.2.</span> <span class="nav-text">tanh</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ReLU"><span class="nav-number">23.3.</span> <span class="nav-text">ReLU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Leaky-ReLU"><span class="nav-number">23.4.</span> <span class="nav-text">Leaky ReLU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ELU"><span class="nav-number">23.5.</span> <span class="nav-text">ELU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GELU"><span class="nav-number">23.6.</span> <span class="nav-text">GELU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ReLU%E6%AF%94Sigmoid%E6%95%88%E6%9E%9C%E5%A5%BD%E5%9C%A8%E5%93%AA%E9%87%8C%EF%BC%9F"><span class="nav-number">23.7.</span> <span class="nav-text">ReLU比Sigmoid效果好在哪里？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="nav-number">24.</span> <span class="nav-text">优化器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SGD"><span class="nav-number">24.1.</span> <span class="nav-text">SGD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SGDM"><span class="nav-number">24.2.</span> <span class="nav-text">SGDM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adagrad"><span class="nav-number">24.3.</span> <span class="nav-text">Adagrad</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RMSProp"><span class="nav-number">24.4.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adam"><span class="nav-number">24.5.</span> <span class="nav-text">Adam</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="nav-number">25.</span> <span class="nav-text">评价指标</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Top-1-Accuracy%E5%92%8CTop-5-Accuracy"><span class="nav-number">25.1.</span> <span class="nav-text">Top-1 Accuracy和Top-5 Accuracy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DICE"><span class="nav-number">25.2.</span> <span class="nav-text">DICE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IoU"><span class="nav-number">25.3.</span> <span class="nav-text">IoU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MIoU"><span class="nav-number">25.4.</span> <span class="nav-text">MIoU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AP"><span class="nav-number">25.5.</span> <span class="nav-text">AP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mAP"><span class="nav-number">25.6.</span> <span class="nav-text">mAP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Box-AP"><span class="nav-number">25.7.</span> <span class="nav-text">Box AP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mask-AP"><span class="nav-number">25.8.</span> <span class="nav-text">Mask AP</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="nav-number">26.</span> <span class="nav-text">集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Bagging"><span class="nav-number">26.1.</span> <span class="nav-text">Bagging</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="nav-number">26.1.1.</span> <span class="nav-text">随机森林</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">26.1.1.1.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E7%9A%84%E7%BB%88%E6%AD%A2%E6%9D%A1%E4%BB%B6"><span class="nav-number">26.1.1.1.1.</span> <span class="nav-text">决策树构建的终止条件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%89%AA%E6%9E%9D"><span class="nav-number">26.1.1.1.2.</span> <span class="nav-text">决策树剪枝</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95"><span class="nav-number">26.1.1.1.3.</span> <span class="nav-text">决策树生成算法</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Boosting"><span class="nav-number">26.2.</span> <span class="nav-text">Boosting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaBoost"><span class="nav-number">26.2.1.</span> <span class="nav-text">AdaBoost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT"><span class="nav-number">26.2.2.</span> <span class="nav-text">GBDT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95"><span class="nav-number">26.2.2.1.</span> <span class="nav-text">提升树算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient-Boosting-Decision-Tree%EF%BC%88%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%89"><span class="nav-number">26.2.2.2.</span> <span class="nav-text">Gradient Boosting Decision Tree（梯度提升决策树）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#XGBoost"><span class="nav-number">26.2.3.</span> <span class="nav-text">XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="nav-number">26.2.3.1.</span> <span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F%E5%B1%95%E5%BC%80"><span class="nav-number">26.2.3.2.</span> <span class="nav-text">泰勒公式展开</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%91%E7%9A%84%E5%8F%82%E6%95%B0%E5%8C%96"><span class="nav-number">26.2.3.3.</span> <span class="nav-text">树的参数化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%88%86%E8%A3%82"><span class="nav-number">26.2.3.4.</span> <span class="nav-text">特征分裂</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LightGBM"><span class="nav-number">26.2.4.</span> <span class="nav-text">LightGBM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B4%E6%96%B9%E5%9B%BEHistogram%E7%AE%97%E6%B3%95"><span class="nav-number">26.2.4.1.</span> <span class="nav-text">直方图Histogram算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%A6%E6%B7%B1%E5%BA%A6%E9%99%90%E5%88%B6%E7%9A%84Leaf-wise%E7%9A%84%E5%8F%B6%E5%AD%90%E7%94%9F%E9%95%BF%E7%AD%96%E7%95%A5"><span class="nav-number">26.2.4.2.</span> <span class="nav-text">带深度限制的Leaf-wise的叶子生长策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E7%B1%BB%E5%88%AB%E7%89%B9%E5%BE%81"><span class="nav-number">26.2.4.3.</span> <span class="nav-text">支持类别特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%8D%95%E8%BE%B9%E9%87%87%E6%A0%B7-GOSS"><span class="nav-number">26.2.4.4.</span> <span class="nav-text">基于梯度的单边采样(GOSS)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%92%E6%96%A5%E7%89%B9%E5%BE%81%E7%BB%91%E5%AE%9A"><span class="nav-number">26.2.4.5.</span> <span class="nav-text">互斥特征绑定</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CatBoost"><span class="nav-number">26.2.5.</span> <span class="nav-text">CatBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%B1%BB%E5%88%AB%E7%89%B9%E5%BE%81%E7%9A%84Ordered-Target-Statistics%E6%95%B0%E5%80%BC%E7%BC%96%E7%A0%81%E6%96%B9%E6%B3%95"><span class="nav-number">26.2.5.1.</span> <span class="nav-text">基于类别特征的Ordered Target Statistics数值编码方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E8%B4%AA%E5%BF%83%E7%AD%96%E7%95%A5%E7%9A%84%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89%E6%96%B9%E6%B3%95"><span class="nav-number">26.2.5.2.</span> <span class="nav-text">基于贪心策略的特征交叉方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%81%BF%E5%85%8D%E9%A2%84%E6%B5%8B%E5%81%8F%E7%A7%BB%E7%9A%84Ordered-Boosting%E6%96%B9%E6%B3%95"><span class="nav-number">26.2.5.3.</span> <span class="nav-text">避免预测偏移的Ordered Boosting方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91%E4%BD%9C%E4%B8%BA%E5%9F%BA%E6%A8%A1%E5%9E%8B"><span class="nav-number">26.2.5.4.</span> <span class="nav-text">使用对称二叉树作为基模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stacking"><span class="nav-number">26.3.</span> <span class="nav-text">Stacking</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LR"><span class="nav-number">27.</span> <span class="nav-text">LR</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8sigmoid%E5%87%BD%E6%95%B0"><span class="nav-number">27.1.</span> <span class="nav-text">逻辑回归为什么使用sigmoid函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SVM"><span class="nav-number">28.</span> <span class="nav-text">SVM</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CNN"><span class="nav-number">29.</span> <span class="nav-text">CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%B0%BA%E5%BA%A6"><span class="nav-number">29.1.</span> <span class="nav-text">多尺度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">29.2.</span> <span class="nav-text">卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%99%AE%E9%80%9A%E5%8D%B7%E7%A7%AF"><span class="nav-number">29.2.1.</span> <span class="nav-text">普通卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF-group-convolution"><span class="nav-number">29.2.2.</span> <span class="nav-text">分组卷积(group convolution)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%BB%93%E6%9E%9C"><span class="nav-number">29.2.3.</span> <span class="nav-text">卷积结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">29.3.</span> <span class="nav-text">池化层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#padding"><span class="nav-number">29.4.</span> <span class="nav-text">padding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN%E4%B8%AD%E7%9A%84%E7%AD%89%E5%8F%98%E5%92%8C%E4%B8%8D%E5%8F%98"><span class="nav-number">29.5.</span> <span class="nav-text">CNN中的等变和不变</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%89%E5%8F%98%E6%80%A7"><span class="nav-number">29.5.1.</span> <span class="nav-text">等变性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E5%8F%98%E6%80%A7"><span class="nav-number">29.5.2.</span> <span class="nav-text">不变性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E6%9D%83%E5%80%BC%E5%85%B1%E4%BA%AB%E7%9A%84%E7%90%86%E8%A7%A3%EF%BC%9F"><span class="nav-number">29.6.</span> <span class="nav-text">神经网络中权值共享的理解？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E5%BE%AE%E8%B0%83-fine-tuning-%E7%9A%84%E7%90%86%E8%A7%A3%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BF%AE%E6%94%B9%E6%9C%80%E5%90%8E%E5%87%A0%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E5%80%BC%EF%BC%9F"><span class="nav-number">29.7.</span> <span class="nav-text">对微调(fine-tuning)的理解，为什么要修改最后几层神经网络权值？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LeNet-5"><span class="nav-number">29.8.</span> <span class="nav-text">LeNet-5</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AlexNet"><span class="nav-number">29.9.</span> <span class="nav-text">AlexNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout"><span class="nav-number">29.9.1.</span> <span class="nav-text">Dropout</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4Dropout%E5%8F%AF%E4%BB%A5%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="nav-number">29.9.1.1.</span> <span class="nav-text">为什么说Dropout可以解决过拟合？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dropout%E7%BC%BA%E7%82%B9"><span class="nav-number">29.9.1.2.</span> <span class="nav-text">Dropout缺点</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VGG"><span class="nav-number">29.10.</span> <span class="nav-text">VGG</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GoogLeNet"><span class="nav-number">29.11.</span> <span class="nav-text">GoogLeNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ResNet"><span class="nav-number">29.12.</span> <span class="nav-text">ResNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%AE%E7%82%B9"><span class="nav-number">29.12.1.</span> <span class="nav-text">ResNet中的一些亮点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%87%E7%94%A8residual"><span class="nav-number">29.12.2.</span> <span class="nav-text">为什么采用residual?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#residual%E7%BB%93%E6%9E%84"><span class="nav-number">29.12.3.</span> <span class="nav-text">residual结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#residual%E7%9A%84%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F"><span class="nav-number">29.12.3.1.</span> <span class="nav-text">residual的计算方式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ResNet%E4%B8%AD%E4%B8%A4%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84residual"><span class="nav-number">29.12.3.2.</span> <span class="nav-text">ResNet中两种不同的residual</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BatchNormalization"><span class="nav-number">29.12.4.</span> <span class="nav-text">BatchNormalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LayerNormalization"><span class="nav-number">29.12.5.</span> <span class="nav-text">LayerNormalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%94%A8Dropout%EF%BC%9F"><span class="nav-number">29.12.6.</span> <span class="nav-text">ResNet为什么不用Dropout？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DenseNet"><span class="nav-number">29.13.</span> <span class="nav-text">DenseNet</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RNN"><span class="nav-number">30.</span> <span class="nav-text">RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-number">30.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81RNN%EF%BC%9F"><span class="nav-number">30.2.</span> <span class="nav-text">为什么需要RNN？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E4%B8%BB%E8%A6%81%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F"><span class="nav-number">30.3.</span> <span class="nav-text">RNN的主要应用领域</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="nav-number">30.4.</span> <span class="nav-text">RNN的计算过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E5%BB%BA%E6%A8%A1%E6%96%B9%E5%BC%8F"><span class="nav-number">30.5.</span> <span class="nav-text">RNN的建模方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E5%AF%B9%E5%A4%9A-vector-to-sequence"><span class="nav-number">30.5.1.</span> <span class="nav-text">一对多(vector-to-sequence)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%AF%B9%E4%B8%80-sequence-to-vector"><span class="nav-number">30.5.2.</span> <span class="nav-text">多对一(sequence-to-vector)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%AF%B9%E5%A4%9A-Encoder-Decoder"><span class="nav-number">30.5.3.</span> <span class="nav-text">多对多(Encoder-Decoder)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E4%B8%AD%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%87%BA%E7%8E%B0%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%EF%BC%9F%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="nav-number">30.6.</span> <span class="nav-text">RNN中为什么会出现梯度消失？如何解决？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">30.7.</span> <span class="nav-text">RNN的注意力机制</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LSTM"><span class="nav-number">31.</span> <span class="nav-text">LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9FRNN%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">31.1.</span> <span class="nav-text">传统RNN存在的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM-1"><span class="nav-number">31.2.</span> <span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E9%80%89%E5%8F%96"><span class="nav-number">31.3.</span> <span class="nav-text">LSTM的激活函数选取</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer"><span class="nav-number">32.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="nav-number">32.1.</span> <span class="nav-text">词嵌入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">32.2.</span> <span class="nav-text">注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-attention"><span class="nav-number">32.2.1.</span> <span class="nav-text">Self-attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cross-attention"><span class="nav-number">32.2.2.</span> <span class="nav-text">Cross-attention</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-Encoder"><span class="nav-number">32.3.</span> <span class="nav-text">编码器(Encoder)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E6%8B%A9%E9%99%A4%E4%BB%A5-sqrt-d"><span class="nav-number">32.3.1.</span> <span class="nav-text">为什么选择除以$\sqrt{d}$</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8-Decoder"><span class="nav-number">32.4.</span> <span class="nav-text">解码器(Decoder)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer%E4%B8%ADEncoder%E5%92%8CDecoder%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">32.5.</span> <span class="nav-text">Transformer中Encoder和Decoder的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BERT%E5%92%8CTransformer%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">32.6.</span> <span class="nav-text">BERT和Transformer的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BE%93%E5%87%BA"><span class="nav-number">32.7.</span> <span class="nav-text">输出</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-Positional-Encoding"><span class="nav-number">32.8.</span> <span class="nav-text">位置编码(Positional Encoding)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanilla-Transformer%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%84%E7%89%B9%E7%82%B9"><span class="nav-number">32.8.1.</span> <span class="nav-text">Vanilla Transformer的位置编码的特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E7%BC%96%E7%A0%81%E6%96%B9%E5%BC%8F"><span class="nav-number">32.8.2.</span> <span class="nav-text">其他编码方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanilla-Transformer%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%84%E7%BC%BA%E7%82%B9%E4%BB%A5%E5%8F%8A%E6%94%B9%E8%BF%9B"><span class="nav-number">32.8.3.</span> <span class="nav-text">Vanilla Transformer位置编码的缺点以及改进</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Vision-Transformer"><span class="nav-number">33.</span> <span class="nav-text">Vision Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%84%E6%88%90"><span class="nav-number">33.1.</span> <span class="nav-text">模型组成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Embedding%E5%B1%82%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3"><span class="nav-number">33.2.</span> <span class="nav-text">Embedding层结构详解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer-Encoder%E8%AF%A6%E8%A7%A3"><span class="nav-number">33.3.</span> <span class="nav-text">Transformer Encoder详解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MLP-Head%E8%AF%A6%E8%A7%A3"><span class="nav-number">33.4.</span> <span class="nav-text">MLP Head详解</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Swin-Transformer"><span class="nav-number">34.</span> <span class="nav-text">Swin Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84"><span class="nav-number">34.1.</span> <span class="nav-text">整体结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Patch-merging"><span class="nav-number">34.2.</span> <span class="nav-text">Patch merging</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#W-MSA"><span class="nav-number">34.3.</span> <span class="nav-text">W-MSA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SW-MSA"><span class="nav-number">34.4.</span> <span class="nav-text">SW-MSA</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">木霈玖</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/06/%E9%9D%A2%E7%BB%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="木霈玖">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="木霈玖的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          面经
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-03-06 22:04:43" itemprop="dateCreated datePublished" datetime="2023-03-06T22:04:43+08:00">2023-03-06</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2024-11-17 16:48:36" itemprop="dateModified" datetime="2024-11-17T16:48:36+08:00">2024-11-17</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E9%9D%A2%E8%AF%95/" itemprop="url" rel="index"><span itemprop="name">面试</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>48k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>43 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>记录一些自己整理的面经。</p>
<a id="more"></a>
<h1 id="训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些"><a href="#训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些" class="headerlink" title="训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些?"></a>训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些?</h1><p>在训练过程中，如果模型不收敛并不能说明该模型时无效的。</p>
<p>导致模型不收敛的原因包括：</p>
<ol>
<li>没有对数据做归一化处理。</li>
<li>没有使用正则化。</li>
<li><code>Batch Size</code>设的太大。</li>
<li>学习率设置的太大容易产生震荡，太小会导致不收敛。</li>
<li>没有做数据预处理。</li>
<li>没有检查过预处理结果和最终的训练测试结果。</li>
<li>网络存在坏梯度，比如当<code>ReLU</code>对负值的梯度为 $0$ ，反向传播时，梯度为 $0$ 表示不传播。</li>
<li>网络设定不合理，网络太浅或者太深。</li>
<li>最后一层的激活函数错误。</li>
<li>参数初始化错误。</li>
<li>隐藏层神经元数量错误。</li>
<li>数据集标签的设置有错误。</li>
</ol>
<h1 id="特征归一化"><a href="#特征归一化" class="headerlink" title="特征归一化"></a>特征归一化</h1><ul>
<li><p>训练数据集归一化很好理解，用于训练模型。</p>
</li>
<li><p>不能直接对测试数据集按公式进行归一化，而是要使用训练数据集的均值和方差对测试数据集归一化。</p>
<ul>
<li>原因1：真实的环境中，数据会源源不断输出进模型，无法求取均值和方差的。</li>
<li>原因2：训练数据集是模拟真实环境中的数据，不能直接使用自身的均值和方差。</li>
<li>原因3：真实环境中，无法对单个数据进行归一化。</li>
</ul>
</li>
</ul>
<h1 id="加快模型训练速度的方法"><a href="#加快模型训练速度的方法" class="headerlink" title="加快模型训练速度的方法"></a>加快模型训练速度的方法</h1><ul>
<li>合理的超参数设计</li>
<li>权值共享</li>
<li>升级相关软件包</li>
<li>多卡训练、数据并行</li>
<li>混合精度训练</li>
</ul>
<h1 id="训练集、测试集、验证集的作用"><a href="#训练集、测试集、验证集的作用" class="headerlink" title="训练集、测试集、验证集的作用"></a>训练集、测试集、验证集的作用</h1><p>训练集：训练集用来训练模型，即确定模型的权重和偏置这些参数，通常我们称这些参数为学习参数。</p>
<p>验证集：而验证集用于模型的选择，更具体地来说，验证集并不参与学习参数的确定，也就是验证集并没有参与梯度下降的过程。验证集只是为了选择超参数，比如网络层数、网络节点数、迭代次数、学习率这些都叫超参数。比如在k-NN算法中，k值就是一个超参数。所以可以使用验证集来求出误差率最小的k。</p>
<p>测试集：测试集只使用一次，即在训练完成后评价最终的模型时使用。它既不参与学习参数过程，也不参数超参数选择过程，而仅仅使用于模型的评价。值得注意的是，千万不能在训练过程中使用测试集，而后再用相同的测试集去测试模型。这样做其实是一个cheat，使得模型测试时准确率很高。</p>
<h1 id="极大似然估计和贝叶斯估计"><a href="#极大似然估计和贝叶斯估计" class="headerlink" title="极大似然估计和贝叶斯估计"></a>极大似然估计和贝叶斯估计</h1><p>极大似然估计（Maximum Likelihood Estimation，MLE）和贝叶斯估计（Bayesian Estimation）是统计推断中两种最常用的参数估计方法，二者在机器学习中的应用也十分广泛。</p>
<p>考虑这样一个问题：总体 $X$ 的概率密度函数为 $f(x\vert\theta)$，观测到一组样本 $(X_{1},X_{2},\cdots,X_{n})=(x_{1},x_{2},\cdots,x_{n})$，需要估计参数 $\theta$。下面我们将采用不同的估计方法来求解这个问题。</p>
<h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>极大似然估计是典型的频率学派观点，它的基本思想是：待估计参数 $\theta$ 是客观存在的，只是未知而已，当 $\hat{\theta}_{mle}$ 满足“ $\theta=\hat{\theta}_{mle}$ 时，该组观测样本 $(X_{1},X_{2},\cdots,X_{n})=(x_{1},x_{2},\cdots,x_{n})$ 更容易被观测到”，我们就说 $\hat{\theta}_{mle}$ 是 $\theta$ 的极大似然估计值。即估计值 $\hat{\theta}_{mle}$ 使得事件发生的可能性最大。下面给出极大似然估计的数学描述：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231023271.png" alt=""></p>
<h2 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h2><p>贝叶斯估计是典型的贝叶斯学派观点，它的基本思想是：待估计参数 $\theta$ 也是随机的，和一般随机变量没有本质区别，因此只能根据观测样本估计参数 $\theta$ 的分布。贝叶斯估计利用了贝叶斯公式，给出贝叶斯公式的数学描述：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231029120.png" alt=""></p>
<p>下面给出贝叶斯估计的数学描述：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231122873.png" alt=""></p>
<p>其中， $\pi(\theta)$ 为参数 $\theta$ 的先验分布（prior distribution），表示对参数 $\theta$ 的主观认识，是非样本信息， $\pi(\theta\vert x)$ 为参数 $\theta$ 的后验分布（posterior distribution）。因此，贝叶斯估计可以看作是，在假定 $\theta$ 服从 $\pi(\theta)$ 的先验分布前提下，根据样本信息去校正先验分布，得到后验分布 $\pi(\theta\vert x)$ 。由于后验分布是一个条件分布，通常我们取后验分布的期望作为参数的估计值。因此，在观测到n个样本之后，下一个数据样本的 $x_{n+1}$ 的预测分布如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231355906.png" alt=""></p>
<h3 id="最大后验估计"><a href="#最大后验估计" class="headerlink" title="最大后验估计"></a>最大后验估计</h3><p>在贝叶斯估计中，如果我们采用极大似然估计的思想，考虑后验分布极大化而求解 $\theta$，就变成了最大后验估计（Maximum A Posteriori estimation，MAP）：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231127826.png" alt=""></p>
<p>由于 $m(x)$ 与 $\theta$ 无关，因此简化了计算。</p>
<p>作为贝叶斯估计的一种近似解，MAP有其存在的价值，因为贝叶斯估计中后验分布的计算往往是非常棘手的；而且，MAP并非简单地回到极大似然估计，它依然利用了来自先验的信息，这些信息无法从观测样本获得。</p>
<p>对上面的式子稍作处理：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231137928.png" alt=""></p>
<p>如果将机器学习结构风险中的正则化项对应为上式的 $\log\pi(\theta)$ ，那么带有正则化项的最大似然学习就可以被解释为MAP。当然，这并不是总是正确的，例如，有些正则化项可能不是一个概率分布的对数，还有些正则化项依赖于数据，当然也不会是一个先验概率分布。不过，MAP提供了一个直观的方法来设计复杂但可解释的正则化项，例如，更复杂的惩罚项可以通过混合高斯分布作为先验得到，而不是一个单独的高斯分布。</p>
<h3 id="共轭先验"><a href="#共轭先验" class="headerlink" title="共轭先验"></a>共轭先验</h3><p>在贝叶斯估计中，如果选取先验分布 $\pi(\theta)$ ，使得后验分布 $\pi(\theta\vert x)$ 与 $\pi(\theta)$ 属于同一分布簇（即共轭分布），则称 $\pi(\theta)$ 为似然函数 $f(x\vert \theta)$ 的共轭先验。</p>
<p>共轭先验的选取有如下好处：</p>
<ul>
<li>符合直观，先验分布和后验分布应该是相同形式的；</li>
<li>可以给出后验分布的解析形式；</li>
<li>可以形成一个先验链，即现在的后验分布可以作为下一次计算的先验分布，如果形式相同，就可以形成一个链条。</li>
</ul>
<p>常见的共轭先验有：Beta分布（二项分布）、Dirichlet分布（多项分布）。</p>
<h1 id="Zero-Shot-One-Shot-Few-Shot"><a href="#Zero-Shot-One-Shot-Few-Shot" class="headerlink" title="Zero-Shot,One-Shot,Few-Shot"></a>Zero-Shot,One-Shot,Few-Shot</h1><p>Zero-Shot Learning（零样本学习）：利用训练集数据训练模型，使得模型能够对测试集的对象进行分类，但是训练集类别和测试集类别之间没有交集；期间需要借助类别的描述，来建立训练集和测试集之间的联系，从而使得模型有效。</p>
<p>Few-Shot Learning（小样本学习）：我们只通过看几张鸭嘴兽的照片，就能认识鸭嘴兽，这个过程就称作小样本学习。</p>
<p>One-Shot Learning（单样本学习）：Few-Shot Learning的特殊情况，即只看了一张鸭嘴兽的照片，就认识了鸭嘴兽。（人脸识别）</p>
<h1 id="PyTorch和TensorFlow的区别"><a href="#PyTorch和TensorFlow的区别" class="headerlink" title="PyTorch和TensorFlow的区别"></a>PyTorch和TensorFlow的区别</h1><h2 id="动态图与静态图"><a href="#动态图与静态图" class="headerlink" title="动态图与静态图"></a>动态图与静态图</h2><p><code>TensorFlow</code>最初选择使用静态图，这样的设计带来了较高的性能，但在构建网络时较为烦琐，用户需要专门学习<code>TensorFlow</code>的语法架构才能搭建网络，同时很难调试。<code>PyTorch</code>选择使用动态图，动态图的设计模式更加符合人类的思考过程，方便查看、修改中间变量的值，用户可以轻松地搭建网络进行训练。目前，<code>TensorFlow 2.0</code>之后的版本已经支持动态图的构建，并且提供动态图与静态图的转换功能。</p>
<ul>
<li>静态图</li>
</ul>
<p>静态图的生成与执行采用先编译后执行的方式，该模式将计算图的定义和执行进行分离。</p>
<p>静态图只建一次，然后不断复用它，容易在图上做优化，图的效率更高（比如Add操作和ReLU结合在一起优化）。</p>
<p>静态图可以在磁盘中序列化，可以保存整个网络的结构，可以重载，在部署中很实用。</p>
<p>静态图中条件和循环需要特定的语法（tf.condition和tf.while_loop）。</p>
<ul>
<li>动态图</li>
</ul>
<p>动态图采用解析式的执行方式，其核心特点是编译与执行同时发生。</p>
<p>动态图每次使用时建立，不容易优化。</p>
<p>动态图需要重复之前的代码。</p>
<p>只用Python的语法就可以实现条件和循环。</p>
<h2 id="部署和可扩展性"><a href="#部署和可扩展性" class="headerlink" title="部署和可扩展性"></a>部署和可扩展性</h2><p>如果您的项目范围很大，需要大规模部署，或者项目涉及跨平台，那么您的选择应该是<code>TensorFlow</code>，<code>TensorFlow</code>提供了<code>TensorFlow Serving</code>和<code>TensorFlow Lite</code>，可以便捷地将训练好的模型部署到集群以及移动设备上。如果它只是一个较小规模的研究项目的原型设计或类似的东西，那么<code>PyTorch</code>更好。</p>
<h2 id="资源优化和利用率"><a href="#资源优化和利用率" class="headerlink" title="资源优化和利用率"></a>资源优化和利用率</h2><p>如果您正在寻找更好的资源利用率和优化，如<code>GPU</code>，那么<code>PyTorch</code>肯定是首选。但是，当涉及到<code>TensorFlow</code>时，它使用当时可用的所有<code>GPU</code>容量，因此，功能略微缓慢。</p>
<h2 id="学术研究和开源代码"><a href="#学术研究和开源代码" class="headerlink" title="学术研究和开源代码"></a>学术研究和开源代码</h2><p>在学术界<code>PyTorch</code>有很多好评，其中四分之三的论文使用它。而且最早使用<code>TensorFlow</code>的研究人员中，很多人已经迁移到了<code>PyTorch</code>。正因如此，研究会影响教学，从而决定学生学到的是什么。所以如今，大学生对<code>PyTorch</code>了解的相对多一些。</p>
<h1 id="Pytorch底层原理"><a href="#Pytorch底层原理" class="headerlink" title="Pytorch底层原理"></a>Pytorch底层原理</h1><h2 id="自动微分的实现"><a href="#自动微分的实现" class="headerlink" title="自动微分的实现"></a>自动微分的实现</h2><ol>
<li><strong>计算图</strong>：在PyTorch中，每次操作都会被记录在一个计算图上。这个图是一个有向无环图（DAG），表示了所有操作之间的依赖关系。在这个图中，每个节点表示一个操作，每个边表示张量传输。</li>
<li><strong>梯度缓冲区</strong>：在每个操作上，PyTorch都会保留一个梯度缓冲区，用于存储该操作的梯度。在反向传播过程中，这个梯度缓冲区会被填充，然后用于计算每个变量的梯度。</li>
<li><strong>反向传播</strong>：当我们调用<code>.backward()</code>时，PyTorch会开始从最后一层开始，根据每个操作的梯度计算上游的梯度。这个过程会一直进行，直到到达第一层。在这个过程中，每个变量的梯度都会被累积到对应的<code>.grad</code>属性中。</li>
</ol>
<h1 id="Pytorch多卡训练"><a href="#Pytorch多卡训练" class="headerlink" title="Pytorch多卡训练"></a>Pytorch多卡训练</h1><p>对于pytorch而言，有两种方式进行并行：数据并行（DataParallel，DP）和分布式数据并行（DistributedDataParallel，DDP）。</p>
<p>在多卡训练的实现上，DP与DDP的思路是相似的：</p>
<ol>
<li>每张卡都复制一个有相同参数的模型副本。</li>
<li>每次迭代，每张卡分别输入不同批次数据，分别计算梯度。</li>
<li>DP与DDP的主要不同在于接下来的多卡通信：</li>
</ol>
<p>DP的多卡交互实现在一个进程之中，它将一张卡视为主卡，维护单独模型优化器。所有卡计算完梯度后，主卡汇聚其它卡的梯度进行平均并用优化器更新模型参数，再将模型参数更新至其它卡上。</p>
<p>DDP则分别为每张卡创建一个进程，每个进程相应的卡上都独立维护模型和优化器。在每次每张卡计算完梯度之后，进程之间以NCLL（NVIDIA GPU通信）为通信后端，使各卡获取其它卡的梯度。各卡对获取的梯度进行平均，然后执行后续的参数更新。由于每张卡上的模型与优化器参数在初始化时就保持一致，而每次迭代的平均梯度也保持一致，那么即使没有进行参数复制，所有卡的模型参数也是保持一致的。</p>
<h1 id="DataLoader-DataSet-Sampler之间的关系"><a href="#DataLoader-DataSet-Sampler之间的关系" class="headerlink" title="DataLoader, DataSet, Sampler之间的关系"></a>DataLoader, DataSet, Sampler之间的关系</h1><p>首先我们看一下DataLoader.next的源代码长什么样,为方便理解只选取了num_works为0的情况（num_works简单理解就是能够并行化地读取数据）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	...</span><br><span class="line">	</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.num_workers == <span class="number">0</span>:  </span><br><span class="line">            indices = <span class="built_in">next</span>(self.sample_iter)  <span class="comment"># Sampler</span></span><br><span class="line">            batch = self.collate_fn([self.dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices]) <span class="comment"># Dataset</span></span><br><span class="line">            <span class="keyword">if</span> self.pin_memory:</span><br><span class="line">                batch = _utils.pin_memory.pin_memory_batch(batch)</span><br><span class="line">            <span class="keyword">return</span> batch</span><br></pre></td></tr></table></figure>
<p>假设我们的数据是一组图像，每一张图像对应一个index，那么如果我们要读取数据就只需要对应的index即可，即上面代码中的indices，而选取index的方式有多种，有按顺序的，也有乱序的，所以这个工作需要Sampler完成。</p>
<p>那么Dataset和DataLoader在什么时候产生关系呢？没错就是下面一行。我们已经拿到了indices，那么下一步我们只需要根据index对数据进行读取即可了。</p>
<p>综上可以知道DataLoader，Sampler和Dataset三者关系如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308221330259.png" alt=""></p>
<h1 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h1><h2 id="FP16"><a href="#FP16" class="headerlink" title="FP16"></a>FP16</h2><p><strong>半精度浮点数 (FP16)</strong> 是一种计算机使用的二进制浮点数数据类型，使用 2 字节 (16 位) 存储，表示范围为 $[-6.5e^{4},-5.9e^{-8}]\cup[5.9e^{-8},6.5e^{4}]$。而 PyTorch 默认使用<strong>单精度浮点数 (FP32)</strong> 来进行网络模型的计算和权重存储。FP32 在内存中用 4 字节 (32 位) 存储，表示范围为 $[-3e^{38},-1e^{-38}]\cup[1e^{-38},3e^{38}]$。可以看到 FP32 能够表示的范围要比 FP16 大的多得多。</p>
<h2 id="为什么要用FP16"><a href="#为什么要用FP16" class="headerlink" title="为什么要用FP16"></a>为什么要用FP16</h2><ul>
<li>减少显存占用：FP16 的显存占用只有 FP32 的一半，这使得我们可以用更大的 batch size；</li>
<li>加速训练：使用 FP16，模型的训练速度几乎可以提升 1 倍。</li>
</ul>
<h2 id="为什么只用FP16会有问题"><a href="#为什么只用FP16会有问题" class="headerlink" title="为什么只用FP16会有问题"></a>为什么只用FP16会有问题</h2><ul>
<li>上/下溢出：FP16 的表示范围不大，超过 $6.5e^{4}$ 的数字会上溢出变成 inf，小于 $5.9e^{-8}$ 的数字会下溢出变成 0。下溢出更加常见，因为在网络训练的后期，模型的梯度往往很小，甚至会小于 FP16 的下限 $5.9e^{-8}$，此时梯度值就会变成 0，模型参数无法更新。</li>
<li>舍入误差：就算梯度不会上/下溢出，如果梯度值和模型的参数值相差太远，也会发生舍入误差的问题。假设模型参数 $weight=2^{-3}$ ，学习率 $\eta=2^{-2}$ ，梯度 $gradient=2^{-12}$ ，$weight^{‘}=weight+\eta\times gradient=2^{-3}+2^{-2}\times2^{-12}=2^{-3}$ 。</li>
</ul>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><ul>
<li>损失缩放 (Loss Scaling)：为了解决下溢出的问题，论文中对计算出来的 loss 值进行缩放 (scale)，由于链式法则的存在，对 loss 的缩放会作用在每个梯度上。缩放后的梯度，就会平移到 FP16 的有效范围内。这样就可以用 FP16 存储梯度而又不会溢出了。此外，在进行更新之前，需要先将缩放后的梯度转化为 FP32，再将梯度反缩放 (unscale) 回去。 <strong>注意这里一定要先转成 FP32，不然 unscale 的时候还是会下溢出。</strong></li>
<li>FP32 权重备份：为了实现 FP16 的训练，我们需要把模型权重和输入数据都转成 FP16，反向传播的时候就会得到 FP16 的梯度。如果此时直接进行更新，因为<strong>梯度 * 学习率</strong>的值往往较小，和模型权重的差距会很大，可能会出现舍入误差的问题。 所以解决思路是：将<strong>模型权重、激活值、梯度</strong>等数据用 <strong>FP16</strong> 来存储，同时维护一份 <strong>FP32</strong> 的<strong>模型权重副本</strong>用于更新。在反向传播得到 FP16 的梯度以后，<strong>将其转化成 FP32 并 unscale</strong>，最后更新 FP32 的模型权重。因为整个更新过程是在 FP32 的环境中进行的，所以不会出现舍入误差。</li>
<li>黑名单：对于那些在 FP16 环境中运行不稳定的模块，我们会将其添加到黑名单中，强制它在 FP32 的精度下运行。比如需要计算 batch 均值的 BN 层就应该在 FP32 下运行，否则会发生舍入误差。还有一些函数对于算法精度要求很高，比如 torch.acos()，也应该在 FP32 下运行。论文中的黑名单只包含 BN 层。</li>
</ul>
<h1 id="提升模型泛化能力的方法"><a href="#提升模型泛化能力的方法" class="headerlink" title="提升模型泛化能力的方法"></a>提升模型泛化能力的方法</h1><ul>
<li>从数据角度上来说。可以通过数据增强、扩充训练集等方法提高泛化能力。</li>
<li>在训练策略上，可以增加每个<code>Batch Size</code>的大小，进而让模型每次迭代时见到更多数据，防止过拟合。</li>
<li>调整数据分布，做训练数据集的类别均衡。</li>
<li>调整网络结构。如果数据集较小，可以降低模型复杂度防止过拟合。如果数据集较大，可以尝试更加复杂的模型。</li>
<li>减少过拟合的方法也可以提升模型的泛化能力。</li>
</ul>
<h1 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h1><p>偏差：模型预测值的期望与真实值之间的差异，反应的是模型的拟合能力。<br>方差：反应的是训练集的变化所导致的学习性能的变化，即刻画了<strong>数据扰动</strong>所造成的影响，模型过拟合时会出现较大的方差。</p>
<h1 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>过拟合就是随着模型的训练，模型在训练集上的表现越来越好，但是在验证集上的表现却越来越差，也就是说对训练集的拟合程度过高，导致模型的泛化能力降低。<br>欠拟合就是模型在训练集上也无法达到满意的精度。</p>
<h2 id="过拟合的解决方案"><a href="#过拟合的解决方案" class="headerlink" title="过拟合的解决方案"></a>过拟合的解决方案</h2><p>过拟合通常是由于训练数据过少、模型复杂度过大等问题导致的，因此相应的解决方案也是从这两个角度考虑。</p>
<ul>
<li>使用更多的数据进行训练，并且数据集尽量均匀，做一些数据增强。</li>
<li>降低模型复杂度。并不是说模型越复杂越好，如果模型过于复杂，而训练集又较少，那么参数就很容易拟合到一个过于适配训练集的参数空间中，自然就会导致过拟合的出现。</li>
<li><code>Early Stopping</code>，简单来说就是减少迭代次数。随着训练次数的增加，模型对训练数据的拟合程度也会随之增高，所以也可以通过减少训练时间的方法避免过拟合，提高模型泛化能力。</li>
<li><code>L1</code>、<code>L2</code>正则化方法，限制模型权重。</li>
<li>在数据中增加一些噪声，从而通过影响损失函数的优化方向避免过拟合。</li>
<li><code>Dropout</code>，目的也是降低模型的复杂度。</li>
<li><code>ResNet</code>。是的，<code>ResNet</code>也可以解决过拟合问题，因为<code>ResNet</code>的跳线结构可以让部分参数权重归零，进而达到类似于<code>Dropout</code>的效果。</li>
</ul>
<h3 id="L1正则化和L2正则化"><a href="#L1正则化和L2正则化" class="headerlink" title="L1正则化和L2正则化"></a>L1正则化和L2正则化</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202309052221203.png" alt=""></p>
<h4 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h4><p>优点：</p>
<ul>
<li>L1正则化可以自动进行特征选择，这对于需要大量特征的模型非常有用。通过将某些参数优化为零，可以降低模型的复杂度，提高模型的泛化能力。</li>
<li>在L1正则化下，模型的系数向量具有稀疏性（正则化项的解和目标函数的等高线焦点容易在坐标轴上），这使得模型解释性更好。</li>
</ul>
<p>缺点：</p>
<ul>
<li>L1正则化可能会导致模型在零附近的波动较大，因为参数的绝对值可能会受到较大影响。</li>
<li>在L1正则化下，某些特征可能会被错误地忽略，因为它们可能只是与其他特征有微小的差异。</li>
</ul>
<h4 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h4><p>优点：</p>
<ul>
<li>L2正则化可以减少模型的复杂性，降低过拟合的风险。</li>
<li>L2正则化可以使得模型的系数向量的范数较小，这可以防止模型过度拟合训练数据。</li>
</ul>
<p>缺点：</p>
<ul>
<li>L2正则化不会自动进行特征选择，因此对于需要大量特征的模型可能需要手动选择特征。</li>
<li>在L2正则化下，所有特征的重要性都是相等的，这可能会导致某些特征被错误地赋予更高的权重。</li>
</ul>
<h2 id="欠拟合的解决方案"><a href="#欠拟合的解决方案" class="headerlink" title="欠拟合的解决方案"></a>欠拟合的解决方案</h2><p>欠拟合通常是由于模型表征能力不足、数据量过大导致的，刚好和过拟合相反。</p>
<ul>
<li>使用更复杂的模型。</li>
<li>增加迭代次数。</li>
<li>减少数据中的噪声。</li>
</ul>
<h1 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h1><h2 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h2><p>梯度消失就是指在网络反向传播过程中由于链式求导法则不断的累积，如果每一层的梯度都小于 $1$ ，由于累乘效应，出现了某些参数的梯度非常小的现象。在使用这些梯度更新梯度的时候参数值基本没有发生变化，因此就出现了网络训练停滞、模型无法继续优化的问题。</p>
<p>梯度爆炸与之刚好相反，在网络反向传播过程中由于链式求导法则的累乘效应，在每一层梯度都大于 $1$ 的时候，就可能会出现某些参数的梯度非常大。在使用这些梯度更新参数的时候就会导致参数变化过大，就会出现损失函数震荡的现象。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol>
<li>预训练和<code>fine-tuning</code>就是将一些在公开训练集上训练好的模型参数加载到自己对应的模型中，这样损失函数通常就能稳定的优化。</li>
<li>梯度裁剪：梯度裁剪是一个针对梯度爆炸的解决方案，也就是说将梯度限制在某个阈值范围内，如果梯度超过的这个阈值，那么就将其设置为这个阈值。</li>
<li>正则化：正则化也是一种限制梯度爆炸的解决方案，同时也有限制过拟合的作用。</li>
<li>使用<code>ReLU</code>、<code>Leaky ReLU</code>、<code>ELU</code>等激活函数：梯度消失通常是因为损失函数选择 <code>Sigmoid</code> 导致的，而<code>ReLU</code>激活函数在正数部分梯度是恒等于 $1$ 的，由于 $1$ 不会累积加权的特性，自然就可以避免梯度消失或梯度爆炸现象。但是<code>ReLU</code>同样有缺点，作为分段函数，<code>ReLU</code>在负数部分恒为 $0$ ，导致一些神经元无法被激活。而<code>Leaky ReLU</code>、<code>ELU</code>就可以避免这个问题。</li>
<li><code>BN(Batch Normalization)</code>：<code>BN</code>可以加速网络收敛提升训练的稳定性，它把每一层神经网络的任意神经元输入值的分布规范为正态分布，如果采用<code>Sigmoid</code>激活函数，那么就可以使得激活函数的输入落在梯度较大的区域，因此就能一定程度解决梯度消失的问题。</li>
<li>使用类似<code>ResNet</code>的跳线结构：由于离输出近的层学习效果好，而由于链式求导法则的影响可能会导致梯度消失或者梯度爆炸，因此可以模仿<code>ResNet</code>在网络的中间增加跳线结构，这样对应层求导梯度时候由于跳线的连接可以增加一个让梯度无损传播的通路，从而避免梯度消失或者梯度爆炸。</li>
<li>采用<code>LSTM</code>等结构：在<code>NLP</code>领域中，<code>LSTM</code>有时也会被用于对抗梯度现象，这是由于其具有复杂的门结构来控制梯度更新。</li>
</ol>
<h1 id="解决样本不均衡的方法"><a href="#解决样本不均衡的方法" class="headerlink" title="解决样本不均衡的方法"></a>解决样本不均衡的方法</h1><ul>
<li>以多种数据组合形式训练模型并做模型融合。顾名思义，这种方法就是将全部的小样本数据和等量的大样本数据分别组合成几批训练数据集，并以此训练出几个不同的模型并做模型融合，这种方法能够有效的解决样本不均衡问题。</li>
<li>在计算损失函数时改变数据的权重，增加小样本数据的权重，减少大样本的权重。这种方法实际上参考了<code>focal loss</code>的思想，只不过解决的不是难易样本不均衡，而是样本数据量不均衡，同样能保证模型的泛化性能。</li>
<li>过采样小样本，欠采样大样本。也就是对于训练数据，把那些样本量较小的数据多在<code>load</code>数据时重复几遍，对于那些样本量较大的数据，则尽量减少<code>load</code>它们，以此来达到样本均衡的目的。例如：SMOTE、ADASYN、bSMOTE算法通过插值生成合成样本进行过采样；使用Tomek Links、Cluster Centroids、NearMiss进行欠采样。</li>
<li>使用集成方法：如Bagging、Boosting等集成方法可以改善不平衡数据集的分类性能。</li>
<li>将分类问题看成异常检测问题。</li>
</ul>
<h1 id="LabelSmoothing"><a href="#LabelSmoothing" class="headerlink" title="LabelSmoothing"></a>LabelSmoothing</h1><p>传统做图像分类采用的损失是交叉熵损失，具体形式是： $\mathcal{L}=-\sum_{i=1}^{m}t_{i}\log(y_{i})$ 。其中， $m$ 表示类别数， $y_{i}$ 表示<code>softmax</code>之后每个类的预测概率， $t_{i}$ 表示样本的真实标签值。然而，神经网络有一个坏习惯，就是在训练过程中对预测变得”过于自信“，这可能会降低它们的泛化能力，从而在新的、看不见的未来数据上表现得同样出色。此外，大型数据集通常会包含标签错误的数据，这意味着神经网络在本质上应该对“正确答案”持怀疑态度，以减少一定程度上围绕错误答案的极端情况下的建模。</p>
<p>因此，标签平滑所做的就是通过训练网络向<code>1-adjustment</code>目标移动，然后在其余的类上除以这个<code>adjustment</code>，从而使它对自己的答案不那么自信，而不是简单的设为 $1$ 。新标签的表现形式为：$T^{\prime}=(1-\varepsilon)*T+\frac{\varepsilon}{N}$。其中， $N$ 表示类别的个数， $T$ 表示真实标签值， $T^{\prime}$ 表示平滑后的标签。</p>
<p>例如，原来的标签 $T$ 为 $[0,0,1,0,0]$ ， $\varepsilon=0.1$ ，经过<code>LabelSmoothing</code>之后的标签 $T^{\prime}$ 为 $[0.02,0.02,0.92,0.02,0.02]$ 。</p>
<h1 id="图像处理中平滑和锐化操作是什么？"><a href="#图像处理中平滑和锐化操作是什么？" class="headerlink" title="图像处理中平滑和锐化操作是什么？"></a>图像处理中平滑和锐化操作是什么？</h1><h2 id="概念-2"><a href="#概念-2" class="headerlink" title="概念"></a>概念</h2><p>锐化就是通过增强图像的高频信息，也就是纹理边缘来减少图像中的模糊细节，但是在增强纹理的时候也引入了图像噪声。</p>
<p>平滑处理<code>(smoothing)</code>也称模糊处理<code>(bluring)</code>，主要用于消除图像中的噪声部分，平滑处理常用的用途是用来减少图像上的噪点或失真，平滑主要使用图像滤波。在这里，我个人认为可以把图像平滑和图像滤波联系起来，因为图像平滑常用的方法就是图像滤波器。</p>
<h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><ul>
<li>在计算机视觉的一些任务中，涉及到图像重建的、如高精度的深度估计、医学图像分割、三维重建等任务，最终需要得到原始图像分辨率大小的输出，同时对图像的边缘清晰度也有较高的要求，这时候可以通过增强特征图中的高频分量，在计算损失函数的时候放大这些区域的损失，进而放大对应参数的梯度，使得网络往更突出边缘的方向上优化。</li>
<li>与上相反，如果是一些希望输出更加平滑的任务，则可以考虑对特征图进行平滑操作，进而减小高频区域的损失，减小对应参数的梯度，使得网络往更平滑的方向上优化，通常这种技术都会用在<code>smooth loss</code>中。</li>
</ul>
<h1 id="图像中的低频信号和高频信号"><a href="#图像中的低频信号和高频信号" class="headerlink" title="图像中的低频信号和高频信号"></a>图像中的低频信号和高频信号</h1><p>图像中的高频分量，指的是图像强度（亮度/灰度）变化剧烈的地方，也就是我们常说的边缘（轮廓）；图像中的低频分量，指的是图像强度（亮度/灰度）变换平缓的地方，也就是大片色块的地方。人眼对图像中的高频信号更为敏感。</p>
<h1 id="batchsize"><a href="#batchsize" class="headerlink" title="batchsize"></a>batchsize</h1><ul>
<li><code>batchsize</code>：批大小。在深度学习中，一般采用<code>SGD</code>训练，即每次训练在训练集中取<code>batchsize</code>个样本训练。</li>
<li><code>iteration</code>：1个<code>iteration</code>等于使用<code>batchsize</code>个样本训练一次。</li>
<li><code>epoch</code>：1个<code>epoch</code>等于使用训练集中的全部样本训练一次。</li>
</ul>
<p>如果数据集比较小，则完全可以采用全数据集的形式。这样做的好处有两点：</p>
<ol>
<li>全数据集的方向能够更好的代表样本总体，确定其极值所在。</li>
<li>由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。</li>
</ol>
<p>增大<code>batchsize</code>的好处有三点：</p>
<ol>
<li>内存的利用率提高了，大矩阵乘法的并行化效率提高。</li>
<li>跑完一次<code>epoch</code>（全数据集）所需迭代次数减少，对于相同的数据量的处理速度进一步加快。</li>
<li>一定范围内，<code>batchsize</code>越大，其确定的下降方向就越准，引起训练震荡越小。</li>
</ol>
<p>盲目增大<code>batchsize</code>的坏处有三点：</p>
<ol>
<li>当数据集太大时，内存撑不住。</li>
<li>跑完一次<code>epoch</code>（全数据集）所需迭代次数减少了，但要想达到相同的精度，时间开销太大，参数的修正更加缓慢。</li>
<li><code>batchsize</code>增大到一定的程度，其确定的下降方向已经基本不再变化。</li>
</ol>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="Huber-Loss"><a href="#Huber-Loss" class="headerlink" title="Huber Loss"></a>Huber Loss</h2><p>Huber损失函数是一种在回归问题中常用的损失函数，它对于输入数据的小偏差具有较低的敏感性，对于大偏差具有较高的敏感性。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308261842588.png" alt=""></p>
<h2 id="Smooth-L1-Loss"><a href="#Smooth-L1-Loss" class="headerlink" title="Smooth L1 Loss"></a>Smooth L1 Loss</h2><p>smooth L1 loss是Huber loss在 $\delta=1$ 条件下的特例。</p>
<ol>
<li>当预测框与<code>ground truth</code>差别过大时，梯度值不至于过大；</li>
<li>当预测框与<code>ground truth</code>差别很小时，梯度值足够小。</li>
</ol>
<p>为啥要做这两方面的限制呢？</p>
<ol>
<li>差距大时，梯度过于大，可能会导致梯度爆炸；</li>
<li>差距很小时，梯度足够小，能够接近最优点，避免大幅横跳。</li>
</ol>
<p><code>L2</code>、<code>L1</code>、<code>Smooth L1</code>损失函数分别定义为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222347197.png" alt=""></p>
<p>损失函数对 $x$ 的导数分别为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222349037.png" alt=""></p>
<p><code>L2</code>对 $x$ 的导数，与 $x$ 成<strong>正比关系</strong>。也就是当 $x$ 增大时，对 $x$ 的导数也线性增大。这就导致在训练初期，预测值与 <code>groud truth</code> 差异过于大时，损失函数对预测值的梯度十分大，<strong>训练初期不稳定</strong>。</p>
<p><code>L1</code>对 $x$ 的导数为常数，始终为 $1$ 或 $-1$ 。这就导致训练后期，预测值与<code>ground truth</code>差异很小时，损失对预测值的导数的绝对值仍然为 $1$ ，而 <code>learning rate</code>如果不变，损失函数将在稳定值附近波动，难以继续收敛以达到更高精度。</p>
<p><code>Smooth L1</code> 在 $x$ 较小时，也就是 $[-1,1]$ 区间，对 $x$ 的梯度也会变小；而在 $x$ 很大时，对 $x$ 的梯度的绝对值达到上限 $1$ ，也不会太大以至于破坏网络参数。 完美地避开了<code>L1</code> 和 <code>L2</code>损失的缺陷。</p>
<h2 id="Cross-Entropy-Loss"><a href="#Cross-Entropy-Loss" class="headerlink" title="Cross Entropy Loss"></a>Cross Entropy Loss</h2><p>既可用于多分类任务（只有一个能胜出），也可用于二分类任务。设标签为 $y$ （多分类中是one-hot编码），网络预测结果为 $\hat{y}$ ，<code>CE</code>损失函数为（这里只考虑了一个样本，c指的是多分类的类别数目）：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211132940.png" alt=""></p>
<p><strong>分类为什么选择交叉熵而不是MAE：</strong></p>
<ol>
<li>稀疏性：交叉熵对于错误分类的样本具有稀疏性，即将一个样本错误分类为另一个类别的损失相对较小。相比之下，MAE对于每一个错误分类的样本都会产生较大的损失。因此，交叉熵在处理不平衡的数据集时表现更好。</li>
<li>对异常值的鲁棒性：交叉熵对于异常值不太敏感，而MAE对于异常值非常敏感。因此，使用交叉熵的模型对异常值的鲁棒性更强。</li>
<li>优化性能：对于深度学习模型，使用交叉熵作为损失函数通常会导致更快的优化收敛。这是由于交叉熵损失函数相对于模型参数的梯度具有较小的方差。</li>
</ol>
<h2 id="Binary-Cross-Entropy-Loss"><a href="#Binary-Cross-Entropy-Loss" class="headerlink" title="Binary Cross Entropy Loss"></a>Binary Cross Entropy Loss</h2><p>二值交叉熵损失，虽然总是用来学习0/1分布，即二分类问题，但不是0/1两个数，只要在0~1之间的数也都能学习。设标签为 $y$ ，网络预测结果为 $\hat{y}$ ，<code>BCE</code>损失函数为（这里考虑了N个样本，每个样本都是二分类）： </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211055521.png" alt=""></p>
<p>某些情况下需要加上权重：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211059473.png" alt=""></p>
<h2 id="Focal-loss"><a href="#Focal-loss" class="headerlink" title="Focal loss"></a>Focal loss</h2><p>Focal Loss的引入主要是为了解决one-stage目标检测中正负样本数量极不平衡问题。当易区分负样本超级多时，整个训练过程将会围绕着易区分负样本进行，进而淹没正样本，造成大损失。所以这里引入了一个调制因子 ，用来聚焦难分样本，公式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211251919.png" alt=""></p>
<p>当 $p_{t}$ 趋向于1，即说明该样本是易区分样本，此时调制因子是趋向于0，说明对损失的贡献较小，即减低了易区分样本的损失比例。当 $p_{t}$ 很小，也就是假如某个样本被分到正样本，但是该样本为前景的概率特别小，即被错分到正样本了，此时调制因子是趋向于1，对loss也没有太大的影响。</p>
<p>我们在实验中采用了如下的$\alpha$-平衡变体形式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308101255449.png" alt=""></p>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><p><code>Sigmoid</code>是最基础的激活函数，可以将任意数值转换为概率（缩放到 $0 \thicksim 1$ 之间），在分类等场景中有广泛的应用。<code>Sigmoid</code>函数的形式是 $\sigma(z)=\frac{1}{1+e^{-z}}$ 。其对应的函数图像如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101955821.png" alt=""></p>
<h2 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h2><p>激活函数<code>tanh</code>和<code>Sigmoid</code>类似，都是<code>S</code>形曲线，输出范围是$[-1, 1]$。<code>tanh</code>函数的形式为 $g(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$ 。其对应的函数图像如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102004442.png" alt=""></p>
<h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p><code>ReLU(Rectified Linear Unit)</code>，是一种人工神经网络中常用的激活函数。通常意义下，其指代数学中的斜坡函数，即<br> $f(x)=\max(0,x)$ 。其对应的函数图像如下所示：<br><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101950246.png" alt=""></p>
<h2 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h2><p>为了解决<code>dead ReLU</code>问题（<code>ReLU</code>在训练的时很“脆弱”。在 $x&lt;0$ 时，梯度为 $0$ ，这个神经元及之后的神经元梯度永远为 $0$ ，不再对任何数据有所响应，导致相应参数永远不会被更新）。<code>Leaky ReLU</code>用一个类似 $0.01$ 的小值来初始化神经元，从而使得<code>ReLU</code>在负数区域更偏向于激活而不是坏死，这里的斜率都是确定的。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102008358.png" alt=""></p>
<h2 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h2><p><code>ELU</code>的提出也解决了<code>ReLU</code>的问题。与<code>ReLU</code>相比，<code>ELU</code>有负值，这会使激活的平均值接近零，让模型学习得更快。当 $x&lt;0$ 时，<code>ELU</code>的函数形式为 $f(x)=\alpha(e^{x}-1)$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102010115.png" alt=""></p>
<h2 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h2><p>激活函数<code>GELU</code>的灵感来源于<code>ReLU</code>和<code>Dropout</code>，在激活中引入了<strong>随机正则</strong>的思想。<code>GELU</code>通过输入自身的概率分布情况，决定抛弃还是保留当前的神经元。<code>GELU</code>函数的形式是 $GELU(x)=0.5x(1+\tanh(\sqrt\frac{2}{\pi}(x+0.044715x^{3})))$ 。其对应的函数图像如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102019772.png" alt=""></p>
<p>可以理解为，对于输入的值，根据它的情况乘上 $1$ 或 $0$ 。更「数学」一点的描述是，对于每一个输入 $x$ ，其服从于标准正态分布 $\mathcal{N}(0, 1)$ ，它会乘上一个伯努利分布 $Bernoulli(\Phi(x))$，其中 $\Phi(x) = P(X \leq x)$ 。随着 $x$ 的降低，它被归零的概率会升高。对于<code>ReLU</code>来说，这个界限就是 $0$ ，输入少于零就会被归零。这一类激活函数，不仅保留了概率性，同时也保留了对输入的依赖性。<code>GELU</code>在最近的<code>Transformer</code>模型中（包括<code>BERT</code>，<code>RoBertA</code>和<code>GPT2</code>等）得到了广泛的应用。</p>
<h2 id="ReLU比Sigmoid效果好在哪里？"><a href="#ReLU比Sigmoid效果好在哪里？" class="headerlink" title="ReLU比Sigmoid效果好在哪里？"></a>ReLU比Sigmoid效果好在哪里？</h2><p><code>ReLU</code>的输出要么是 $0$ , 要么是输入本身。虽然方程简单，但实际上效果更好。</p>
<ol>
<li><code>ReLU</code>函数计算简单，可以减少很多计算量。反向传播求误差梯度时，涉及除法，计算量相对较大，采用<code>ReLU</code>激活函数，可以节省很多计算量。</li>
<li>避免梯度消失问题。对于深层网络，<code>Sigmoid</code>函数反向传播时，很容易就会出现梯度消失问题（在<code>Sigmoid</code>接近饱和区时，变换太缓慢，导数趋于 $0$ ，这种情况会造成信息丢失），从而无法完成深层网络的训练。例如在<code>RNN</code>当中，随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于 $0$ ，这就是“梯度消失“现象。此时采用<code>ReLU</code>激活函数就避免了“梯度消失“的发生。</li>
<li>可以缓解过拟合问题的发生，<code>ReLU</code>会使一部分神经元的输出为 $0$ ，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。</li>
</ol>
<h1 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h1><p>记号： $\theta_{t}$ 表示第 $t$ 轮的参数， $\eta$ 表示学习率， $g_{t}$ 表示第 $t$ 轮的梯度即 $\triangledown\hat{\mathcal{L}}(\theta_{t})$ ， $m_{t}$ 表示第 $t$ 轮的一阶动量， $v_{t}$ 表示第 $t$ 轮的二阶动量， $\hat{m}_{t}$ 为偏差纠正后的一阶矩估计， $\hat{v}_{t}$ 为偏差纠正后的二阶矩估计。</p>
<h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p><code>SGD(Stochastic Gradient Descent)</code>，随机梯度下降。每次选择一个<code>mini-batch</code>，而不是全部样本，使用梯度下降来更新模型参数。它解决了随机小批量样本的问题，但仍然有自适应学习率、容易卡在梯度较小点等问题。</p>
<p>$m_{t}=g_{t}, \ v_{t}=1$</p>
<p>$\theta_{t+1}=\theta_{t}-\eta \frac{m_{t}}{\sqrt{v_{t}}}=\theta_{t}-\eta g_{t} $</p>
<p><strong>缺点：</strong>下降速度慢，而且可能会在沟壑的两边持续振荡，停留在一个局部最优点</p>
<h2 id="SGDM"><a href="#SGDM" class="headerlink" title="SGDM"></a>SGDM</h2><p><code>SGDM(SGD with momentum)</code>，在<code>SGD</code>基础上增加一阶动量。  参数更新时以上一个时刻的一阶动量为主，其中 $\beta$ 通常取 $0.9$。</p>
<p>$m_{t} = \beta m_{t-1} + (1-\beta)  g_{t}, \ v_{t} = 1$</p>
<p>$\theta_{t+1} = \theta_{t} - \eta \frac{m_{t}}{\sqrt{v_{t}}} = \theta_{t} - \eta (\beta m_{t-1} + (1-\beta) g_{t}) $</p>
<h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>在<code>SGD</code>基础上增加二阶动量，可以对模型中的每个参数分配自适应学习率。</p>
<p>$m_{t}=g_{t}$</p>
<p>$v_{t}=\sum_{\tau=1}^{t}g_{\tau}^{2}$</p>
<p>$\theta_{t+1}=\theta_{t}-\eta \frac{m_{t}}{\sqrt{v_{t}+\epsilon}}=\theta_{t}-\eta \frac{g_{t}}{\sqrt{\sum_{\tau=1}^{t}g_{\tau}^{2}+\epsilon}}$</p>
<p><strong>优点：Adagrad在稀疏数据场景下表现最好</strong>，因为对于频繁出现的参数，学习率衰减快；对于稀疏的参数，学习率衰减的更慢</p>
<p><strong>缺点：</strong>在实际很多情况下，<strong>二阶动量呈单调递增，累积从训练开始的梯度，学习率会很快减至0，导致参数不再更新</strong>，训练过程提前结束</p>
<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p><code>SGD(Root Mean Square Prop)</code>在<code>SGD</code>基础上增加二阶动量，由于<code>Adagrad</code>的学习率衰减太过激进，改变二阶动量的计算策略：<strong>不累计全部梯度，只关注过去某一窗口内的梯度</strong>。<strong>指数移动平均值</strong>大约是过去一段时间的平均值，反映<strong>局部的</strong>参数信息，用这个方法来计算二阶累积动量。</p>
<p>$m_{t}=g_{t}$</p>
<p>$v_{t}=\beta v_{t-1}+(1-\beta)g_{t}^{2}$</p>
<p>$\theta_{t+1}=\theta_{t}-\eta\frac{g_{t}}{\sqrt{v_{t}}}=\theta_{t}-\eta\frac{g_{t}}{\sqrt{\beta v_{t-1}+(1-\beta)g_{t}^{2}}} $</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p><code>Adam(Adaptive Moment Estimation)</code>，自适应矩估计。是2014年提出的一种万金油式的优化器，使用起来非常方便，梯度下降速度快，但是容易在最优值附近震荡。竞赛中性能会略逊于<code>SGD</code>，毕竟最简单的才是最有效的。但是超强的易用性使得<code>Adam</code>被广泛使用。是<code>SGDM</code>和<code>RMSProp</code>的结合。</p>
<p>$m_{t}=\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}$</p>
<p>$v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}$</p>
<p>$\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}}$</p>
<p>$\hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}}$</p>
<p>$\theta_{t+1}=\theta_{t}-\eta\frac{\hat{m}_t}{\sqrt{\hat{v}_{t}}+\epsilon}$</p>
<p><strong>解释：</strong></p>
<p>第一项 $m_{t}$ 为t时刻，梯度在动量形式下的一阶矩估计。</p>
<p>第二项 $v_{t}$ 为梯度在动量形式下的二阶矩估计。</p>
<p>第三项 $\hat{m}_{t}$ 为偏差纠正后的一阶矩估计。</p>
<p>第四项 $\hat{v}_{t}$ 为偏差纠正后的二阶矩估计。</p>
<p>最后一项是更新公式，可以参考<code>RMSProp</code>以及之前的算法。</p>
<p><strong>为什么需要偏差纠正？</strong></p>
<p>拿梯度在动量形式下的二阶矩估计 $v_{t}$ 为例，各个 $v_{t}$ 的公式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071555276.png" alt=""></p>
<p>而我们实际上需要的是梯度的二阶矩估计，也就是 $E(g_{i}^{2})$ 。因此使用动量求出来的二阶矩估计是有偏的，需要纠正。我们对动量二阶矩估计 $v_{t}$ 求期望 $E(v_{t})$ ，可以通过等比数列公式得到 $E(v_{t})$ 与 $E(g_{i}^{2})$ 的关系：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071559636.png" alt=""></p>
<p>因此，要得到 $E(g_{i}^{2})$ ，就需要除掉前面的系数 $(1-\beta_{2}^{t})$ 是一个常数</p>
<h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><h2 id="Top-1-Accuracy和Top-5-Accuracy"><a href="#Top-1-Accuracy和Top-5-Accuracy" class="headerlink" title="Top-1 Accuracy和Top-5 Accuracy"></a>Top-1 Accuracy和Top-5 Accuracy</h2><p>Top-1：就是你预测的label取最后概率向量里面最大的那一个作为预测结果 ，如过预测结果中概率最大的那个分类正确，则预测正确，否则预测错误。<br>Top-5：就是最后概率向量最大的前五名中，只要出现了正确概率即为预测正确，否则预测错误。</p>
<h2 id="DICE"><a href="#DICE" class="headerlink" title="DICE"></a>DICE</h2><p>用来度量图像分割结果的精度，可以有效地度量预测结果<code>pred</code>和标签<code>label</code>的相似度（和评价指标<code>F1</code>是相同的）</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306191056073.png" alt=""></p>
<h2 id="IoU"><a href="#IoU" class="headerlink" title="IoU"></a>IoU</h2><p><code>IoU</code>全称<code>Intersection-over-Union</code>，即交并比，在目标检测和语义分割领域都有使用。</p>
<ul>
<li>在目标检测领域，定义为两个矩形框面积的交集和并集的比值，$IoU=\frac{A\cap B}{A\cup B}$。如果完全重叠，则<code>IoU</code>等于1，是最理想的情况。一般在检测任务中，<code>IoU</code>大于等于<code>0.5</code>就认为召回，如果设置更高的<code>IoU</code>阈值，则召回率下降，同时定位框也越更加精确。</li>
<li>在图像分割中也会经常使用<code>IoU</code>，此时就不必限定为两个矩形框的面积。比如对于二分类的前背景分割，那么$IoU=\frac{真实前景像素面积\cap预测前景像素面积}{真实前景像素面积\cup预测前景像素面积}$，这一个指标，通常比直接计算每一个像素的分类正确概率要低，也对错误分类更加敏感。</li>
</ul>
<h2 id="MIoU"><a href="#MIoU" class="headerlink" title="MIoU"></a>MIoU</h2><p>均交并比，语义分割的标准度量。计算两个集合的交集与并集之比，在语义分割中，这两个集合为真实值和预测值。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306191111890.png" alt=""></p>
<p>上述公式和下面的公式是等价的：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306191113911.png" alt=""></p>
<p>计算<code>MIoU</code>的三个步骤：</p>
<ol>
<li>计算混淆矩阵</li>
<li>计算每个类别的<code>IoU</code></li>
<li>对每个类别的<code>IoU</code>取平均</li>
</ol>
<h2 id="AP"><a href="#AP" class="headerlink" title="AP"></a>AP</h2><p>AP为某一类别的目标平均精确度，具体为该类目标在11个不同置信度阈值下计算得到的P-R曲线下方的面积值。</p>
<h2 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h2><p>mAP(mean Average Precision)：是多个类别AP的平均值。这个mean的意思是对每个类的AP再求平均，得到的就是mAP的值。mAP的大小一定在[0,1]并且越大越好。</p>
<h2 id="Box-AP"><a href="#Box-AP" class="headerlink" title="Box AP"></a>Box AP</h2><p>Box AP 即Box Average Precision，用于综合评价目标检测模型效能。要清楚的是AP的计算是先使用<strong>confidence threshold</strong>去除一些置信度过低的框；然后要使用NMS要用到 <strong>nms_boxiou threshold</strong>，去除和置信度最高的框IoU大于阈值的重叠框；接着会使用<strong>box_iou threshold</strong>，将每个框分为TP、FP，对于同一个GT，预测框根据置信度从高到低排列，只有IoU大于阈值且置信度最高的那个算是TP，其余IOU大于阈值的都算是FP；分配好TP、FP以后，所有框按照置信度从高到低排列，计算AP值。</p>
<ul>
<li>confidence threshold：置信度阈值，每个输出框都会给出置信度，只有置信度高于某个阈值的框才会被考虑。</li>
<li>nms_boxiou threshold：NMS方法的逻辑是先根据预测出的前景分数从高到低排序box，从分数最高的box开始遍历，与这个box的overlap大于某个threshold的box就会被去掉，例如我们以score=0.8的box为基准，overlap_threshold=0.2，那么与score=0.8的box的overlap大于0.2的框，都会被丢弃。如果框X与score=0.8的框的overlap小于0.2，那么我们会认为框X属于另一个物体，不用抑制。</li>
<li>box_iou threshold：例如我们以IoU为30%作为评估的阈值，如果检测结果和GT的IoU超过30%，则此次检测为TP，否则就是FP。</li>
<li>AP：AP的值就是PR曲线下方围成的面积的值。</li>
</ul>
<h2 id="Mask-AP"><a href="#Mask-AP" class="headerlink" title="Mask AP"></a>Mask AP</h2><p>Mask AP用于综合评价实例分割模型效能。Mask AP和Box AP的区别仅仅在于box_iou threshold作用的对象不相同，Box AP里面作用的是标准普通的GT和预测框的IoU值，Mask AP里面作用的是GT mask和预测mask的mask IoU，即像素点之间的mask IoU。</p>
<h1 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h1><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p><code>Bagging(Bootstrap aggregating)</code>，引导聚集算法，又称装袋算法，是机器学习领域的一种团体学习算法。<code>Bagging</code>算法可与其他分类、回归算法结合，提高其准确率、稳定性的同时，通过降低结果的方差，避免过拟合的发生。</p>
<p><strong>随机采样（bootstrap sample）</strong>从 $n$ 个数据点中<strong>有放回地重复随机</strong>抽取一个样本（即同一个样本可被多次抽取），共抽取 $n$ 次。创建一个与原数据大小相同得数据集，但有些数据点会缺失（大约 $1/3$ ），有些会重复。</p>
<p><code>Bagging</code>对于弱学习器没有限制，这和<code>Adaboost</code>一样。但是最常用的一般也是<strong>决策树</strong>和<strong>神经网络</strong>。</p>
<p><code>Bagging</code>的集合策略也比较简单，对于<strong>分类问题</strong>，通常使用<strong>简单投票法</strong>，得到最多票数的类别或者类别之一为最终的模型输出。对于<strong>回归问题</strong>，通常使用<strong>简单平均法</strong>，对 $T$ 个弱学习器得到的回归结果进行算术平均得到最终的模型输出。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071107979.png" alt=""></p>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p>随机森林以决策树为基本单元，通过集成大量的决策树，就构成了随机森林。其构造过程如下：</p>
<ol>
<li>$T$ 中共有 $N$ 个样本，有放回的随机选择 $N$ 个样本（因为有放回，所以虽然是 $N$ 但是不可能遍历所有样本）。这选择好了的 $N$ 个样本用来训练一个决策树，作为决策树根节点处的样本。</li>
<li>当每个样本有 $M$ 个属性时，在决策树的每个节点需要分裂时，随机从这 $M$ 个属性中选取出 $m$ 个属性，满足条件 $m &lt;&lt; M$ 。然后从这 $m$ 个属性中采用某种策略来选择某个属性作为该节点的分裂属性。</li>
<li>决策树形成过程中每个节点都要按照上述步骤来分裂，一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。</li>
<li>重复建立大量的决策树，这样就构成了随机森林了。</li>
</ol>
<h4 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h4><h5 id="决策树构建的终止条件"><a href="#决策树构建的终止条件" class="headerlink" title="决策树构建的终止条件"></a>决策树构建的终止条件</h5><ol>
<li><p>样本进来属于同一个类别，直接输出结果是 $C$ 类返回。</p>
</li>
<li><p>没法划分了，特征向量中所有属性都用完了；或者 $D$ 样本中的属性 $A$ 都相同，然后将此节点标记为叶子节点，将样本 $D$ 中数目最多的类当做类别返回。</p>
</li>
<li>当对数据进行划分成多个分支，如果存在分支中没有数据（分支为空），将划分前类别数目多的当做类别返回。</li>
</ol>
<h5 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h5><p>各种准则虽然对决策树的尺寸有较大影响，<strong>但对泛化性能的影响很有限，剪枝方法和程度对决策树泛化性能的影响更为显著；剪枝是决策树防止过拟合的手段</strong>。</p>
<p><strong>预剪枝：</strong>在决策树构造时就进行剪枝。在决策树构造过程中，对节点进行评估，如果对其划分并不能再验证集中提高准确性，那么该节点就不要继续往下划分。这时就会把当前节点作为叶节点。</p>
<p><strong>后剪枝：</strong>在生成决策树之后再剪枝。通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉该节点，带来的验证集中准确性差别不大或有明显提升，则可以对它进行剪枝，用叶子节点来代填该节点。</p>
<p><strong>预剪枝vs后剪枝</strong></p>
<p>时间开销：</p>
<ul>
<li>预剪枝：训练时间开销降低，测试时间开销降低</li>
<li>后剪枝：训练时间开销增加，测试时间开销降低</li>
</ul>
<p>过/欠拟合风险:</p>
<ul>
<li>预剪枝：过拟合风险降低，欠拟合风险增加</li>
<li>后剪枝：过拟合风险降低，欠拟合风险基本不变</li>
</ul>
<p>泛化性能：后剪枝通常优于预剪枝</p>
<h5 id="决策树生成算法"><a href="#决策树生成算法" class="headerlink" title="决策树生成算法"></a>决策树生成算法</h5><p><strong>ID3</strong></p>
<p>使用信息熵增益作为特征选择的标准。</p>
<p>数据集 $D$ 的经验熵定义为： $Ent(D)=-\sum_{k=1}^{K}\frac{|C_{k}|}{|D|}\log_{2}\frac{|C_{k}|}{|D|}$ ，其中 $|C_{k}|$ 为第 $k$ 类样本的数目， $|D|$ 为数据集 $D$ 中样本的数目。</p>
<p>计算特征 $A$ 对数据集 $D$ 的经验条件熵： $Ent(D|A)=\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}Ent(D_{i})=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_{i}|}\log_{2}\frac{|D_{ik}|}{|D_{i}|}$  。意思就是对划分后的新的 $n$ 个小数据集的经验熵加权，权重为每一个小数据集中的样本数目占未划分前的大数据集的比例。</p>
<p>计算信息熵增益： $gain(D,A)=Ent(D)-Ent(D|A)$ ，选择信息熵增益最大的特征。</p>
<p><strong>C4.5</strong></p>
<p>与<code>ID3</code>算法的最大不同在于使用信息熵增益率代替信息熵增益。信息熵增益率的定义如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304211122841.png" alt=""></p>
<p>其中 $IV(A)=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}\log_{2}\frac{|D_{i}|}{|D|}$ 称为数据集 $D$ 关于 $A$ 的取值熵。</p>
<p>这种方法对可能取值少的属性有所偏好，因此<code>C4.5</code>算法也不是直接使用增益率最大的来划分属性，而是使用了一种“启发式”的方法，先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。</p>
<p><strong>CART</strong></p>
<p><code>CART</code>决策树使用“基尼指数”来选择划分属性，选取那个使划分后基尼指数<strong>最小</strong>的属性。数据集 $D$ 的纯度可用基尼值来度量。 $Gini(D)$ 越小，则数据集的纯度越高。 $Gini(D)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p_{k}^{2}$ ，则属性 $A$ 的基尼指数表达式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304211122760.png" alt=""></p>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p><code>Bagging</code>在随机森林的构建过程中，各棵树之间是相互独立的，在构建第 $m$ 棵树的时候，不会考虑前面的 $m-1$ 棵树。<code>Boosting</code>在构建第 $m$ 棵子树的时候，会考虑到前 $m-1$ 棵子树的结果。</p>
<p><strong>提升学习(Boosting)</strong>是一种机器学习技术，通过从训练数据构建模型，然后创建第二个模型来尝试纠正第一个模型中的错误来完成的。添加模型直到完美预测训练集或添加最大数量的模型。提升学习的每一步产生弱预测模型（如决策树），并加权累加到总模型中；如果每一步的弱预测模型的生成都是依据损失函数的梯度方式，就称为梯度提升<code>(Gradient Boosting)</code>。</p>
<h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071138731.png" alt=""></p>
<p><code>Adaptive Boosting(AdaBoost)</code>是第一个为二进制分类开发的真正成功的提升算法。这是理解<code>Boosting</code>的最佳起点，现代提升方法建立在<code>AdaBoost</code>之上。</p>
<p><code>AdaBoost</code>流程：</p>
<ol>
<li><p>训练数据集中的每个实例都被加权。初始权重设置为： $Weight(X_{i})=\frac{1}{N_{}}$ </p>
</li>
<li><p>使用加权之后的样本作为训练数据，以弱分类器（决策树桩）进行训练。</p>
</li>
<li><p>为训练后的模型计算当前分类器的分类误差。传统的计算方式如下：</p>
<p>分类误差： $error_t = \sum_{i=1}^{N}(W_{t,i}\times terror_{i})$ 。其中 $terror_{i}=I(G_{t}(X_{i})\neq y_{i})$ 。</p>
<p>例如：如果我们有三个训练实例，权重分别为 $0.01$ 、 $0.5$ 和 $0.2$ 。预测值为 $-1$ 、 $-1$ 和 $-1$ ，实例中的真实输出变量为 $-1$ 、 $1$ 和 $-1$ ，则  $terror$ 为 $0$ 、 $1$ 和 $0$ 。误分类率将计算为：<strong>error = (0.01*0 + 0.5*1 + 0.2*0) or error = 0.5</strong>。</p>
</li>
<li><p>为经过训练的模型计算阶段权值，该值为模型做出的任何预测提供权重。训练模型的阶段值计算公式： $\alpha_{t}=\frac{1}{2}\times\ln\frac{1-error_{t}}{error_{t}}$</p>
</li>
<li><p>更新训练权重，为错误预测的实例提供更多的权重，为正确预测的实例提供更少权重。计算公式： $W_{t+1,i}=\frac{W_{t,i}}{Z_{t}}\exp(-\alpha_{t}G_{t}(X_{i})y_{i})$ ， $Z_{t}$ 是规范因子， $Z_{t}=\sum_{i=1}^{N}W_{t,i}\exp(-\alpha_{t}G_{t}(X_{i})y_{i})$</p>
</li>
<li><p>最终的分类器为： $G(X)=sign(\sum_{m=1}^{K}\alpha_{m}G_{m}(X))$</p>
</li>
</ol>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p><code>GBDT(Gradient Boosting Decision Tree)</code>在数据分析和预测中的效果很好。它是一种基于决策树的集成算法。其中<code>Gradient Boosting</code>是集成方法<code>Boosting</code>中的一种算法，通过梯度下降来对新的学习器进行迭代。将表现一般的数个模型（通常是深度固定的决策树）组合在一起来集成一个表现较好的模型。抽象地说，模型的训练过程是对一任意可导目标函数的优化过程。通过反复地选择一个指向负梯度方向的函数，该算法可被看做在函数空间里对目标函数进行优化。因此可以说<code>Gradient Boosting = Gradient Descent + Boosting</code>。</p>
<p>模型的结果是一组回归分类树组合<code>(CART Tree Ensemble)</code>： $T_{1},\cdots,T_{K}$ 。其中 $T_{j}$ 学习的是之前 $j-1$ 棵树预测结果的残差，这种思想就像准备考试前的复习，先做一遍习题册，然后把做错的题目挑出来，再做一次，然后把做错的题目挑出来再做一次，经过反复多轮训练，取得最好的成绩。而模型最后的输出，是一个样本在各个树中输出的结果的和： $\bar{y}=\sum_{k=1}^{K}f_{k}(x)$ 。</p>
<p>和<code>AdaBoost</code>一样，<code>Gradient Boosting</code>也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，<code>AdaBoost</code>是通过提升错分数据点的权重来定位模型的不足而<code>Gradient Boosting</code>是通过算梯度<code>(gradient)</code>来定位模型的不足。因此相比<code>AdaBoost</code>，<code>Gradient Boosting</code>可以使用更多种类的目标函数。</p>
<h4 id="提升树算法"><a href="#提升树算法" class="headerlink" title="提升树算法"></a>提升树算法</h4><p>提升树是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差=真实值-预测值。提升树即是整个迭代过程生成的回归树的累加。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101613543.jpg" alt=""></p>
<p>具体的算法步骤：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101613198.png" alt=""></p>
<h4 id="Gradient-Boosting-Decision-Tree（梯度提升决策树）"><a href="#Gradient-Boosting-Decision-Tree（梯度提升决策树）" class="headerlink" title="Gradient Boosting Decision Tree（梯度提升决策树）"></a>Gradient Boosting Decision Tree（梯度提升决策树）</h4><p>提升树利用加法模型和前向分步算法实现学习的优化过程。当损失函数时平方损失和指数损失函数时，每一步的优化很简单，如平方损失函数学习残差回归树。但对于一般的损失函数，往往每一步优化没那么容易。针对这一问题，<code>Freidman</code>提出了梯度提升算法：利用最速下降的近似方法，<strong>即利用损失函数的负梯度在当前模型的值，作为回归问题中提升树算法的残差的近似值</strong>，拟合一个回归树。</p>
<h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p><strong>原始目标函数</strong></p>
<p>目标函数，可以分为两个部分，一部分是损失函数，一部分是正则（用于控制模型的复杂度）。</p>
<p>对于第 $t$ 颗树，第 $i$ 个样本的，模型的预测值是这样的：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101650959.png" alt=""></p>
<p>进一步，我们可以得到我们的原始目标函数，如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101651061.png" alt=""></p>
<p><strong>损失函数化简</strong></p>
<p><code>XGBoost</code>是前向迭代，我们的重点在于第 $t$ 个树，所以涉及到前 $t-1$ 个树变量或者说参数我们是可以<strong>看做常数</strong>的。所以我们的损失函数进一步可以化为如下，其中一个变化是我们对正则项进行了拆分，变成可前 $t-1$ 项和第 $t$ 项：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101656177.png" alt=""></p>
<h4 id="泰勒公式展开"><a href="#泰勒公式展开" class="headerlink" title="泰勒公式展开"></a>泰勒公式展开</h4><p>使用泰勒公式进行近似展开的核心目标是对目标函数进行化简，将常数项抽离出来。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101658315.png" alt=""></p>
<p>这里 $\Delta x$ 对应的是第 $t$ 棵树的模型 $f_{t}(x_{i})$ ， $x$ 对应的是 $\hat{y}_{i}^{(t-1)}$ ，相应的 $f(x)$ 对应到损失函数应该是 $l(y_{i},\hat{y}_{i}^{(t-1)})+f_{t}(x_{i})$ 。</p>
<p>所以原有公式进行泰勒公式二阶展开，结果为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101704500.png" alt=""></p>
<p>进而我们可以得到目标函数展开公式为如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101704670.png" alt=""></p>
<h4 id="树的参数化"><a href="#树的参数化" class="headerlink" title="树的参数化"></a>树的参数化</h4><p><strong>树模型参数化</strong></p>
<ul>
<li>每棵树每个叶子节点的值（或者说每个叶子节点的权重） $w$ ：这是一个向量，因为每个树有很多叶子节点</li>
<li>样本到叶子节节点的映射关系 $q$ ：告诉每个样本落在当前这个树的哪一个叶子节点上</li>
<li>叶子节点样本归属集合 $I$ ：告诉每个叶子节点包含哪些样本</li>
</ul>
<p><strong>树复杂度参数化</strong></p>
<p>树的复杂度定义如下，其中 $T$ 参数表示当前这棵树叶子节点的个数； $w_{j}^{2}$ 是叶子节点值的 $L_{2}$ 范数：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101716405.png" alt=""></p>
<p>进而我们可以对树进行参数化，带入到目标函数我们可以得到如下式子：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101727004.png" alt=""></p>
<p>最后一步的转化思路是从在这个树中，每个样本落在哪个节点转为了每个节点上有哪些样本。</p>
<p>叶子节点 $j$ 所包含的样本的一阶导数累加之和为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101743580.png" alt=""></p>
<p>叶子节点 $j$ 所包含的样本的二阶导数累加之和为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101744292.png" alt=""></p>
<p>进而我们可以进一步化简为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101744829.png" alt=""></p>
<p>对目标函数对 $w_{j}$ 进行求导就能得出极值点：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101746573.png" alt=""></p>
<h4 id="特征分裂"><a href="#特征分裂" class="headerlink" title="特征分裂"></a>特征分裂</h4><p>对于上述的目标函数，我们仍存在问题，即 $T$ 的取值，也就是如何做特征分裂。</p>
<p><strong>贪心算法</strong></p>
<p>本质上是做两次循环，第一个循环是针对每个特征的每个分割点做一次循环，计算收益，从而选择此特征的最佳分割点。分裂收益使用的是分裂之后的目标函数的变化差值。第二个循环是对样本所有特征的循环，从中挑选出收益最大的特征。</p>
<p>简单说就是首先找到基于每个特征找到收益最大的分割点，然后基于所有特征找到收益最大的特征。</p>
<p><strong>近似算法-分位数候选点</strong></p>
<p>对于每个特征，不去暴力搜索每个值，而是使用分位点</p>
<ul>
<li>根据样本数量选择三分位点或者四分位点等</li>
<li>根据二阶导数（也就是梯度）作为权重进行划分</li>
</ul>
<p>也就是说原来是某个特征的所有取值作为候选点，现在是某个特征的分位点作为候选点。</p>
<h3 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h3><p><code>LightGBM(Light Gradient Boosting Machine)</code>是一种梯度提升框架，它使用决策树作为基学习器。<code>LightGBM</code>为高效并行计算而生，它的<code>Light</code>体现在以下几个点上：</p>
<ul>
<li>更快的训练速度</li>
<li>更低的内存使用</li>
<li>支持单机多线程，多机并行计算，以及<code>GPU</code>训练</li>
<li>能够处理大规模数据</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102112494.png" alt=""></p>
<p>概括来说，<code>lightGBM</code>主要有以下特点：</p>
<ul>
<li>基于<code>Histogram</code>的决策树算法</li>
<li>带深度限制的<code>Leaf-wise</code>的叶子生长策略</li>
<li>直方图做差加速</li>
<li>直接支持类别特征<code>(Categorical Feature)</code></li>
<li><code>Cache</code>命中率优化</li>
<li>基于直方图的稀疏特征优化</li>
<li>多线程优化</li>
</ul>
<h4 id="直方图Histogram算法"><a href="#直方图Histogram算法" class="headerlink" title="直方图Histogram算法"></a>直方图Histogram算法</h4><p><strong>寻找最佳分类点</strong></p>
<p>将连续型特征值放入离散化的箱子<code>(bin)</code>中，然后用这些箱子构建特征直方图。然后模型基于特征直方图寻找最佳分裂点，构建直方图的时间复杂度是 $O(data \times feature)$ ，但寻找最佳分裂点的时间复杂度为 $O(bin \times feature)$ 。模型训练速度会因此而提高，而且因为不需要存储排序索引，内存压力也变小了。<code>LGBM</code>采用的就是直方图算法（现在<code>XGBoost</code>开源代码也支持直方图算法）。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102149708.png" alt=""></p>
<p><strong>直方图差加速</strong></p>
<p>直方图做差是指：“一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的 $k$ 个桶。利用这个方法，<code>LightGBM</code>可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。” 示意图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102152516.png" alt=""></p>
<h4 id="带深度限制的Leaf-wise的叶子生长策略"><a href="#带深度限制的Leaf-wise的叶子生长策略" class="headerlink" title="带深度限制的Leaf-wise的叶子生长策略"></a>带深度限制的Leaf-wise的叶子生长策略</h4><p><code>GBDT</code>与<code>XGBoost</code>模型在叶子生长策略上均采用按层<code>level-wise</code>分裂的方式，这种方式在分裂时会针对同一层的每一个节点，即每次迭代都要遍历整个数据集中的全部数据，这种方式虽然可以使每一层的叶子节点并行完成，并控制模型的复杂度，但也会产生许多不必要搜索或分裂，从而消耗更多的运行内存，增加计算成本。</p>
<p>而<code>LightGBM</code>算法对其进行了改进，使用了按叶子节点<code>leaf-wise</code>分裂的生长方式，即每次是对所有叶子中<strong>分裂增益最大的叶子节点进行分裂</strong>，其他叶子节点则不会分裂。这种分裂方式比按层分裂会带来更小的误差，并且加快算法的学习速度，但由于没有对其他叶子进行分裂，会使得分裂结果不够细化，并且在每层中只对一个叶子不断进行分裂将增大树的深度，造成模型过拟合。因此，<code>LightGBM</code>算法在按叶子节点生长过程中会限制树的深度来避免过拟合。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102156269.png" alt=""></p>
<h4 id="支持类别特征"><a href="#支持类别特征" class="headerlink" title="支持类别特征"></a>支持类别特征</h4><p>实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化<code>one-hot</code>特征，降低了空间和时间的效率。而类别特征的使用是在实践中很常用的。基于这个考虑，<code>LightGBM</code>优化了对类别特征的支持，可以直接输入类别特征，不需要额外的 $0/1$ 展开。并在决策树算法上增加了类别特征的决策规则。决策树在学习节点分裂时，是一种<code>one-vs-rest</code>模式，每次只能根据一个类别做分类，如下图。这种模式效率比较低，而且不利于决策树学习。<code>LightGBM</code>对此进行了优化，采用<code>many-vs-many</code>模式分裂节点，如下图。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102208969.png" alt=""></p>
<h4 id="基于梯度的单边采样-GOSS"><a href="#基于梯度的单边采样-GOSS" class="headerlink" title="基于梯度的单边采样(GOSS)"></a>基于梯度的单边采样(GOSS)</h4><p>在<code>AdaBoost</code>中，样本权重指示了数据样本的重要性，而在<code>GBDT</code>上并没有样本权重这一说，可作者发现：在<code>GBDT</code>中，梯度对于每个样本是个很有用的信息，它可以用来帮助采样。为什么这么说呢？让我们打个比方，如果某样本得到的一个小的梯度值，那么说明该样本的训练误差也小，模型在该样本上表现得就很好，那这些小梯度的样本其实是不是不用参与训练了？就好像准备考试时刷题不刷简单题，这样可以吗？不可以！因为如果真的直接剔除它们，数据分布会改变，从而损害模型的准确率。为了处理这个问题，作者提出了<code>GOSS</code>，<code>GOSS</code>全称是<code>Gradient-based One-Side Sampling</code>单边梯度采样，它保留所有大梯度的样本，然后小梯度样本采用随机采样，在不改变原始数据分布的同时，减小了样本数量，提升了模型的训练速度。</p>
<h4 id="互斥特征绑定"><a href="#互斥特征绑定" class="headerlink" title="互斥特征绑定"></a>互斥特征绑定</h4><p>从特征角度来看，稀疏特征会包含很多 $0$ 元素；从样本角度来看，一个样本的多个稀疏特征经常同时为 $0$ 。<code>EFB(Exclusive Feature Bundling)</code>基于这种想法，对<strong>互斥特征</strong>进行了<strong>捆绑</strong>，整体过程有点类似于<code>One-Hot</code>逆过程。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102216271.png" alt=""></p>
<p><strong>捆哪些特征</strong></p>
<p>尝试每种组合，是<code>NP</code>难问题，现有算力做不到。所以只能采用贪心算法找。具体过程如下：</p>
<ol>
<li>遍历特征，先把第一个特征拿出来作为一个组合</li>
<li>第二个特征往这个组合里放，冲突比例小就放进去合并成一个特征，冲突比例大就单拿出来作为另一个组合</li>
<li>第三个特征继续往已有的组合里放，能放就放，不能放就单成一个新组合</li>
<li>以此类推对所有特征做同样的操作。</li>
</ol>
<p><strong>如何捆绑特征</strong></p>
<p>因为不同特征下的值有不同的量纲，比如：特征<code>A</code>的值范围为 $[0, 10)$ ，特征<code>B</code>的值范围为 $[0, 20)$ ，将特征<code>A</code>和特征<code>B</code>的直方图加起来捆绑一起后，<code>bundle</code>的值范围变为 $[0, 20)$ ，但是我们无法从中辨别哪些是特征<code>A</code>，哪些是特征<code>B</code>。这样对模型是不利的，因为模型这样就没法根据<code>bundle</code>直方图的值范围去很好区分特征，对树生成会带来误差。对于该问题的解决办法就是<strong>加偏移量</strong>，如果我们对特征<code>B</code>加偏移量 $10$ ，特征<code>B</code>的值范围变为 $[10, 30)$ ，合并后的<code>bundle</code>值范围变为 $[0, 30)$ 。</p>
<h3 id="CatBoost"><a href="#CatBoost" class="headerlink" title="CatBoost"></a>CatBoost</h3><p><code>CatBoost</code>主要是在类别特征上的处理上做了很多的改进。从用户使用角度来看，相比<code>XGBoost</code>和<code>LightGBM</code>，<code>CatBoost</code>具有如下特点。</p>
<ul>
<li><strong>模型精度：</strong><code>XGBoost</code>和<code>LightGBM</code>相当，<code>CatBoost</code>往往略好一些，无需调参即可获取很好的结果。</li>
<li><strong>训练速度：</strong><code>LightGBM</code>远快于<code>XGBoost</code>，<code>CatBoost</code>快于<code>XGBoost</code>但比<code>LightGBM</code>慢。</li>
<li><strong>预测速度：</strong><code>LightGBM</code>与<code>XGBoost</code>相当，<code>CatBoost</code>远快于<code>LightGBM</code>与<code>XGBoost</code>，是它们的几十分之一。</li>
<li><strong>内存消耗：</strong><code>LightGBM</code>远小于<code>XGBoost</code>，<code>CatBoost</code>小于<code>XGBoost</code>，但大于<code>LightGBM</code>。</li>
<li><strong>类别特征：</strong><code>XGBoost</code>不支持类别特征，需要<code>One-Hot</code>编码预处理。<code>LightGBM</code>支持类别特征，需转换成整数编码。<code>CatBoost</code>提供更强大的对类别特征的支持，直接支持字符串类型的类别特征，无需预处理。</li>
<li><strong>缺失值特征：</strong><code>XGBoost</code>和<code>LightGBM</code>都可以自动处理特征缺失值，<code>CatBoost</code>不能自动处理缺失值（或者将缺失值视为最小值/最大值）。</li>
<li><strong>GPU支持：</strong><code>LightGBM</code>与<code>CatBoost</code>支持<code>GPU</code>训练，<code>XGBoost</code>也支持<code>GPU</code>训练。</li>
<li><strong>可视化：</strong><code>CatBoost</code>还自带一套可视化工具，可以在<code>Jupyter Notebook</code>或者<code>TensorBoard</code>中实时看到指标变化。</li>
</ul>
<h4 id="基于类别特征的Ordered-Target-Statistics数值编码方法"><a href="#基于类别特征的Ordered-Target-Statistics数值编码方法" class="headerlink" title="基于类别特征的Ordered Target Statistics数值编码方法"></a>基于类别特征的Ordered Target Statistics数值编码方法</h4><p>对于类别特征，如果类别数目不多，可以使用<code>One-Hot</code>编码。但如果类别数量成百上千，使用<code>One-Hot</code>编码会导致特征数量爆炸。<strong>CatBoost设计了一种基于预测目标统计值的方法可以将类别特征转化为数值特征。</strong>先将样本随机打乱，然后每个样本只使用它排序在它前面的样本来计算其类别特征的数值编码。这样就防止了<code>label</code>的泄露，并且能够较为合理地评估这个特征的真实有效性。具体公式表达为： $i \rightarrow \frac{Current\ Count+a\star P}{Max\ Count + a}$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111538347.png" alt=""></p>
<p>对上述例子来说，我们要计算第 $i$ 条数据的<code>label</code>，计算结果就为 $\frac{2+a \star P}{3+a}$ ，如果将第三行的<code>label</code>改为 $1$ ，那么结果就变成了 $\frac{3+a\star P}{3+a}$ 。</p>
<h4 id="基于贪心策略的特征交叉方法"><a href="#基于贪心策略的特征交叉方法" class="headerlink" title="基于贪心策略的特征交叉方法"></a>基于贪心策略的特征交叉方法</h4><p>使用<code>Ordered Target Statistics</code>方法将类别特征转化成为数值特征以后，会影响到特征交叉，因为数值特征无法有效地进行交叉。为了有效地利用特征交叉，<code>CatBoost</code>在将类别特征转换为数值编码的同时，会自动生成交叉特征。但如果让全部的类别特征之间都进行交叉，两两交叉，三三交叉，四四交叉，这个复杂度是指数级的，特征维度一定会爆炸。<code>CatBoost</code>使用一种贪心的策略来进行特征交叉。生成<code>tree</code>的第一次分裂，<code>CatBoost</code>不使用任何交叉特征。在后面的分裂中，<code>CatBoost</code>会使用生成<code>tree</code>所用到的<strong>全部原始特征和交叉特征</strong>跟数据集中的<strong>全部类别特征</strong>进行交叉。</p>
<h4 id="避免预测偏移的Ordered-Boosting方法"><a href="#避免预测偏移的Ordered-Boosting方法" class="headerlink" title="避免预测偏移的Ordered Boosting方法"></a>避免预测偏移的Ordered Boosting方法</h4><p>使用<code>XGBoost</code>或者<code>LightGBM</code>做模型时，我们可能经常会发现模型在训练集上拟合的很好，<code>train_auc</code>甚至达到了 $1.0$ ，但是在验证集上却差了很多，<code>val_auc</code>可能只有 $0.7$ 。这当然有可能是因为<code>tree</code>的数量太多了，或者是每棵<code>tree</code>的<code>leaves</code>太多了，总之模型太复杂了造成了过拟合。</p>
<p>但也有一些<code>XGBoost</code>和<code>LightGBM</code>自身算法的缺陷因素。我们知道<code>LightGBM</code>在训练下一棵<code>tree</code>的时候，需要计算前面这些<code>tree</code>构成的加法模型在所有样本上的一阶梯度和二阶梯度（<code>Loss</code>对模型预测结果的导数），然后用这些梯度来决定下一棵树的结构和叶子节点取值。</p>
<p>但是我们计算的这些一阶梯度和二阶梯度值是有问题的。前面的这些<code>tree</code>都是在这些样本上训练的，现在我们又在这些样本上估计模型预测结果的一阶和二阶梯度。我们应该换一些新的样本才更合理。但是我们从哪里找这些新的样本呢？</p>
<p><code>CatBoost</code>的作者故伎重演。先将样本随机打乱，然后每个样本只使用<strong>排序在它前面的样本</strong>来训练模型。用这样的模型来估计这个样本预测结果的一阶和二阶梯度。然后用这些梯度构建一棵<code>tree</code>的结构，最终<code>tree</code>的每个叶子节点的取值，是使用全体样本进行计算的。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111601603.png" alt=""></p>
<h4 id="使用对称二叉树作为基模型"><a href="#使用对称二叉树作为基模型" class="headerlink" title="使用对称二叉树作为基模型"></a>使用对称二叉树作为基模型</h4><p><code>XGBoost</code>和<code>LightGBM</code>采用的基模型是普通的二叉树，但是<code>CatBoost</code>采用的是对称的二叉树。这种对树结构上的约束有一定的<strong>正则作用</strong>。更为重要的是，它可以让<code>CatBoost</code>模型的推断过程极快。对于<code>CatBoost</code>的<code>tree</code>的预测过程来说，每个特征的分裂都是独立的，不分先后顺序，多个样本可以一起预测。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111619137.jpg" alt=""></p>
<h2 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h2><p>待总结</p>
<h1 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h1><h2 id="逻辑回归为什么使用sigmoid函数"><a href="#逻辑回归为什么使用sigmoid函数" class="headerlink" title="逻辑回归为什么使用sigmoid函数"></a>逻辑回归为什么使用sigmoid函数</h2><ol>
<li>sigmoid函数可以将任何输入转换为概率值输出，这使得逻辑回归可以用于分类问题。通过设置一个阈值，可以决定将输入分类为哪一类。</li>
<li>sigmoid函数具有性质，当函数的自变量趋近于正无穷时，函数值将趋近于1；当函数的自变量趋近于负无穷时，函数值将趋近于0。这一性质与逻辑回归的假设相符，即当事件发生的条件概率相对于其发生概率比无限大时，事件发生的概率为1；当事件发生的条件概率相对于其发生概率比无限小时，事件发生的概率为0。</li>
<li>sigmoid函数的导数计算简单，这使得逻辑回归的模型训练更快。</li>
</ol>
<h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><p>支持向量机<code>(support vector machines，SVM)</code>是一种二分类模型，它将实例的特征向量映射为空间中的一些点，<code>SVM</code>的目的就是想要画出一条线，以 “最好地” 区分这两类点，以至如果以后有了新的点，这条线也能做出很好的分类。<code>SVM</code>适合中小型数据样本、非线性、高维的分类问题。</p>
<p>将实例的特征向量（以二维为例）映射为空间中的一些点，如下图的实心点和空心点，它们属于不同的两类。<code>SVM</code>的目的就是想要画出一条线，以“最好地”区分这两类点，以至如果以后有了新的点，这条线也能做出很好的分类。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071255957.png" alt=""></p>
<p><strong>Q1：能够画出多少条线对样本点进行区分？</strong><br>答：线是有无数条可以画的，区别就在于效果好不好，每条线都可以叫做一个划分超平面。比如上面的绿线就不好，蓝线还凑合，红线看起来就比较好。我们所希望找到的这条效果最好的线就是具有 “最大间隔的划分超平面”。</p>
<p><strong>Q2：为什么要叫作“超平面”呢？</strong><br>答：因为样本的特征很可能是高维的，此时样本空间的划分就不是一条线了。</p>
<p><strong>Q3：画线的标准是什么？/ 什么才叫这条线的效果好？/ 哪里好？</strong><br>答：<code>SVM</code>将会寻找可以区分两个类别并且能使间隔<code>(margin)</code>最大的划分超平面。比较好的划分超平面，样本局部扰动时对它的影响最小、产生的分类结果最鲁棒、对未见示例的泛化能力最强。</p>
<p><strong>Q4：间隔margin是什么？</strong><br>答：对于任意一个超平面，其两侧数据点都距离它有一个最小距离（垂直距离），这两个最小距离的和就是间隔。比如下图中两条虚线构成的带状区域就是<code>margin</code>，虚线是由距离中央实线最近的两个点所确定出来的（也就是由支持向量决定）。但此<code>margin</code>比较小，如果用第二种方式画，<code>margin</code>明显变大也更接近我们的目标。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071300267.png" alt=""></p>
<p><strong>Q5：为什么要让margin尽量大？</strong><br>答：因为大<code>margin</code>犯错的几率比较小，也就是更鲁棒啦。</p>
<p><strong>Q6：支持向量是什么？</strong><br>答：从上图可以看出，虚线上的点到划分超平面的距离都是一样的，实际上只有这几个点共同确定了超平面的位置，因此被称作 “支持向量<code>(support vectors)</code>”，“支持向量机” 也是由此来的。</p>
<p><strong>Q7：SVM 算法特性</strong></p>
<ol>
<li>训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以<code>SVM</code>不太容易产生<code>overfitting</code>。</li>
<li><code>SVM</code>训练出来的模型完全依赖于支持向量，即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果仍然会得到完全一样的模型。</li>
<li>一个<code>SVM</code>如果训练得出的支持向量个数比较少，那么<code>SVM</code>训练出的模型比较容易被泛化。</li>
<li><code>SVM</code>算法对大规模训练样本难以实施。</li>
<li>用<code>SVM</code>解决多分类问题存在困难。</li>
</ol>
<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><h2 id="多尺度"><a href="#多尺度" class="headerlink" title="多尺度"></a>多尺度</h2><p>多尺度，实际上就是对信号的不同粒度的采样。粒度小，说明是一个很密集的采样，能看到更多更多的细节；而粒度大，说明是一个很稀疏的采样，但是点与点之间隔得远了，就容易看到趋势了。通常在不同尺度下我们可以观察到不同的特征，从而完成不同的任务。</p>
<p>例如，我们判断一张图片中是否有前景，那么<code>12×8</code>的图像尺度就够了；如果我们要识别图中的水果种类，那么<code>64×48</code>的图像尺度勉强够用；如果我们要后期合成该图像的景深，则需要更高分辨率的图像，例如<code>640×480</code>。</p>
<h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>卷积的作用是<strong>提取特征</strong>，图像的空间联系是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。同时卷积通过权重共享降低参数量。</p>
<p>常见的卷积核选择都是<code>3x3</code>、<code>5x5</code>、<code>7x7</code>的，为什么很少见到偶数的卷积核呢？</p>
<ul>
<li>其主要原因是为了保护位置信息，使用奇数的卷积核，保证了中心点刚好在中间，避免了位置信息发生偏移。在需要使用位置信息的任务，如目标检测、目标识别、三维重建、图像重建等任务中非常有价值。</li>
<li>另一个就是因为<code>padding</code>时候能够保证左右对称，实际上也是为了位置信息。</li>
</ul>
<h3 id="普通卷积"><a href="#普通卷积" class="headerlink" title="普通卷积"></a>普通卷积</h3><p>在卷积神经网络中我们通常需要输入<code>in_channels</code>和<code>out_channels</code>，即输入通道数和输出通道数。</p>
<p>对于最初输入图片样本的通道数<code>in_channels</code>取决于图片的类型，如果是彩色的，即<code>RGB</code>类型，这时候通道数固定为 $3$ ，如果是灰度图，通道数为 $1$ 。<br>卷积完成之后，输出的通道数<code>out_channels</code>取决于过滤器的数量。从这个方向理解，这里的<code>out_channels</code>设置的就是过滤器的数目。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20230106231925.png" alt=""></p>
<p>如上图，输入的通道数为 $3$ ，所以卷积的时候每个需要对每个通道有个卷积核；输出的通道数为 $4$ ，输出的通道数就是我们设置的过滤器的数目。</p>
<h3 id="分组卷积-group-convolution"><a href="#分组卷积-group-convolution" class="headerlink" title="分组卷积(group convolution)"></a>分组卷积(group convolution)</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303212301932.png" alt=""></p>
<p><strong>我们用同等的参数量运算量生成了g个feature map！！！</strong></p>
<p>所以<code>group convolution</code>常用在轻量型高效网络中，因为它用少量的参数量和运算量就能生成大量的<code>feature map</code>，大量的<code>feature map</code>意味着能够编码更多的信息！</p>
<p>从分组卷积的角度来看，分组数 $g$ 就像一个控制旋钮，最小值是 $1$ ，此时 $g=1$ 的卷积就是普通卷积；最大值是输入<code>feature map</code>的通道数 $C$ ，此时 $g=C$ 的卷积就是<strong>depthwise sepereable convolution</strong>，即深度分离卷积，又叫逐通道卷积。</p>
<h3 id="卷积结果"><a href="#卷积结果" class="headerlink" title="卷积结果"></a>卷积结果</h3><p>假设 $W_{1}$ 、 $H_{1}$ 表示输入的宽度和长度， $W_{2}$ 、 $H_{2}$ 表示输出特征图的的宽度和长度， $F$ 表示卷积核长和宽的大小， $S$ 表示滑动窗口的步长， $P$ 表示边界填充。那么输出特征图的宽度和长度分别为： $W_{2}=\frac{W_{1}-F_{W}+2P}{S}+1$ ， $H_{2}=\frac{H_{1}-F_{H}+2P}{S}+1$ 。</p>
<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化的作用是进行<strong>特征压缩（下采样）</strong>，池化层是当前卷积神经网络中常用组件之一，它最早见于<code>LeNet</code>一文，称之为<code>Subsample</code>。自<code>AlexNet</code>之后采用<code>Pooling</code>命名。池化层是模仿人的视觉系统对数据进行降维，用更高层次的特征表示图像。</p>
<p>实施池化的目的：(1) 降低信息冗余；(2) 提升模型的尺度不变性、旋转不变性；(3) 降低特征维数，防止过拟合。</p>
<h2 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h2><p>padding的主要作用是使得图像边界的特征也能够被充分利用。</p>
<h2 id="CNN中的等变和不变"><a href="#CNN中的等变和不变" class="headerlink" title="CNN中的等变和不变"></a>CNN中的等变和不变</h2><p>简单的来说，CNN中的卷积操作中的参数共享使得它对平移操作有等变性，而一些池化操作对平移有近似不变性。先来说前者， 我们举个很简单的例子，我们都知道CNN的第一层往往可以解释为一些简单的线条处理，比如竖直/水平线条检测等等，那么如果图像平移，显然并不会影响到这一层线条检测的功能，但是其输出也会做相应平移。后者的之所以说是近似不变性，是因为池化层并非能保持完全不变，例如我们使用max池化，只要变换不影响到最大值，我们的池化结果不会收到影响，对于一个NxN的filter，只有一个值的变动会影响到输出， 其他的变换都不会造成扰动。 平均池化的近似不变性就稍弱些。这里池化的其实是一个非常强的先验，等于是忽视了这一步维数约简带来的信息损失而保证了近似不变性。</p>
<p>我们换个角度来说，CNN是既具有不变性，又具有等变性。 可以这么理解，如果我们的输出是给出图片中猫的位置，那么我们将图片中的猫从左边移到右边，这种平移也会反应在输出上，我们输出的位置也是从左边到右边，那么我们则可以说CNN有等变性；如果我们只是输出图片中是否有猫，那么我们无论把猫怎么移动，我们的输出都保持”有猫”的判定，因此体现了CNN的不变性。</p>
<h3 id="等变性"><a href="#等变性" class="headerlink" title="等变性"></a>等变性</h3><p>等变性 equivariant，对于一个函数，如果你对其输入施加的变换也会同样反应在输出上，那么这个函数就对该变换具有等变性。</p>
<h3 id="不变性"><a href="#不变性" class="headerlink" title="不变性"></a>不变性</h3><p>不变性 invraiant，对于一个函数，如果对其输入施加的某种操作丝毫不会影响到输出，那么这个函数就对该变换具有不变性。</p>
<h2 id="神经网络中权值共享的理解？"><a href="#神经网络中权值共享的理解？" class="headerlink" title="神经网络中权值共享的理解？"></a>神经网络中权值共享的理解？</h2><p>所谓权值共享就是说给定一张输入图片，用一个卷积核来卷积这张图，卷积核里的值叫做权重，这张图的每个位置是被同一个卷积核扫的，即卷积的时候所用的权重是一样的。其实权值共享这个词说全了就是整张图片在使用同一个卷积核内的参数，比如一个 $3\times 3 \times 1$ 的卷积核，这个卷积核内 $9$ 个的参数被整张图共享，而不会因为图像内位置的不同而改变卷积核内的权系数。说的再直白一些，就是用一个卷积核不改变其内权系数的情况下卷积处理整张图片（当然<code>CNN</code>中每一层不会只有一个卷积核的，这样说只是为了方便解释而已）。<br>作用：大大减少网络训练参数的同时，还可以实现并行训练。</p>
<h2 id="对微调-fine-tuning-的理解，为什么要修改最后几层神经网络权值？"><a href="#对微调-fine-tuning-的理解，为什么要修改最后几层神经网络权值？" class="headerlink" title="对微调(fine-tuning)的理解，为什么要修改最后几层神经网络权值？"></a>对微调(fine-tuning)的理解，为什么要修改最后几层神经网络权值？</h2><p>使用预训练模型的好处，在于利用训练好的<code>SOTA</code>模型权重去做特征提取，可以节省我们训练模型和调参的时间。</p>
<p>为什么只微调最后几层神经网络权重，是因为：</p>
<ol>
<li><code>CNN</code>中更靠近底部的层（定义模型时先添加到模型中的层）编码的是更加通用的可复用特征，而更靠近顶部的层（最后添加到模型中的层）编码的是更专业化的特征。微调这些更专业化的特征更加有用，它更代表了新数据集上的有用特征。</li>
<li>训练的参数越多，过拟合的风险越大。很多<code>SOTA</code>模型拥有超过千万的参数，在一个不大的数据集上训练这么多参数是有过拟合风险的，除非你的数据集像<code>ImageNet</code>那样大。</li>
</ol>
<h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><p>优点：这种网络当时提出来成为了CNN标准的“模板”——叠加卷积层和池化层，并以一个全连接层结束网络。</p>
<p>缺点：当时一段时间并未火起来，原因在于当时历史背景，这个简单的网络仅有 $6000$ 多个参数，但训练起来费时且没有<code>GPU</code>加速，相比较于传统的<code>SVM</code>等算法，效率还是差了许多，所以并没有大放异彩。</p>
<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><ul>
<li>使用<code>ReLU</code>激活函数，不容易发生梯度消失问题。</li>
<li>对输入的数据进行了数据增强处理。（水平变换、光照增强、随机裁剪、平移变换等等）</li>
<li>首次使用<code>Dropout</code>防止过拟合。</li>
<li>采用两块<code>GPU</code>并行计算，每一层分两块进行计算，所以看着比较繁琐，这里也是由于<code>GPU</code>不是很好，所以要两块并行。</li>
</ul>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p><code>2012</code>年，<code>Hinton</code>在其论文中提出<code>Dropout</code>。当一个复杂的前馈神经网络被训练在小的数据集时，容易造成过拟合。为了防止过拟合，可以通过阻止特征检测器的共同作用来提高神经网络的性能。<br><code>Droupout</code>是一种针对深度学习广泛应用的<strong>正则化技术</strong>。在每次迭代时随机关闭一些神经单元，随着迭代的进行，由于其他神经元可能在任何时候都被关闭，因此神经元对其他特定神经元的激活变得不那么敏感。</p>
<p>经过上面屏蔽掉某些神经元，使其激活值为 $0$ 以后，我们还需要对向量 $y_{1},\cdots,y_{n}$ <strong>进行缩放</strong>，也就是乘以 $1/(1-p)$ ，此方法为<code>invert Dropout</code>；如果你在训练的时候，经过置 $0$ 后，没有对 $y_{1}\cdots y_{n}$ 进行<code>rescale</code>，那么在测试的时候，就需要对权重进行缩放，即对每个神经元的权重都乘以一个 $p$ ，这样在“总体上”使得测试数据和训练数据是大致一样的，此方法为<code>vanilla Dropout</code>。比如一个神经元的输出是 $x$ ，那么在训练的时候它有 $p$ 的概率参与训练， $(1-p)$ 的概率丢弃，那么它输出的期望是 $p\cdot x+ (1-p)\cdot 0=p\cdot x$。因此测试的时候把这个神经元 $d$ 的权重乘以 $p$ 可以得到同样的期望。</p>
<h4 id="为什么说Dropout可以解决过拟合？"><a href="#为什么说Dropout可以解决过拟合？" class="headerlink" title="为什么说Dropout可以解决过拟合？"></a>为什么说Dropout可以解决过拟合？</h4><ol>
<li>取平均的作用。<code>Dropout</code>掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个<code>Dropout</code>过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</li>
<li>减少神经元之间复杂的共适应关系：因为<code>Dropout</code>导致两个神经元不一定每次都在一个<code>Dropout</code>网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。</li>
<li><code>Dropout</code>类似于性别在生物进化中的角色：物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。</li>
</ol>
<h4 id="Dropout缺点"><a href="#Dropout缺点" class="headerlink" title="Dropout缺点"></a>Dropout缺点</h4><p>明确定义的损失函数每一次迭代都会下降，而<code>Dropout</code>每一次都会随机删除节点，也就是说每一次训练的网络都是不同的，损失函数不再被明确地定义，在某种程度上很难计算，我们失去了调试工具。</p>
<h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><p>全部使用<code>3×3</code>卷积核的堆叠，来模拟更大的感受野，并且网络层数更深。<code>VGG</code>有五段卷积，每段卷积后接一层最大池化。卷积核数目逐渐增加。</p>
<p><strong>作者用的是多个3×3卷积叠加，而不是例如7×7、11×11的单个卷积，原因如下：</strong></p>
<ul>
<li><code>2</code>个<code>3×3</code>卷积叠加得到的理论感受野和一个<code>5×5</code>卷积的理论感受野是相同的，<code>3</code>个<code>3×3</code>卷积叠加得到的理论感受野和一个<code>7×7</code>卷积的理论感受野是相同的。</li>
<li>因为每个卷积层后面都会跟着一个<code>ReLU</code>，<code>3</code>个<code>3×3</code>卷积就会有<code>3</code>个<code>ReLU</code>，但是一个<code>7×7</code>的卷积只有一个，所以这么做可以使得模型的非线性拟合能力更强。</li>
<li>减少了参数数量。假设<code>3</code>个<code>3×3</code>卷积的输入和输出都是<code>C</code>个通道，那么参数数量为 $3 \times 3 \times 3 \times C \times C = 27C^{2}$  ，而<code>7×7</code>卷积的参数数量为 $7 \times 7 \times C \times C = 49C^{2}$ 。</li>
</ul>
<p><strong>1×1卷积的作用是什么？</strong></p>
<ul>
<li>为了在不影响卷积层感受野的前提下，增加模型的非线性。</li>
<li>可以压缩通道数，即减少特征的维度。</li>
</ul>
<h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p><code>GoogLeNet</code>专注于加深网络结构，与此同时引入了新的基本结构——<code>Inception</code>模块，从而来增加网络的宽度。每个原始<code>Inception</code>模块由<code>previous layer</code>、并行处理层及<code>filter concatenation</code>层组成。并行处理层包含 $4$ 个分支，即<code>1×1</code>卷积分支，<code>3×3</code>卷积分支，<code>5×5</code>卷积分支和<code>3×3</code>最大池化分支。一个关于原始<code>Inception</code>模块的最大问题是，<code>5×5</code>卷积分支即使采用中等规模的卷积核个数，在计算代价上也可能是无法承受的。这个问题在混合池化层之后会更为突出，很快的出现计算量的暴涨。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222128650.png" alt=""></p>
<p>为了克服原始<code>Inception</code>模块上的困难，<code>GoogLeNet</code>推出了一个新款，即采用<code>1×1</code>的卷积层来降低输入层的维度，使网络参数减少，因此减少网络的复杂性。因此得到降维<code>Inception</code>模块，称为<code>inception V1</code>。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222129537.png" alt=""></p>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><h3 id="ResNet中的一些亮点"><a href="#ResNet中的一些亮点" class="headerlink" title="ResNet中的一些亮点"></a>ResNet中的一些亮点</h3><ol>
<li>超深的网络结构（超过 $1000$ 层）。</li>
<li>提出<code>residual</code>（残差结构）模块。</li>
<li>使用<code>Batch Normalization</code>加速训练（丢弃<code>Dropout</code>）。</li>
</ol>
<h3 id="为什么采用residual"><a href="#为什么采用residual" class="headerlink" title="为什么采用residual?"></a>为什么采用residual?</h3><p>人们认为卷积层和池化层的层数越多，获取到的图片特征信息越全，学习效果也就越好。但是在实际的试验中发现，随着卷积层和池化层的叠加，不但没有出现学习效果越来越好的情况，反而两种问题：</p>
<ol>
<li>梯度消失和梯度爆炸<br>梯度消失：若每一层的误差梯度小于 $1$ ，反向传播时，网络越深，梯度越趋近于 $0$<br>梯度爆炸：若每一层的误差梯度大于 $1$ ，反向传播时，网络越深，梯度越来越大</li>
<li>退化问题<br>随着层数的增加，预测效果反而越来越差。</li>
</ol>
<ul>
<li>为了解决梯度消失或梯度爆炸问题，<code>ResNet</code>论文提出通过数据的预处理以及在网络中使用 <code>BN(Batch Normalization)</code>层来解决。</li>
<li>为了解决深层网络中的退化问题，可以人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系。这种神经网络被称为残差网络<code>(ResNets)</code>。<code>ResNet</code>论文提出了<code>residual</code>结构（残差结构）来减轻退化问题，随着网络的不断加深，效果并没有变差，而是变的更好了。</li>
</ul>
<h3 id="residual结构"><a href="#residual结构" class="headerlink" title="residual结构"></a>residual结构</h3><h4 id="residual的计算方式"><a href="#residual的计算方式" class="headerlink" title="residual的计算方式"></a>residual的计算方式</h4><p><code>residual</code>结构使用了一种<code>shortcut</code>的连接方式，也可理解为捷径。让特征矩阵隔层相加，注意 $\mathcal{F}(\mathbb{x})$ 和 $\mathbb{x}$ 形状要相同，所谓相加是特征矩阵相同位置上的数字进行相加。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317819.png" alt=""></p>
<h4 id="ResNet中两种不同的residual"><a href="#ResNet中两种不同的residual" class="headerlink" title="ResNet中两种不同的residual"></a>ResNet中两种不同的residual</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317983.png" alt=""></p>
<ol>
<li>左侧残差结构称为<code>BasicBlock</code></li>
<li>右侧残差结构称为<code>Bottleneck</code><ul>
<li>其中第一层的 $1\times1$ 的卷积核的作用是对特征矩阵进行降维操作，将特征矩阵的深度由 $256$ 降为 $64$ ；</li>
<li>第三层的 $1\times1$ 的卷积核是对特征矩阵进行升维操作，将特征矩阵的深度由 $64$ 升成 $256$ 。<br>降低特征矩阵的深度主要是为了减少参数的个数。<br>如果采用<code>BasicBlock</code>，参数的个数应该是： $256\times256\times3\times3\times2=1179648$<br>采用<code>Bottleneck</code>，参数的个数是： $1\times1\times256\times64+3\times3\times64\times64+1\times1\times256\times64=69632$</li>
<li>先降后升为了主分支上输出的特征矩阵和捷径分支上输出的特征矩阵形状相同，以便进行加法操作。</li>
</ul>
</li>
</ol>
<h3 id="BatchNormalization"><a href="#BatchNormalization" class="headerlink" title="BatchNormalization"></a>BatchNormalization</h3><p><strong>BatchNormalization就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布。</strong></p>
<p>在神经网络中, 数据分布对训练会产生影响。比如某个神经元 $x$ 的值为 $1$ ，某个 <code>Weights</code> 的初始值为 $0.1$ ，这样后一层神经元计算结果就是 $Wx = 0.1$ ；又或者 $x = 20$ ，这样 $Wx$ 的结果就为 $2$ 。现在还不能看出什么问题,，但是，当我们加上一层激活函数，激活这个  $Wx$ 值的时候，问题就来了。如果使用像 <code>tanh</code> 的激活函数， $Wx$ 的激活值就变成了 $\approx 0.1$ 和 $\approx 1$, 接近于 $1$ 的部已经处在了激活函数的饱和阶段, 也就是 $x$ 无论再怎么扩大， <code>tanh</code> 激励函数输出值也还是接近 $1$ 。</p>
<p>我们为了避免这种情况，就会对数据进行归一化，<strong>对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0​，方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。</strong>同时了为了恢复出原始的某一层所学到的特征，我们引入了这个可学习重构参数 $\gamma$ 、$\beta$ ，让我们的网络可以学习恢复出原始网络所要学习的特征分布。</p>
<p>均值的计算，就是在一个批次内，将每个通道中的数字单独加起来，再除以 $N \times H \times W$。举个例子：该批次内有10张图片，每张图片有三个通道RBG，每张图片的高、宽是H、W，那么均值就是计算<strong>10张图片R通道的像素数值总和</strong>除以 $10 \times H \times W$，再计算B通道全部像素值总和除以 $10 \times H \times W$，最后计算G通道的像素值总和除以 $10 \times H \times W$。方差的计算类似。</p>
<h3 id="LayerNormalization"><a href="#LayerNormalization" class="headerlink" title="LayerNormalization"></a>LayerNormalization</h3><ul>
<li>BN在mini-batch较小的情况下不太适用。BN是对整个mini-batch的样本统计均值和方差，当训练样本数很少时，样本的均值和方差不能反映全局的统计分布信息，从而导致效果下降。</li>
<li>BN无法应用于RNN。<ul>
<li>RNN实际是共享的MLP，在时间维度上展开，每个step的输出是(bsz, hidden_dim)。由于不同句子的同一位置的分布大概率是不同的，所以应用BN来约束是没意义的。注：而BN应用在CNN可以的原因是同一个channel的特征图都是由同一个卷积核产生的。</li>
<li>LN原文的说法是：在训练时，对BN来说需要保存每个step的统计信息（均值和方差）。在测试时，由于变长句子的特性，测试集可能出现比训练集更长的句子，所以对于后面位置的step，是没有训练的统计量使用的。（不过实践中的话都是固定了max len，然后padding的。）</li>
<li>当然还有一种说法是，不同句子的长度不一样，对所有的样本统计均值是无意义的，因为某些样本在后面的timestep时其实是padding。</li>
<li>还有一种说法是（Normalization helps training of quantized lstm.）：应用BN层的话，每个timestep都需要去保存和计算batch统计量，耗时又耗力，后面就有人提出across timestep去shared BN的统计量，这明显不对，因为不同timestep的分布明显是不同的。</li>
<li>最后，大家发现LN的效果还很不错，比BN好，所以就变成NLP data里面的default config了。</li>
</ul>
</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306151052109.png" alt=""></p>
<h3 id="ResNet为什么不用Dropout？"><a href="#ResNet为什么不用Dropout？" class="headerlink" title="ResNet为什么不用Dropout？"></a>ResNet为什么不用Dropout？</h3><p><code>Dropout</code>与<code>BN</code>不兼容。<code>BN</code>在训练过程对每个单个样本的<code>forward</code>均引入多个样本的统计信息，相当于自带一定噪音，起到正则效果，所以也就基本消除了<code>Dropout</code>的必要。</p>
<h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p>每个层从前面的所有层获得额外的输入，并将自己的特征映射传递到后续的所有层，使用级联<code>(Concatenation)</code>方式，每一层都在接受来自前几层的“集体知识”<code>(collective knowledge)</code>。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222332494.png" alt=""></p>
<p>如图所示，第<code>i</code>层的输入不仅与<code>i-1</code>层的输出相关，还有所有之前层的输出有关，记作： $X_{l}=H_{l}([X_{0},\cdots,X_{l-1}])$ 。第<code>l</code>层产生 $k_{0}+(l-1)k$ 个<code>feature maps</code>，其中 $k$ 称为网络的增长率。</p>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>循环神经网络<code>(Recurrent Neural Network, RNN)</code>是一类以序列<code>(sequence)</code>数据为输入，在序列的演进方向进行递归<code>(recursion)</code>且所有节点（循环单元）按链式连接的递归神经网络<code>(recursive neural network)</code> 。</p>
<p>对循环神经网络的研究始于二十世纪八九十年代，并在二十一世纪初发展为深度学习算法之一 ，其中双向循环神经网络<code>(Bidirectional RNN, Bi-RNN)</code>和长短期记忆网络<code>(Long Short-Term Memory networks, LSTM)</code>是常见的循环神经网络。</p>
<h2 id="为什么需要RNN？"><a href="#为什么需要RNN？" class="headerlink" title="为什么需要RNN？"></a>为什么需要RNN？</h2><p>在<code>CNN</code>网络中的训练样本的数据为<code>IID</code>数据（独立同分布数据），所解决的问题也是分类问题或者回归问题或者是特征表达问题。<strong>但更多的数据是不满足IID的</strong>，如语言翻译，自动文本生成。它们是一个序列问题，包括时间序列和空间序列。比如时间序列数据，这类数据是在不同时间点上收集到的数据，反映了某一事物、现象等随时间的变化状态或程度。一般的神经网络，在训练数据足够、算法模型优越的情况下，给定特定的 $x$ ，就能得到期望 $y$ 。其一般处理单个的输入，前一个输入和后一个输入完全无关，但实际应用中，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。 这时就要用到<code>RNN</code>网络，<code>RNN</code>的结构图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071328562.png" alt=""></p>
<h2 id="RNN的主要应用领域"><a href="#RNN的主要应用领域" class="headerlink" title="RNN的主要应用领域"></a>RNN的主要应用领域</h2><p>可以说只要考虑时间先后顺序的问题都可以使用<code>RNN</code>来解决，这里主要说一下几个常见的应用领域：</p>
<ul>
<li>自然语言处理<code>(NLP)</code>：主要有视频处理，文本生成，语言模型，图像处理。</li>
<li>机器翻译，机器写文章。</li>
<li>语音识别。</li>
<li>图像描述生成。</li>
<li>文本相似度计算。</li>
<li>推荐系统。例如：音乐推荐、网易考拉商品推荐、<code>Youtube</code>视频推荐等新的应用领域。</li>
</ul>
<h2 id="RNN的计算过程"><a href="#RNN的计算过程" class="headerlink" title="RNN的计算过程"></a>RNN的计算过程</h2><p><code>RNN</code>引入了隐状态 $h$ ， $h$ 可对序列数据提取特征，接着再转换为输出。首先我们计算 $h_{1}$ ：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071332166.jpg" alt=""></p>
<p><code>RNN</code>中，每个步骤使用的参数 $U,W,b$ 相同， $h_{2},h_{3},h_{4}$ 的计算方式和 $h_{1}$ 类似，其计算结果如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071333924.jpg" alt=""></p>
<p>接下来，计算<code>RNN</code>的输出 $y_1$ ，采用<code>Softmax</code>作为激活函数，根据 $y_n=f(Wx+b)$ ，得到 $y_1$ :</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071335563.jpg" alt=""></p>
<p>使用和 $y_1$ 相同的参数 $V,c$ ，得到 $y_2,y_3,y_4$ 的输出结构：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071336003.jpg" alt=""></p>
<h2 id="RNN的建模方式"><a href="#RNN的建模方式" class="headerlink" title="RNN的建模方式"></a>RNN的建模方式</h2><h3 id="一对多-vector-to-sequence"><a href="#一对多-vector-to-sequence" class="headerlink" title="一对多(vector-to-sequence)"></a>一对多(vector-to-sequence)</h3><p>输入是一个单独的值，输出是一个序列。此时，有两种主要建模方式：</p>
<p>方式一：可只在其中的某一个序列进行计算，比如序列第一个进行输入计算，其建模方式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339814.jpg" alt=""></p>
<p>方式二：把输入信息 $X$ 作为每个阶段的输入，其建模方式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339457.jpg" alt=""></p>
<h3 id="多对一-sequence-to-vector"><a href="#多对一-sequence-to-vector" class="headerlink" title="多对一(sequence-to-vector)"></a>多对一(sequence-to-vector)</h3><p>输入是一个序列，输出是一个单独的值，此时通常在最后的一个序列上进行输出变换，其建模如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071340668.jpg" alt=""></p>
<h3 id="多对多-Encoder-Decoder"><a href="#多对多-Encoder-Decoder" class="headerlink" title="多对多(Encoder-Decoder)"></a>多对多(Encoder-Decoder)</h3><p><strong>步骤一</strong>：将输入数据编码成一个上下文向量 $c$ ，这部分称为<code>Encoder</code>，得到 $c$ 有多种方式，最简单的方法就是把<code>Encoder</code>的最后一个隐状态赋值给 $c$ ，还可以对最后的隐状态做一个变换得到 $c$ ，也可以对所有的隐状态做变换。其示意如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071341013.jpg" alt=""></p>
<p><strong>步骤二</strong>：用另一个<code>RNN</code>网络（我们将其称为<code>Decoder</code>）对其进行编码。</p>
<p>方法一是将步骤一中的 $c$ 作为初始状态输入到<code>Decoder</code>，示意图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071342703.jpg" alt=""></p>
<p>方法二是将 $c$ 作为<code>Decoder</code>的每一步输入，示意图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071343619.jpg" alt=""></p>
<h2 id="RNN中为什么会出现梯度消失？如何解决？"><a href="#RNN中为什么会出现梯度消失？如何解决？" class="headerlink" title="RNN中为什么会出现梯度消失？如何解决？"></a>RNN中为什么会出现梯度消失？如何解决？</h2><p><strong>梯度消失的原因：</strong><code>Sigmoid</code>函数的导数范围是 $(0,0.25]$，<code>tanh</code>函数的导数范围是 $(0,1]$ ，他们的导数最大都不大于 $1$ ，如果取<code>tanh</code>或<code>Sigmoid</code>函数作为激活函数嵌套到<code>RNN</code>中，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于 $0$ ，这就是“梯度消失“现象。实际使用中，会优先选择<code>tanh</code>函数，原因是<code>tanh</code>函数相对于<code>Sigmoid</code>函数来说梯度较大，收敛速度更快且引起梯度消失更慢。</p>
<p> <strong>解决RNN中的梯度消失方法主要有：</strong></p>
<ol>
<li>选取更好的激活函数，如<code>ReLU</code>激活函数。<code>ReLU</code>函数的左侧导数为 $0$ ，右侧导数恒为 $1$ ，这就避免了“梯度消失“的发生。但恒为 $1$ 的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。</li>
<li>加入<code>BN</code>层，其优点包括可加速收敛、控制过拟合，可以少用或不用<code>Dropout</code>和正则、降低网络对初始化权重不敏感，且能允许使用较大的学习率等。</li>
<li>改变传播结构，选择更高级的模型，例如：<code>LSTM</code>结构可以有效解决这个问题。</li>
</ol>
<h2 id="RNN的注意力机制"><a href="#RNN的注意力机制" class="headerlink" title="RNN的注意力机制"></a>RNN的注意力机制</h2><p>在上述的<code>Encoder-Decoder</code>结构中，<code>Encoder</code>把所有的输入序列都编码成一个统一的语义特征 $c$ 再解码。因此， $c$ 中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。如机器翻译问题，当要翻译的句子较长时，一个 $c$ 可能存不下那么多信息，就会造成翻译精度的下降。<code>Attention</code>机制通过在每个时间输入不同的 $c$ 来解决此问题。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071356215.png" alt=""></p>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><h2 id="传统RNN存在的问题"><a href="#传统RNN存在的问题" class="headerlink" title="传统RNN存在的问题"></a>传统RNN存在的问题</h2><p><strong>长期依赖(Long Term Dependencies)</strong></p>
<p>在深度学习领域中（尤其是<code>RNN</code>），“长期依赖”问题是普遍存在的。长期依赖产生的原因是当神经网络的节点经过许多阶段的计算后，之前比较长的时间片的特征已经被覆盖，例如：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101147762.png" alt=""></p>
<p>我们想预测<code>full</code>之前系动词的单复数情况，显然<code>full</code>是取决于第二个单词<code>cat</code>的单复数情况，而非其前面的单词<code>food</code>。随着数据时间片的增加，<code>RNN</code>丧失了学习连接如此远的信息的能力。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101148082.png" alt=""></p>
<p><strong>梯度消失/爆炸</strong></p>
<p>梯度消失和梯度爆炸是困扰<code>RNN</code>模型训练的关键原因之一，产生梯度消失和梯度爆炸是由于<code>RNN</code>的权值矩阵循环相乘导致的，相同函数的多次组合会导致极端的非线性行为。梯度消失和梯度爆炸主要存在<code>RNN</code>中，因为<code>RNN</code>中每个时间片使用相同的权值矩阵。对于一个<code>DNN</code>，虽然也涉及多个矩阵的相乘，但是通过精心设计权值的比例可以避免梯度消失和梯度爆炸的问题。</p>
<p>处理梯度爆炸可以采用梯度截断的方法。所谓梯度截断是指将梯度值超过阈值 $\theta$ 的梯度手动降到 $\theta$ 。虽然梯度截断会一定程度上改变梯度的方向，但梯度截断的方向依旧是朝向损失函数减小的方向。</p>
<p>对比梯度爆炸，梯度消失不能简单的通过类似梯度截断的阈值式方法来解决，因为长期依赖的现象也会产生很小的梯度。在上面例子中，我们希望 $t_{9}$ 时刻能够读到 $t_{1}$ 时刻的特征，在这期间内我们自然不希望隐层节点状态发生很大的变化，所以 $[t_{2},t_{8}]$ 时刻的梯度要尽可能的小才能保证梯度变化小。很明显，如果我们刻意提高小梯度的值将会使模型失去捕捉长期依赖的能力。</p>
<h2 id="LSTM-1"><a href="#LSTM-1" class="headerlink" title="LSTM"></a>LSTM</h2><p><code>LSTM</code>的全称是<code>Long Short Term Memory</code>，顾名思义，它具有记忆长短期信息的能力的神经网络。<code>LSTM</code>提出的动机是为了解决上面我们提到的长期依赖问题。传统的<code>RNN</code>节点输出仅由权值，偏置以及激活函数决定。<code>RNN</code>是一个链式结构，每个时间片使用的是相同的参数。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101504342.png" alt=""></p>
<p>而<code>LSTM</code>之所以能够解决<code>RNN</code>的长期依赖问题，是因为<code>LSTM</code>引入了门<code>(gate)</code>机制用于控制特征的流通和损失。对于上面的例子，<code>LSTM</code>可以做到在 $t_{9}$ 时刻将 $t_{2}$ 时刻的特征传过来，这样就可以非常有效的判断 $t_{9}$ 时刻使用单数还是复数了。<code>LSTM</code>是由一系列<code>LSTM</code>单元<code>(LSTM Unit)</code>组成，其链式结构如下图。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101504156.png" alt=""></p>
<p><code>LSTM</code>的核心部分是在上图最上边类似于传送带的部分，这一部分一般叫做单元状态<code>(cell state)</code>它自始至终存在于<code>LSTM</code>的整个链式系统中。其中 $C_{t}=f_{t} \times C_{t-1} + i_{t} \times \tilde{C}_{t}$ ，其中 $f_{t}$ 叫做遗忘门，表示 $C_{t-1}$ 的哪些特征被用于计算 $C_{t}$ 。 $f_{t}$ 是一个向量，向量的每个元素均位于 $[0,1]$ 范围内。通常我们使用<code>Sigmoid</code>作为激活函数，<code>Sigmoid</code>的输出是一个介于 $[0,1]$ 区间内的值，但是当你观察一个训练好的<code>LSTM</code>时，你会发现门的值绝大多数都非常接近 $0$ 或者 $1$ ，其余的值少之又少。其中 $\otimes$ 是<code>LSTM</code>最重要的门机制，表示 $f_{t}$ 和 $C_{t-1}$ 之间的单位乘的关系。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101539655.png" alt=""></p>
<p> $\tilde{C}_{t}$ 表示单元状态更新值，由输入数据 $x_{t}$ 和隐节点 $h_{t-1}$ 经由一个神经网络层得到，单元状态更新值的激活函数通常使用<code>tanh</code>。 $i_{t}$ 叫做输入门，同 $f_{t}$ 一样也是一个元素介于 $[0,1]$ 区间内的向量，同样由 $x_{t}$ 和 $h_{t-1}$ 经由<code>Sigmoid</code>激活函数计算而成。 $i_{t}$ 用于控制 $\tilde{C}_{t}$ 的哪些特征用于更新 $C_{t}$ ，使用方式和 $f_t$ 相同。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101545859.png" alt=""></p>
<p>最后，为了计算预测值 $\hat{y}_{t}$ 和生成下个时间片完整的输入，我们需要计算隐节点的输出 $h_{t}$，这里有一个输出门，用来控制新的细胞状态 $C_{t}$ 有多少用于输出。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101553212.png" alt=""></p>
<h2 id="LSTM的激活函数选取"><a href="#LSTM的激活函数选取" class="headerlink" title="LSTM的激活函数选取"></a>LSTM的激活函数选取</h2><p>算门的时候使用sigmoid，算值的时候使用tanh（可以替换成其他的）。</p>
<ul>
<li>Sigmoid的输出在0-1之同，符合门控的物理定义，且当输入较大或较小时，其输出会非常接近1或0，从而保证该门开或关；</li>
<li>使用tanh函数，是因为其输出在-1-1之间，这与大多数场景下特征分布是0中心的吻合；</li>
<li>tanh函数在输入为0附近相比 Sigmoid函数有更大的梯度，通常使模型收敛更快。</li>
</ul>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p><code>Transformer</code>是一个利用注意力机制来提高模型训练速度的模型。<code>Transformer</code>可以说是完全基于自注意力机制的一个深度学习模型，因为它适用于并行化计算，和它本身模型的复杂程度导致它在精度和性能上都要高于之前流行的<code>RNN</code>循环神经网络。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302227637.png" alt=""></p>
<p>当我输入一个文本的时候，该文本数据会先经过一个叫<code>Encoders</code>的模块，对该文本进行编码，然后将编码后的数据再传入一个叫<code>Decoders</code>的模块进行解码，解码后就得到了翻译后的文本，对应的我们称<code>Encoders</code>为编码器，<code>Decoders</code>为解码器。一般情况下，<code>Encoders</code>里边有 $6$ 个小编码器<code>(Encoder)</code>，<code>Decoders</code>里边有 $6$ 个小解码器<code>(Decoder)</code>。我们看到，在编码部分，每一个的小编码器的输入是前一个小编码器的输出，而每一个小解码器的输入不光是它的前一个解码器的输出，还包括了整个编码部分的输出。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302042116.png" alt=""></p>
<h2 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h2><p>Encoder输入源语言序列，Decoder里面输入需要被翻译的语言文本。一个文本常有许多序列组成，常见操作为将序列进行一些预处理（如词切分等）变成列表，一个序列的列表的元素通常为词表中不可切分的最小词，整个文本就是一个大列表，元素为一个个由序列组成的列表。如一个序列经过切分后变为[“am”, “##ro”, “##zi”, “meets”, “his”, “father”]，接下来按照它们在词表中对应的索引进行转换，假设结果如[23, 94, 13, 41, 27, 96]。假如整个文本一共100个句子，那么就有100个列表为它的元素，因为每个序列的长度不一，需要设定最大长度，这里不妨设为128，那么将整个文本转换为数组之后，形状即为100 x 128，这就对应着batch_size和seq_length。</p>
<p>输入之后，紧接着进行词嵌入处理，词嵌入就是将每一个词用预先训练好的向量进行映射。词嵌入在torch里基于<code>torch.nn.Embedding</code>实现，实例化时需要设置的参数为词表的大小和被映射的向量的维度比如<code>embed = nn.Embedding(10,8)</code>。向量的维度通俗来说就是向量里面有多少个数。注意，第一个参数是词表的大小，如果你目前最多有8个词，通常填写10（多一个位置留给unk和pad），你后面万一进入与这8个词不同的词就映射到unk上，序列padding的部分就映射到pad上。</p>
<p>假如我们打算映射到8维（num_features或者embed_dim），那么整个文本的形状变为100 x 128 x 8。接下来举个小例子解释一下：假设我们词表一共有10个词（算上unk和pad），文本里有2个句子，每个句子有4个词，每个词在词表中都有一个index，然后我们通过index取embed中对应的向量，就可以把每个词映射到8维的向量。</p>
<h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><p>Self-Attention属于Attention，要求QKV必须同源，依然代表X，本质上可以看作是相等的，只是对同一个词向量X乘上了参数矩阵，作了空间上的变换。</p>
<h3 id="Cross-attention"><a href="#Cross-attention" class="headerlink" title="Cross-attention"></a>Cross-attention</h3><ul>
<li>Transformer架构中混合两种不同嵌入序列的注意机制</li>
<li>两个序列<strong>必须具有相同的维度</strong></li>
<li>两个序列可以是不同的模式形态（如：文本、声音、图像）</li>
<li>一个序列作为输入的Q，定义了输出的序列长度，另一个序列提供输入的K&amp;V</li>
</ul>
<h2 id="编码器-Encoder"><a href="#编码器-Encoder" class="headerlink" title="编码器(Encoder)"></a>编码器(Encoder)</h2><p>我们放大一个<code>Encoder</code>，发现里边的结构是一个多头自注意力机制加上一个前馈神经网络。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302053360.png" alt=""></p>
<p>像大部分<code>NLP</code>应用一样，我们首先将每个输入单词通过词嵌入算法转换为词向量，每个单词都被嵌入为 $512$ 维的向量。</p>
<ul>
<li>计算自注意力的第一步就是从每个编码器的输入向量（每个单词的词向量）中生成三个向量。也就是说对于每个单词，我们创造一个查询向量、一个键向量和一个值向量。这三个向量是通过词嵌入与三个权重矩阵后相乘创建的。可以发现这些新向量在维度上比词嵌入向量更低。他们的维度是 $64$ ，而词嵌入和编码器的输入/输出向量的维度是 $512$ 。但实际上不强求维度更小，这只是一种基于架构上的选择，它可以使多头注意力<code>(Multi-Head Attention)</code>的大部分计算保持不变。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302112845.png" alt=""></p>
<ul>
<li>计算自注意力的第二步是计算得分。这个得分是通过计算<code>Q</code>与各个单词的<code>K</code>向量的点积得到的。我们以<code>X1</code>为例，分别将<code>Q1</code>和<code>K1</code>、<code>K2</code>进行点积运算，假设分别得到得分 $112$ 和 $96$ 。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302133753.png" alt=""></p>
<ul>
<li>将得分分别除以一个特定数值 $8$ （<code>K</code>向量的维度的平方根，通常<code>K</code>向量的维度是 $64$ ）这能让梯度更加稳定，则得到结果如下：</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302134012.png" alt=""></p>
<ul>
<li>将上述结果进行<code>softmax</code>运算得到，<code>softmax</code>主要将分数标准化，使他们都是正数并且加起来等于 $1$ 。这个<code>softmax</code>分数决定了每个单词对编码当下位置的贡献。显然，已经在这个位置上的单词将获得最高的<code>softmax</code>分数，但有时关注另一个与当前单词相关的单词也会有帮助。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302137784.png" alt=""></p>
<ul>
<li>将<code>V</code>向量乘上<code>softmax</code>的结果，这个思想主要是为了保持我们想要关注的单词的值不变，而掩盖掉那些不相关的单词（例如将他们乘上很小的数字）。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302138098.png" alt=""></p>
<ul>
<li>将带权重的各个<code>V</code>向量加起来，至此，产生在这个位置上（第一个单词）的<code>self-attention</code>层的输出，其余位置的<code>self-attention</code>输出也是同样的计算方式。（注：自注意力的另一种解释就是在编码某个单词时，就是将所有单词的表示（值向量<code>V</code>）进行加权求和，而权重是通过该词的表示（键向量<code>K</code>）与被编码词表示（查询向量<code>Q</code>）的点积并通过<code>softmax</code>得到。）</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302139598.png" alt=""></p>
<p>论文为了进一步细化自注意力机制层，增加了<strong>多头注意力机制</strong>的概念，这从两个方面提高了自注意力层的性能。</p>
<ol>
<li>扩展了模型关注不同位置的能力，比如<code>Apple</code>可能和<code>Banana</code>比较相关，但是如果使用单个自注意力机制可能这种关系就被<code>Apple</code>自己支配了（体现在<code>softmax</code>之后的权重最大），而采用多头自注意力机制则可以缓解这种现象。</li>
<li>他给了自注意力层多个表示子空间。对于多头自注意力机制，我们不止有一组权重矩阵，而是有多组（论文中使用 $8$ 组），所以每个编码器/解码器使用 $8$ 个头（可以理解为 $8$ 个互不干扰自的注意力机制运算），每一组的<code>Q/K/V</code>都不相同。然后，得到 $8$ 个不同的权重矩阵 <code>Z</code>，每个权重矩阵被用来将输入向量投射到不同的表示子空间。论文中说到这样的好处是可以允许模型在不同的表示子空间里学习到相关的信息。输出矩阵的维度是（序列长度×单词向量长度）</li>
</ol>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302156146.png" alt=""></p>
<p>为了解决梯度消失的问题，在<code>Encoder</code>和<code>Decoder</code>中都是用了<strong>残差神经网络</strong>的结构，即每一个前馈神经网络的输入不光包含上述<code>self-attention</code>的输出，还包含最原始的输入。</p>
<h3 id="为什么选择除以-sqrt-d"><a href="#为什么选择除以-sqrt-d" class="headerlink" title="为什么选择除以$\sqrt{d}$"></a>为什么选择除以$\sqrt{d}$</h3><ol>
<li><p>防止softmax输入值过大，当embedding的维度越大，矩阵乘法的数值越大，所以防止softmax输入值过大，偏导数趋于0，有益于训练稳定；</p>
</li>
<li><p>$\frac{qk}{\sqrt{d}}$服从均值为0，方差为1的分布，作归一化；</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306151023478.png" alt=""></p>
</li>
<li><p>类似softmax加温度系数，温度系数根号d越大，softmax输出越平滑（而非尖锐），如果不除以$\sqrt{d}$，相当于softmax输出更尖锐，进而导致梯度稀疏。</p>
</li>
</ol>
<h2 id="解码器-Decoder"><a href="#解码器-Decoder" class="headerlink" title="解码器(Decoder)"></a>解码器(Decoder)</h2><p>同样的，在<code>Decoder</code>中使用的也是类似的结构。不同的地方在于，<code>Decoder</code>不是并行的，而是像<code>RNN</code>一样是一个一个产生的，有时序概念，在这个前提之下，<code>Encoder</code>的第一个模块是一个<code>Masked Multi-Head Attention</code>（就是生产第一步只有一个词，自己做<code>self-attention</code>，第二步生成两个词的时候，就做两个词的<code>self-attetion</code>，那么每一步在该层有一个输出，假设为<code>Q</code>，那么送入到中间的<code>Multi-Head Attention</code>层，和<code>Encoder</code>部分的<code>K</code>，<code>V</code>做<code>attention</code>）。进行过自注意力机制后，将<code>self-attention</code>的输出再与<code>Encoder</code>模块的输出计算一遍注意力机制得分之后，再进入前馈神经网络模块。</p>
<h2 id="Transformer中Encoder和Decoder的区别"><a href="#Transformer中Encoder和Decoder的区别" class="headerlink" title="Transformer中Encoder和Decoder的区别"></a>Transformer中Encoder和Decoder的区别</h2><ul>
<li><p>Decoder包含两个 Multi-Head Attention 层。</p>
</li>
<li><p>Decoder第一个 Multi-Head Attention 层采用了 Masked 操作，也就是一句话中左边的word看不到右边的word信息，这是因为在真实翻译的场景中也是word by word依次翻译出来的，在翻译当前词的时候肯定是不知道下一个翻译词是什么，所以在模型训练的时候就mask掉右边的信息让模型去学习，称之为<strong>Sequence mask</strong>。（假设target序列有m个token，那么可以构建m*m的矩阵，以主对角线为界，上三角的元素设置为-INF，这样在后续的softmax中其attention值趋于0，做到了mask的效果，并且mask操作是在计算出Q，K点积之后，softmax之前。）</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308231436850.jpg" alt=""></p>
</li>
<li><p>Decoder第二个 Multi-Head Attention 层的<strong>K, V</strong>矩阵使用 Encoder 的<strong>编码信息矩阵C</strong>进行计算，而<strong>Q</strong>使用上一个 Decoder block 的输出计算。</p>
</li>
<li><p>Decoder最后有一个 Softmax 层计算下一个翻译单词的概率。</p>
</li>
</ul>
<h2 id="BERT和Transformer的区别"><a href="#BERT和Transformer的区别" class="headerlink" title="BERT和Transformer的区别"></a>BERT和Transformer的区别</h2><ul>
<li>模型结构：BERT是基于Transformer编码器结构的模型，<strong>只有Encoder部分</strong>。而Transformer是由Encoder和Decoder组成的完整序列到序列结构的模型。因此，BERT的模型结构相对更简单，主要用于上下文语义理解任务，如文本分类、文本相似度计算等。而Transformer可以应用于更复杂的任务，如机器翻译、摘要生成等需要生成语言序列的任务。</li>
<li>预训练任务：BERT的预训练任务包括Masked Language Model（MLM）和Next Sentence Prediction（NSP）。在MLM任务中，BERT会随机掩码一部分输入单词，并让模型预测这些被掩码的单词。NSP任务是让BERT预测两段文本之间的关系，即是否是同一句话。而Transformer的预训练任务主要是MLM和一种被称为Wikitext-103的数据集的下一行预测任务。</li>
<li>预训练数据：BERT使用的预训练数据是Books Corpus（762M）和 English Wikipedia（2.56B）。而Transformer使用的预训练数据是Wikitext-103（1.9B）和Common Crawl（8.7G）。因此，BERT的预训练数据量相对更大。</li>
<li>训练方式：BERT使用的是无监督的预训练方式（MLM可以看成是加噪自编码，“有/无监督”只是从很多视角中的一种视角来对机器学习模型进行分类，一个非常重要的特征是看Label的来源是否需要人工参与，参照这个特征确实可以将BERT看成是无监督模型），而Transformer使用的是有监督的预训练方式。</li>
</ul>
<h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p>解码器输出本来是一个浮点型的向量，怎么转化成这两个词呢？解决办法是在最后的线性层接上一个<code>softmax</code>，其中线性层是一个简单的全连接神经网络，它将解码器产生的向量投影到一个更高维度的向量<code>(logits)</code>上，假设我们模型的词汇表是 $10000$ 个词，那么<code>logits</code>就有 $10000$ 个维度，每个维度对应一个唯一的词的得分。之后的<code>softmax</code>层将这些分数转换为概率。选择概率最大的维度，并对应地生成与之关联的单词作为此时间步的输出就是最终的输出。</p>
<h2 id="位置编码-Positional-Encoding"><a href="#位置编码-Positional-Encoding" class="headerlink" title="位置编码(Positional Encoding)"></a>位置编码(Positional Encoding)</h2><p><code>Transformer</code>中没有考虑顺序信息，那怎么办呢，我们可以在输入中做手脚，把输入变得有位置信息。我们可以给每个词向量加上一个有顺序特征的向量，发现<code>sin</code>函数和<code>cos</code>函数能够很好的表达这种特征，所以通常位置向量用以下公式来表示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310947858.png" alt=""></p>
<p>也即第 $t$ 个位置的位置编码为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310948367.png" alt=""></p>
<p>对编码的可视化：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310952548.png" alt=""></p>
<h3 id="Vanilla-Transformer的位置编码的特点"><a href="#Vanilla-Transformer的位置编码的特点" class="headerlink" title="Vanilla Transformer的位置编码的特点"></a>Vanilla Transformer的位置编码的特点</h3><ol>
<li>只要位置小于 $10000$ ，每一个位置的编码都是不同的。</li>
<li>奇数维度之间或者偶数维度之间周期不同。</li>
<li>也可以很好的表示相对位置信息。给定k,存在一个固定的与 $k$ 相关的线性变换矩阵，从而由 <code>pos</code> 的位置编码线性变换而得到 <code>pos+k</code> 的位置编码。这个相对位置信息可能可以被模型发现而利用。因为绝对位置信息只保证了各个位置不一样，但是并不是像 $0,1,2$ 这样的有明确前后关系的编码。</li>
</ol>
<p>我们拿出位置编码的两个维度出来做个例子，其他维度也是一样的，可以拼接起来变成完整的位置编码：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311001584.png" alt=""></p>
<p>其中 $M$ 为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311002558.png" alt=""></p>
<p>上面的操作也只可以看到线性关系，怎么可以更直白地知道每个<code>token</code>的距离关系？我们将两个位置编码对应相乘，可以发现发现相乘后的结果为一个余弦的加和。这里影响值的因素就是 $k$ 。如果两个<code>token</code>的距离越大，也就是 $k$ 越大，根据余弦函数的性质可以知道，两个位置编码相乘结果越小。这样的关系可以得到，如果两个<code>token</code>距离越远则乘积的结果越小。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311016054.png" alt=""></p>
<h3 id="其他编码方式"><a href="#其他编码方式" class="headerlink" title="其他编码方式"></a>其他编码方式</h3><p><strong>用[0,1]范围标记位置</strong></p>
<p>产生的问题是，当序列长度不同时，<code>token</code>间的相对距离是不一样的。例如在序列长度为 $3$ 时，<code>token</code>间的相对距离为 $0.5$ ；在序列长度为 $4$ 时，<code>token</code>间的相对距离就变为 $0.33$ 。</p>
<p><strong>用整型值标记位置</strong></p>
<p>存在的问题是：模型可能遇见比训练时所用的序列更长的序列，不利于模型的泛化；模型的位置表示是无界的，随着序列长度的增加，位置值会越来越大。</p>
<p><strong>用二进制向量标记位置</strong></p>
<p>这种编码方式也存在问题是编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的。</p>
<h3 id="Vanilla-Transformer位置编码的缺点以及改进"><a href="#Vanilla-Transformer位置编码的缺点以及改进" class="headerlink" title="Vanilla Transformer位置编码的缺点以及改进"></a>Vanilla Transformer位置编码的缺点以及改进</h3><p>看一个序列中，第 $i$ 个单词和第 $j$ 个单词的<code>attention score</code>的计算：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311035374.png" alt=""></p>
<p>其中 $W_{q}$ ， $W_{k}$ 分别是<code>Multi-Head Attention</code>给每个头加的<code>Query</code>和<code>Key</code>参数， $E_{x_{i}}$ 和 $E_{x_{j}}$ 是 $x_{i}$ 和 $x_{j}$ 的词嵌入， $U_{i}$ 和 $U_{j}$ 是第 $i$ 个位置和第 $j$ 个位置的位置向量。因式分解得到下式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311039224.png" alt=""></p>
<p>实际上，按照<code>Vanilla Transformer</code>的位置编码方法，如果没有 $W_{q}$ 和 $W_{k}$ 那么它是包含相对位置信息的，证明见上上小节。但是中间加入一个“不可知”的线性变换以后，就没有相对位置信息了，这个可以使用实验证明，具体如下图：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311047793.png" alt=""></p>
<p><strong>Transformer中加入相对位置信息的改进方法</strong></p>
<ul>
<li><p>既然相对位置信息是在<code>self-attention</code>计算时候丢失的，那么最直接的想法就是在计算<code>self-attention</code>的时候再加回来。该工作出自<code>Transformer</code>的原班人马，具体做法是在计算<code>attention score</code>和<code>weighted value</code>时各加入一个可训练的表示相对位置的参数，并且<code>multi-head</code>之间可以共享。</p>
</li>
<li><p>改写<code>self-attention</code>的计算公式</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311102674.png" alt=""></p>
</li>
<li><p>前面两个方法都是基于”词向量+位置向量”的模式，说白了是在“亡羊补牢”，而没有一开始就修建一个牢固的羊圈。而一种新的角度是推导出一个<strong>复数域</strong>的词向量方法，理论十分优美。</p>
</li>
</ul>
<h1 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a>Vision Transformer</h1><h2 id="模型组成"><a href="#模型组成" class="headerlink" title="模型组成"></a>模型组成</h2><p>模型由三个模块组成：</p>
<ul>
<li><code>Linear Projection of Flattened Patches</code>（<code>Embedding</code>层）</li>
<li><code>Transformer Encoder</code></li>
<li><code>MLP Head</code>（最终用于分类的层结构）</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101646509.png" alt=""></p>
<h2 id="Embedding层结构详解"><a href="#Embedding层结构详解" class="headerlink" title="Embedding层结构详解"></a>Embedding层结构详解</h2><p>对于标准的<code>Transformer</code>模块，要求输入的是<code>token</code>（向量）序列，即二维矩阵<code>[num_token, token_dim]</code>，<code>0-9</code>对应的<code>token</code>都是向量，以<code>ViT-B/16</code>为例，每个<code>token</code>向量长度为 $768$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101836412.png" alt=""></p>
<p>对于图像数据而言，其数据格式为<code>[H, W, C]</code>是三维矩阵明显不是<code>Transformer</code>想要的。所以需要先通过一个<code>Embedding</code>层来对数据做个变换。如上图所示，首先将一张图片按给定大小分成一堆<code>Patches</code>。以<code>ViT-B/16</code>为例，将输入图片<code>(224x224)</code>按照<code>16x16</code>大小的<code>Patch</code>进行划分，划分后会得到 $( 224 / 16 )^{2} = 196$ 个<code>Patches</code>。接着通过线性映射将每个<code>Patch</code>映射到一维向量中，以<code>ViT-B/16</code>为例，每个<code>Patch</code>数据<code>shape</code>为 $[16, 16, 3]$ 通过映射得到一个长度为 $768$ 的向量（后面都直接称为<code>token</code>）。</p>
<p><strong>在输入Transformer Encoder之前注意需要加上[class]token以及Position Embedding。</strong> 在原论文中，作者说参考<code>BERT</code>，在刚刚得到的一堆<code>tokens</code>中插入一个专门用于分类的<code>[class]token</code>，这个<code>[class]token</code>是一个可训练的参数，数据格式和其他<code>token</code>一样都是一个向量，以<code>ViT-B/16</code>为例，就是一个长度为<code>768</code>的向量，与之前从图片中生成的<code>tokens</code>拼接在一起， $([1, 768], [196, 768]) \rightarrow [197, 768]$ 。然后关于<code>Position Embedding</code>就是之前<code>Transformer</code>中讲到的<code>Positional Encoding</code>，这里的<code>Position Embedding</code>采用的是一个可训练的参数，是直接叠加在<code>tokens</code>上的<code>(add)</code>，所以<code>shape</code>要一样。以<code>ViT-B/16</code>为例，刚刚拼接<code>[class]token</code>后<code>shape</code>是 $[197, 768]$ ，那么这里的<code>Position Embedding</code>的<code>shape</code>也是 $[197, 768]$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101836048.png" alt=""></p>
<h2 id="Transformer-Encoder详解"><a href="#Transformer-Encoder详解" class="headerlink" title="Transformer Encoder详解"></a>Transformer Encoder详解</h2><p><code>Transformer Encoder</code>其实就是重复堆叠<code>Encoder Block</code> $L$ 次，主要由以下几部分组成：</p>
<ul>
<li><code>Layer Norm</code>，这种<code>Normalization</code>方法主要是针对<code>NLP</code>领域提出的，这里是对每个<code>token</code>进行<code>Norm</code>处理。</li>
<li><code>Multi-Head Attention</code>，也就是<code>Transformer</code>中的多头注意力。</li>
<li><code>Dropout/DropPath</code>，在原论文的代码中是直接使用的<code>Dropout</code>层，但<code>rwightman</code>实现的代码中使用的是<code>DropPath(stochastic depth)</code>，可能后者会更好一点。</li>
<li><code>MLP Block</code>，就是全连接+<code>GELU</code>激活函数+<code>Dropout</code>组成也非常简单，需要注意的是第一个全连接层会把输入节点个数翻 $4$ 倍 $[197, 768] \rightarrow [197, 3072]$ ，第二个全连接层会还原回原节点个数 $[197, 3072] \rightarrow [197, 768]$ 。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101738369.png" alt=""></p>
<h2 id="MLP-Head详解"><a href="#MLP-Head详解" class="headerlink" title="MLP Head详解"></a>MLP Head详解</h2><p>上面通过<code>Transformer Encoder</code>后输出的<code>shape</code>和输入的<code>shape</code>是保持不变的，以<code>ViT-B/16</code>为例，输入的是 $[197, 768]$ 输出的还是 $[197, 768]$。注意，在<code>Transformer Encoder</code>后其实还有一个<code>Layer Norm</code>没有画出来。这里我们只是需要分类的信息，所以我们只需要提取出<code>[class]token</code>生成的对应结果就行，即 $[197, 768]$ 中抽取出 $[class]token$ 对应的 $[1, 768]$ 。接着我们通过<code>MLP Head</code>得到我们最终的分类结果。<code>MLP Head</code>原论文中说在训练<code>ImageNet21K</code>时是由<code>Linear</code>+<code>tanh</code>激活函数+<code>Linear</code>组成。但是迁移到<code>ImageNet1K</code>上或者自己的数据上时，只用一个<code>Linear</code>即可。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101841213.png" alt=""></p>
<h1 id="Swin-Transformer"><a href="#Swin-Transformer" class="headerlink" title="Swin Transformer"></a>Swin Transformer</h1><h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><p>每个窗口固定有 $7\ast7$ 个patch，所以会有 $8\ast8$个window。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202307131454285.png" alt=""></p>
<h2 id="Patch-merging"><a href="#Patch-merging" class="headerlink" title="Patch merging"></a>Patch merging</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202307131152042.png" alt=""></p>
<h2 id="W-MSA"><a href="#W-MSA" class="headerlink" title="W-MSA"></a>W-MSA</h2><p>MSA全称为Windows Multi-head Self-Attention也就是窗口化的Self-Attention机制，此处以在一个 $4\ast4$ 的特征图上做为例子，在Vision Transformer中的MSA模块， $4\ast4$ 中的每个像素都要去和其他像素进行关联度的计算，那么在W-MSA中，其将原 $4\ast4$ 的特征图首先分割成了 $4$ 个 $2\ast2$ 的Window窗口，然后再在每个窗口内部进行单独的Self-Attention的计算。也就是说，每个像素只需要和自己所属Window内部的像素进行关联度的计算即可。这样一来，确实大大减少了计算量，但是你会发现窗口之间的像素也无法进行通信了，导致我们的感受野变小，对于最终的结果产生影响。优劣势还是非常的明确的。</p>
<h2 id="SW-MSA"><a href="#SW-MSA" class="headerlink" title="SW-MSA"></a>SW-MSA</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202307131153000.png" alt=""></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%80%BB%E7%BB%93/" rel="tag"># 总结</a>
              <a href="/tags/%E9%9D%A2%E8%AF%95/" rel="tag"># 面试</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/11/22/%E7%AE%97%E6%B3%95Tricks/" rel="prev" title="算法Tricks">
                  <i class="fa fa-chevron-left"></i> 算法Tricks
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/06/14/Linux%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/" rel="next" title="Linux使用总结">
                  Linux使用总结 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">木霈玖</span>
</div>

    </div>
  </footer>

  
  <script src="//unpkg.com/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//unpkg.com/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
