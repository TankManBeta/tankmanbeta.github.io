<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//unpkg.com/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//unpkg.com/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="前言记录一下暑期实习面试经验。">
<meta property="og:type" content="article">
<meta property="og:title" content="面经">
<meta property="og:url" content="http://example.com/2023/03/06/%E9%9D%A2%E7%BB%8F/index.html">
<meta property="og:site_name" content="木霈玖的博客">
<meta property="og:description" content="前言记录一下暑期实习面试经验。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071107979.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071138731.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071255957.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071300267.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317819.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317983.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071328562.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071332166.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071333924.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071335563.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071336003.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339814.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339457.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071340668.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071341013.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071342703.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071343619.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071356215.png">
<meta property="article:published_time" content="2023-03-06T14:04:43.000Z">
<meta property="article:modified_time" content="2023-03-07T06:02:31.039Z">
<meta property="article:author" content="木霈玖">
<meta property="article:tag" content="总结">
<meta property="article:tag" content="面试">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071107979.png">


<link rel="canonical" href="http://example.com/2023/03/06/%E9%9D%A2%E7%BB%8F/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>面经 | 木霈玖的博客</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">木霈玖的博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="nav-number">2.</span> <span class="nav-text">偏差和方差</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Bagging"><span class="nav-number">3.</span> <span class="nav-text">Bagging</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Boosting"><span class="nav-number">4.</span> <span class="nav-text">Boosting</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#batchsize"><span class="nav-number">5.</span> <span class="nav-text">batchsize</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#BatchNorm"><span class="nav-number">6.</span> <span class="nav-text">BatchNorm</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SVM"><span class="nav-number">7.</span> <span class="nav-text">SVM</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CNN"><span class="nav-number">8.</span> <span class="nav-text">CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">8.1.</span> <span class="nav-text">卷积层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">8.2.</span> <span class="nav-text">池化层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ResNet"><span class="nav-number">8.3.</span> <span class="nav-text">ResNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%AE%E7%82%B9"><span class="nav-number">8.3.1.</span> <span class="nav-text">ResNet中的一些亮点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%87%E7%94%A8residual"><span class="nav-number">8.3.2.</span> <span class="nav-text">为什么采用residual?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#residual%E7%BB%93%E6%9E%84"><span class="nav-number">8.3.3.</span> <span class="nav-text">residual结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#residual%E7%9A%84%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F"><span class="nav-number">8.3.3.1.</span> <span class="nav-text">residual的计算方式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ResNet%E4%B8%AD%E4%B8%A4%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84residual"><span class="nav-number">8.3.3.2.</span> <span class="nav-text">ResNet中两种不同的residual</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RNN"><span class="nav-number">9.</span> <span class="nav-text">RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-number">9.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81RNN%EF%BC%9F"><span class="nav-number">9.2.</span> <span class="nav-text">为什么需要RNN？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E4%B8%BB%E8%A6%81%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F"><span class="nav-number">9.3.</span> <span class="nav-text">RNN的主要应用领域</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="nav-number">9.4.</span> <span class="nav-text">RNN的计算过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E5%BB%BA%E6%A8%A1%E6%96%B9%E5%BC%8F"><span class="nav-number">9.5.</span> <span class="nav-text">RNN的建模方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E5%AF%B9%E5%A4%9A%EF%BC%88vector-to-sequence-%EF%BC%89"><span class="nav-number">9.5.1.</span> <span class="nav-text">一对多（vector-to-sequence ）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%AF%B9%E4%B8%80%EF%BC%88sequence-to-vector-%EF%BC%89"><span class="nav-number">9.5.2.</span> <span class="nav-text">多对一（sequence-to-vector ）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%AF%B9%E5%A4%9A%EF%BC%88Encoder-Decoder-%EF%BC%89"><span class="nav-number">9.5.3.</span> <span class="nav-text">多对多（Encoder-Decoder ）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E4%B8%AD%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%87%BA%E7%8E%B0%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%EF%BC%9F%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="nav-number">9.6.</span> <span class="nav-text">RNN中为什么会出现梯度消失？如何解决？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">9.7.</span> <span class="nav-text">RNN的注意力机制</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">木霈玖</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/06/%E9%9D%A2%E7%BB%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="木霈玖">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="木霈玖的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          面经
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-03-06 22:04:43" itemprop="dateCreated datePublished" datetime="2023-03-06T22:04:43+08:00">2023-03-06</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2023-03-07 14:02:31" itemprop="dateModified" datetime="2023-03-07T14:02:31+08:00">2023-03-07</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">总结</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>记录一下暑期实习面试经验。</p>
<a id="more"></a>
<h1 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h1><p>偏差：模型预测值的期望与真实值之间的差异，反应的是模型的拟合能力。<br>方差：反应的是训练集的变化所导致的学习性能的变化，即刻画了<strong>数据扰动</strong>所造成的影响，模型过拟合时会出现较大的方差。</p>
<h1 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h1><p>Bagging算法（英语：Bootstrap aggregating，引导聚集算法），又称装袋算法，是机器学习领域的一种团体学习算法。最初由Leo Breiman于1996年提出。Bagging算法可与其他分类、回归算法结合，提高其准确率、稳定性的同时，通过降低结果的方差，避免过拟合的发生。</p>
<p><strong>随机采样（bootstrap sample）</strong>从n个数据点中<strong>有放回地重复随机</strong>抽取一个样本（即同一个样本可被多次抽取），共抽取n次。创建一个与原数据大小相同得数据集，但有些数据点会缺失（大约1/3），有些会重复。</p>
<p>Bagging对于弱学习器没有限制，这和Adaboost一样。但是最常用的一般也是<strong>决策树</strong>和<strong>神经网络</strong>。</p>
<p>Bagging的集合策略也比较简单，对于分类问题，通常使用简单投票法，得到最多票数的类别或者类别之一为最终的模型输出。对于回归问题，通常使用简单平均法，对T个弱学习器得到的回归结果进行算术平均得到最终的模型输出。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071107979.png" alt=""></p>
<h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><p>Bagging在随机森林的构建过程中，各棵树之间是相互独立的，在构建第m棵树的时候，不会考虑前面的m-1棵树。Boosting在构建第m棵子树的时候，会考虑到前m-1棵子树的结果。</p>
<p><strong>提升学习</strong>（Boosting）是一种机器学习技术，通过从训练数据构建模型，然后创建第二个模型来尝试纠正第一个模型中的错误来完成的。添加模型直到完美预测训练集或添加最大数量的模型。提升学习的每一步产生弱预测模型（如决策树），并加权累加到总模型中；如果每一步的弱预测模型的生成都是依据损失函数的梯度方式，就称为梯度提升（Gradient Boosting）。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071138731.png" alt=""></p>
<p>Adaptive Boosting（AdaBoost）是第一个为二进制分类开发的真正成功的提升算法。这是理解Boosting的最佳起点，现代提升方法建立在AdaBoost之上。</p>
<p>AdaBoost流程：</p>
<ol>
<li><p>训练数据集中的每个实例都被加权。初始权重设置为：$Weight(X_{i})=\frac{1}{N_{}}$</p>
</li>
<li><p>使用加权之后的样本作为训练数据，以弱分类器（决策树桩）进行训练。</p>
</li>
<li><p>为训练后的模型计算当前分类器的分类误差。传统的计算方式如下：</p>
<p>分类误差：$error_t = \sum_{i=1}^{N}(W_{t,i}\times terror_{i})$。其中$terror_{i}=I(G_{t}(X_{i})\neq y_{i})$。</p>
<p>例如：如果我们有三个训练实例，权重分别为0.01、0.5和0.2。预测值为-1、-1和-1，实例中的真实输出变量为-1、1和-1，则terror为0、1和0。误分类率将计算为：<strong>error = (0.01*0 + 0.5*1 + 0.2*0) or error = 0.5</strong>。</p>
</li>
<li><p>为经过训练的模型计算阶段权值，该值为模型做出的任何预测提供权重。训练模型的阶段值计算公式：$\alpha_{t}=\frac{1}{2}\times\ln\frac{1-error_{t}}{error_{t}}$</p>
</li>
<li><p>更新训练权重，为错误预测的实例提供更多的权重，为正确预测的实例提供更少权重。计算公式：$W_{t+1,i}=\frac{W_{t,i}}{Z_{t}}\exp(-\alpha_{t}G_{t}(X_{i})y_{i})$，其中$Z_{t}$是规范因子，$Z_{t}=\sum_{i=1}^{N}W_{t,i}\exp(-\alpha_{t}G_{t}(X_{i})y_{i})$</p>
</li>
<li><p>最终的分类器为：$G(X)=sign(\sum_{m=1}^{K}\alpha_{m}G_{m}(X))$</p>
</li>
</ol>
<h1 id="batchsize"><a href="#batchsize" class="headerlink" title="batchsize"></a>batchsize</h1><p>（1）batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练。<br>（2）iteration：1个iteration等于使用batchsize个样本训练一次。<br>（3）epoch：1个epoch等于使用训练集中的全部样本训练一次。<br>如果数据集比较小，则完全可以采用全数据集的形式。这样做的好处有两点：</p>
<ol>
<li>全数据集的方向能够更好的代表样本总体，确定其极值所在。</li>
<li>由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。</li>
</ol>
<p>增大batchsize的好处有三点：</p>
<ol>
<li>内存的利用率提高了，大矩阵乘法的并行化效率提高。</li>
<li>跑完一次epoch(全数据集)所需迭代次数减少，对于相同的数据量的处理速度进一步加快。</li>
<li>一定范围内，batchsize越大，其确定的下降方向就越准，引起训练震荡越小。</li>
</ol>
<p>盲目增大batchsize的坏处有三点：</p>
<ol>
<li>当数据集太大时，内存撑不住。</li>
<li>跑完一次epoch(全数据集)所需迭代次数减少了，但要想达到相同的精度，时间开销太大，参数的修正更加缓慢。</li>
<li>batchsize增大到一定的程度，其确定的下降方向已经基本不再变化。</li>
</ol>
<h1 id="BatchNorm"><a href="#BatchNorm" class="headerlink" title="BatchNorm"></a>BatchNorm</h1><p><strong>BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布。</strong></p>
<p>在神经网络中, 数据分布对训练会产生影响。比如某个神经元 x 的值为1，某个 Weights 的初始值为 0.1，这样后一层神经元计算结果就是 Wx = 0.1；又或者 x = 20，这样 Wx 的结果就为2。现在还不能看出什么问题,，但是，当我们加上一层激活函数，激活这个 Wx 值的时候，问题就来了。如果使用像tanh的激活函数， Wx 的激活值就变成了 $\approx 0.1$ 和 $\approx 1$, 接近于 1 的部已经处在了激活函数的饱和阶段, 也就是 x 无论再怎么扩大, tanh 激励函数输出值也还是接近1。</p>
<p>我们为了避免这种情况，就会对数据进行归一化，<strong>对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。</strong></p>
<h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><p>支持向量机（support vector machines，SVM）是一种二分类模型，它将实例的特征向量映射为空间中的一些点，SVM 的目的就是想要画出一条线，以 “最好地” 区分这两类点，以至如果以后有了新的点，这条线也能做出很好的分类。SVM 适合中小型数据样本、非线性、高维的分类问题。</p>
<p>将实例的特征向量（以二维为例）映射为空间中的一些点，如下图的实心点和空心点，它们属于不同的两类。SVM 的目的就是想要画出一条线，以“最好地”区分这两类点，以至如果以后有了新的点，这条线也能做出很好的分类。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071255957.png" alt=""></p>
<p><strong>Q1：能够画出多少条线对样本点进行区分？</strong><br>答：线是有无数条可以画的，区别就在于效果好不好，每条线都可以叫做一个划分超平面。比如上面的绿线就不好，蓝线还凑合，红线看起来就比较好。我们所希望找到的这条效果最好的线就是具有 “最大间隔的划分超平面”。</p>
<p><strong>Q2：为什么要叫作“超平面”呢？</strong><br>答：因为样本的特征很可能是高维的，此时样本空间的划分就不是一条线了。</p>
<p><strong>Q3：画线的标准是什么？/ 什么才叫这条线的效果好？/ 哪里好？</strong><br>答：SVM 将会寻找可以区分两个类别并且能使间隔（margin）最大的划分超平面。比较好的划分超平面，样本局部扰动时对它的影响最小、产生的分类结果最鲁棒、对未见示例的泛化能力最强。</p>
<p><strong>Q4：间隔（margin）是什么？</strong><br>答：对于任意一个超平面，其两侧数据点都距离它有一个最小距离（垂直距离），这两个最小距离的和就是间隔。比如下图中两条虚线构成的带状区域就是 margin，虚线是由距离中央实线最近的两个点所确定出来的（也就是由支持向量决定）。但此时 margin 比较小，如果用第二种方式画，margin 明显变大也更接近我们的目标。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071300267.png" alt=""></p>
<p><strong>Q5：为什么要让 margin 尽量大？</strong><br>答：因为大 margin 犯错的几率比较小，也就是更鲁棒啦。</p>
<p><strong>Q6：支持向量是什么？</strong><br>答：从上图可以看出，虚线上的点到划分超平面的距离都是一样的，实际上只有这几个点共同确定了超平面的位置，因此被称作 “支持向量（support vectors）”，“支持向量机” 也是由此来的。</p>
<p><strong>Q7：SVM 算法特性</strong></p>
<ol>
<li>训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以SVM不太容易产生overfitting。</li>
<li>SVM训练出来的模型完全依赖于支持向量，即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果仍然会得到完全一样的模型。</li>
<li>一个SVM如果训练得出的支持向量个数比较少，那么SVM训练出的模型比较容易被泛化。</li>
</ol>
<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化层是当前卷积神经网络中常用组件之一，它最早见于<code>LeNet</code>一文，称之为<code>Subsample</code>。自<code>AlexNet</code>之后采用Pooling命名。池化层是模仿人的视觉系统对数据进行降维，用更高层次的特征表示图像。</p>
<p>实施池化的目的：(1) 降低信息冗余；(2) 提升模型的尺度不变性、旋转不变性；(3) 防止过拟合。</p>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><h3 id="ResNet中的一些亮点"><a href="#ResNet中的一些亮点" class="headerlink" title="ResNet中的一些亮点"></a>ResNet中的一些亮点</h3><ol>
<li>超深的网络结构（超过1000层）。</li>
<li>提出residual（残差结构）模块。</li>
<li>使用Batch Normalization加速训练（丢弃dropout）。</li>
</ol>
<h3 id="为什么采用residual"><a href="#为什么采用residual" class="headerlink" title="为什么采用residual?"></a>为什么采用residual?</h3><p>人们认为卷积层和池化层的层数越多，获取到的图片特征信息越全，学习效果也就越好。但是在实际的试验中发现，随着卷积层和池化层的叠加，不但没有出现学习效果越来越好的情况，反而两种问题：</p>
<ol>
<li>梯度消失和梯度爆炸<br>梯度消失：若每一层的误差梯度小于1，反向传播时，网络越深，梯度越趋近于0<br>梯度爆炸：若每一层的误差梯度大于1，反向传播时，网络越深，梯度越来越大</li>
<li>退化问题<br>随着层数的增加，预测效果反而越来越差。</li>
</ol>
<ul>
<li>为了解决梯度消失或梯度爆炸问题，ResNet论文提出通过数据的预处理以及在网络中使用 BN（Batch Normalization）层来解决。</li>
<li>为了解决深层网络中的退化问题，可以人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系。这种神经网络被称为 残差网络 (ResNets)。ResNet论文提出了 residual结构（残差结构）来减轻退化问题，下图是使用residual结构的卷积网络，可以看到随着网络的不断加深，效果并没有变差，而是变的更好了。</li>
</ul>
<h3 id="residual结构"><a href="#residual结构" class="headerlink" title="residual结构"></a>residual结构</h3><h4 id="residual的计算方式"><a href="#residual的计算方式" class="headerlink" title="residual的计算方式"></a>residual的计算方式</h4><p>residual结构使用了一种shortcut的连接方式，也可理解为捷径。让特征矩阵隔层相加，注意 $\mathcal{F}(\mathbb{x})$和$\mathbb{x}$形状要相同，所谓相加是特征矩阵相同位置上的数字进行相加。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317819.png" alt=""></p>
<h4 id="ResNet中两种不同的residual"><a href="#ResNet中两种不同的residual" class="headerlink" title="ResNet中两种不同的residual"></a>ResNet中两种不同的residual</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317983.png" alt=""></p>
<ol>
<li>左侧残差结构称为 BasicBlock</li>
<li>右侧残差结构称为 Bottleneck<ul>
<li>其中第一层的1× 1的卷积核的作用是对特征矩阵进行降维操作，将特征矩阵的深度由256降为64；</li>
<li>第三层的1× 1的卷积核是对特征矩阵进行升维操作，将特征矩阵的深度由64升成256。<br>降低特征矩阵的深度主要是为了减少参数的个数。<br>如果采用BasicBlock,参数的个数应该是：256×256×3×3×2=1179648<br>采用Bottleneck，参数的个数是：1×1×256×64+3×3×64×64+1×1×256×64=69632</li>
<li>先降后升为了主分支上输出的特征矩阵和捷径分支上输出的特征矩阵形状相同，以便进行加法操作。</li>
</ul>
</li>
</ol>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>循环神经网络（Recurrent Neural Network, RNN）是一类以序列（sequence）数据为输入，在序列的演进方向进行递归（recursion）且所有节点（循环单元）按链式连接的递归神经网络（recursive neural network） 。</p>
<p>对循环神经网络的研究始于二十世纪80-90年代，并在二十一世纪初发展为深度学习（deep learning）算法之一  ，其中双向循环神经网络（Bidirectional RNN, Bi-RNN）和长短期记忆网络（Long Short-Term Memory networks，LSTM）是常见的循环神经网络。</p>
<h2 id="为什么需要RNN？"><a href="#为什么需要RNN？" class="headerlink" title="为什么需要RNN？"></a>为什么需要RNN？</h2><p>在CNN网络中的训练样本的数据为IID数据（独立同分布数据），所解决的问题也是分类问题或者回归问题或者是特征表达问题。<strong>但更多的数据是不满足IID的</strong>，如语言翻译，自动文本生成。它们是一个序列问题，包括时间序列和空间序列。比如时间序列数据，这类数据是在不同时间点上收集到的数据，反映了某一事物、现象等随时间的变化状态或程度。一般的神经网络，在训练数据足够、算法模型优越的情况下，给定特定的x，就能得到期望y。其一般处理单个的输入，前一个输入和后一个输入完全无关，但实际应用中，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。 这时就要用到RNN网络，RNN的结构图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071328562.png" alt=""></p>
<h2 id="RNN的主要应用领域"><a href="#RNN的主要应用领域" class="headerlink" title="RNN的主要应用领域"></a>RNN的主要应用领域</h2><p>可以说只要考虑时间先后顺序的问题都可以使用RNN来解决.这里主要说一下几个常见的应用领域:</p>
<ul>
<li>自然语言处理(NLP): 主要有视频处理, 文本生成, 语言模型, 图像处理</li>
<li>机器翻译, 机器写文章</li>
<li>语音识别</li>
<li>图像描述生成</li>
<li>文本相似度计算</li>
<li>推荐系统。例如：音乐推荐、网易考拉商品推荐、Youtube视频推荐等新的应用领域。</li>
</ul>
<h2 id="RNN的计算过程"><a href="#RNN的计算过程" class="headerlink" title="RNN的计算过程"></a>RNN的计算过程</h2><p>RNN引入了隐状态h（hidden state），h可对序列数据提取特征，接着再转换为输出。首先我们计算$h_{1}$：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071332166.jpg" alt=""></p>
<p>RNN中，每个步骤使用的参数<code>U,W,b</code>相同，$h_{2},h_{3},h_{4}$的计算方式和$h_{1}$类似，其计算结果如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071333924.jpg" alt=""></p>
<p>接下来，计算RNN的输出$y_1$，采用Softmax作为激活函数，根据$y_n=f(Wx+b)$，得到$y_1$:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071335563.jpg" alt=""></p>
<p>使用和$y_1$相同的参数<code>V,c</code>，得到$y_2,y_3,y_4$的输出结构：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071336003.jpg" alt=""></p>
<h2 id="RNN的建模方式"><a href="#RNN的建模方式" class="headerlink" title="RNN的建模方式"></a>RNN的建模方式</h2><h3 id="一对多（vector-to-sequence-）"><a href="#一对多（vector-to-sequence-）" class="headerlink" title="一对多（vector-to-sequence ）"></a>一对多（vector-to-sequence ）</h3><p>输入是一个单独的值，输出是一个序列。此时，有两种主要建模方式：</p>
<p>方式一：可只在其中的某一个序列进行计算，比如序列第一个进行输入计算，其建模方式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339814.jpg" alt=""></p>
<p>方式二：把输入信息X作为每个阶段的输入，其建模方式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339457.jpg" alt=""></p>
<h3 id="多对一（sequence-to-vector-）"><a href="#多对一（sequence-to-vector-）" class="headerlink" title="多对一（sequence-to-vector ）"></a>多对一（sequence-to-vector ）</h3><p>输入是一个序列，输出是一个单独的值，此时通常在最后的一个序列上进行输出变换，其建模如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071340668.jpg" alt=""></p>
<h3 id="多对多（Encoder-Decoder-）"><a href="#多对多（Encoder-Decoder-）" class="headerlink" title="多对多（Encoder-Decoder ）"></a>多对多（Encoder-Decoder ）</h3><p><strong>步骤一</strong>：将输入数据编码成一个上下文向量c，这部分称为Encoder，得到c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。其示意如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071341013.jpg" alt=""></p>
<p><strong>步骤二</strong>：用另一个RNN网络（我们将其称为Decoder）对其进行编码。</p>
<p>方法一是将步骤一中的c作为初始状态输入到Decoder，示意图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071342703.jpg" alt=""></p>
<p>方法二是将c作为Decoder的每一步输入，示意图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071343619.jpg" alt=""></p>
<h2 id="RNN中为什么会出现梯度消失？如何解决？"><a href="#RNN中为什么会出现梯度消失？如何解决？" class="headerlink" title="RNN中为什么会出现梯度消失？如何解决？"></a>RNN中为什么会出现梯度消失？如何解决？</h2><p><strong>梯度消失的原因：</strong>sigmoid函数的导数范围是(0,0.25]，tanh函数的导数范围是(0,1]，他们的导数最大都不大于1，如果取tanh或sigmoid函数作为激活函数嵌套到RNN中，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于0，这就是“梯度消失“现象。实际使用中，会优先选择tanh函数，原因是tanh函数相对于sigmoid函数来说梯度较大，收敛速度更快且引起梯度消失更慢。</p>
<p> <strong>解决RNN中的梯度消失方法主要有：</strong></p>
<ol>
<li>选取更好的激活函数，如Relu激活函数。ReLU函数的左侧导数为0，右侧导数恒为1，这就避免了“梯度消失“的发生。但恒为1的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。</li>
<li>加入BN层，其优点包括可加速收敛、控制过拟合，可以少用或不用Dropout和正则、降低网络对初始化权重不敏感，且能允许使用较大的学习率等。</li>
<li>改变传播结构，选择更高级的模型，例如：LSTM结构可以有效解决这个问题。</li>
</ol>
<h2 id="RNN的注意力机制"><a href="#RNN的注意力机制" class="headerlink" title="RNN的注意力机制"></a>RNN的注意力机制</h2><p>在上述的Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征c再解码。因此，c中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。如机器翻译问题，当要翻译的句子较长时，一个c可能存不下那么多信息，就会造成翻译精度的下降。Attention机制通过在每个时间输入不同的c来解决此问题。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071356215.png" alt=""></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%80%BB%E7%BB%93/" rel="tag"># 总结</a>
              <a href="/tags/%E9%9D%A2%E8%AF%95/" rel="tag"># 面试</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/11/22/%E7%AE%97%E6%B3%95Tricks/" rel="prev" title="算法Tricks">
                  <i class="fa fa-chevron-left"></i> 算法Tricks
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">木霈玖</span>
</div>

    </div>
  </footer>

  
  <script src="//unpkg.com/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//unpkg.com/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
