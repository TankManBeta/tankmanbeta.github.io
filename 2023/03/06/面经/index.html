<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//unpkg.com/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//unpkg.com/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="前言记录一些自己整理的面经。">
<meta property="og:type" content="article">
<meta property="og:title" content="面经">
<meta property="og:url" content="http://example.com/2023/03/06/%E9%9D%A2%E7%BB%8F/index.html">
<meta property="og:site_name" content="木霈玖的博客">
<meta property="og:description" content="前言记录一些自己整理的面经。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222347197.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222349037.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211132940.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211055521.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211059473.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211251919.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308101255449.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101955821.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102004442.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101950246.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102008358.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102010115.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102019772.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071555276.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071559636.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306191056073.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306191111890.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306191113911.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071107979.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304211122841.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304211122760.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071138731.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101613543.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101613198.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101650959.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101651061.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101656177.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101658315.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101704500.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101704670.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101716405.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101727004.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101743580.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101744292.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101744829.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101746573.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102112494.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102149708.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102152516.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102156269.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102208969.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102216271.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111538347.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111601603.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111619137.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071255957.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071300267.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20230106231925.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303212301932.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222128650.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222129537.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317819.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317983.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306151052109.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222332494.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071328562.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071332166.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071333924.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071335563.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071336003.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339814.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339457.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071340668.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071341013.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071342703.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071343619.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071356215.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101147762.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101148082.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101504342.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101504156.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101539655.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101545859.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101553212.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302227637.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302042116.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302053360.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302112845.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302133753.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302134012.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302137784.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302138098.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302139598.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302156146.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306151023478.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310947858.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310948367.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310952548.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311001584.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311002558.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311016054.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311035374.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311039224.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311047793.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311102674.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101646509.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101836412.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101836048.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101738369.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101841213.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202307131454285.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202307131152042.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202307131153000.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192001842.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192101633.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192102103.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192115000.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202215784.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202218223.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192140478.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210945142.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210948508.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202254827.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210936324.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210950233.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210958573.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202054636.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202055336.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202037061.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202037832.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202048519.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202104133.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202135669.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192157333.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192205012.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192210225.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192217890.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231131422.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231137649.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231146383.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231152053.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231153530.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231154583.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231206104.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231212913.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231213243.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231220341.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231221419.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231225576.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202004566.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202035154.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202054590.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202054939.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202058666.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202059177.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202100516.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202104262.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202111969.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202113543.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202113829.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202119928.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202146491.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202158449.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202201783.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211446441.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211448528.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211452527.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211453721.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211454010.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211454382.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211518409.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211523344.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211528560.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211603934.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211605627.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211606515.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211606561.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211612604.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211612034.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211613957.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211616699.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211617066.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211617038.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211617296.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211618509.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211618888.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211633299.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211638065.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211639379.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211640294.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211640558.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211642022.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211645673.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211646357.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211646534.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211711238.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211716358.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211728762.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211730740.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211731896.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211735181.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212023626.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212029715.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212030357.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212035441.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212035613.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212037262.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212044848.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212048618.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212049445.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212049720.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212050370.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212050471.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212051995.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212051641.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212051116.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212052434.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212052546.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212053113.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212053253.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212054677.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212054847.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212054640.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212059483.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212101984.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212101121.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212101061.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212103361.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212104750.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212104358.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212104590.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212105264.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212105990.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212105910.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212106394.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212106342.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211740339.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211741089.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211743079.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211744832.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211745271.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221652189.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221654279.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221655094.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221737318.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304191959346.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192002037.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192107149.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192121730.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192201890.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192209525.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192210293.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201650996.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201703206.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201704698.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201707323.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221912175.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221914930.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221916770.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221917878.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221919755.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221925847.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221951440.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221951074.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221952139.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221952937.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221952743.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221953919.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221953829.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221954360.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221954996.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222002416.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222003410.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222003825.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222005263.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222006335.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222009493.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222013355.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222014011.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222014360.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222018983.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222018848.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222019899.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222021880.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222024864.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222023574.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222025480.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222025399.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222028818.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222029092.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222030271.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222030314.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222031199.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222031359.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222032295.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222035784.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222038251.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222040735.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222041255.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222042251.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222043024.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222048779.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222048554.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222049946.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222050330.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222050837.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222051074.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222051287.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222052753.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222052222.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222053977.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222057071.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222059178.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222100079.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222100694.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222103700.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222104051.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222106689.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222106134.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222107108.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222109689.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222110388.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222111207.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222115177.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222115513.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222116327.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222116575.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222116226.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222117210.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222117026.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222117989.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222118339.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222118174.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222119465.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222138309.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222138450.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222139876.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222139053.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222140078.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222140551.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222141214.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222141391.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222142236.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222143624.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222143038.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222143674.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222202895.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222204455.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222205209.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222206641.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222209233.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222209772.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222210887.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222210151.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222211775.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222211538.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222211833.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222212318.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222212723.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222213526.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222213378.png">
<meta property="article:published_time" content="2023-03-06T14:04:43.000Z">
<meta property="article:modified_time" content="2023-08-10T07:39:08.618Z">
<meta property="article:author" content="木霈玖">
<meta property="article:tag" content="总结">
<meta property="article:tag" content="面试">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222347197.png">


<link rel="canonical" href="http://example.com/2023/03/06/%E9%9D%A2%E7%BB%8F/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>面经 | 木霈玖的博客</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">木霈玖的博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E6%A8%A1%E5%9E%8B%E4%B8%8D%E6%94%B6%E6%95%9B%EF%BC%8C%E6%98%AF%E5%90%A6%E8%AF%B4%E6%98%8E%E8%BF%99%E4%B8%AA%E6%A8%A1%E5%9E%8B%E6%97%A0%E6%95%88%EF%BC%8C%E8%87%B4%E6%A8%A1%E5%9E%8B%E4%B8%8D%E6%94%B6%E6%95%9B%E7%9A%84%E5%8E%9F%E5%9B%A0%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="nav-number">2.</span> <span class="nav-text">训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">3.</span> <span class="nav-text">特征归一化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8A%A0%E5%BF%AB%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%80%9F%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">4.</span> <span class="nav-text">加快模型训练速度的方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Zero-Shot-One-Shot-Few-Shot"><span class="nav-number">5.</span> <span class="nav-text">Zero-Shot,One-Shot,Few-Shot</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PyTorch%E5%92%8CTensorFlow%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">6.</span> <span class="nav-text">PyTorch和TensorFlow的区别</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%9B%BE%E4%B8%8E%E9%9D%99%E6%80%81%E5%9B%BE"><span class="nav-number">6.1.</span> <span class="nav-text">动态图与静态图</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2%E5%92%8C%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7"><span class="nav-number">6.2.</span> <span class="nav-text">部署和可扩展性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B5%84%E6%BA%90%E4%BC%98%E5%8C%96%E5%92%8C%E5%88%A9%E7%94%A8%E7%8E%87"><span class="nav-number">6.3.</span> <span class="nav-text">资源优化和利用率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E6%9C%AF%E7%A0%94%E7%A9%B6%E5%92%8C%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81"><span class="nav-number">6.4.</span> <span class="nav-text">学术研究和开源代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Pytorch%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83"><span class="nav-number">7.</span> <span class="nav-text">Pytorch多卡训练</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">8.</span> <span class="nav-text">提升模型泛化能力的方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="nav-number">9.</span> <span class="nav-text">偏差和方差</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">10.</span> <span class="nav-text">过拟合和欠拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5"><span class="nav-number">10.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">10.2.</span> <span class="nav-text">过拟合的解决方案</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">10.3.</span> <span class="nav-text">欠拟合的解决方案</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="nav-number">11.</span> <span class="nav-text">梯度消失和梯度爆炸</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5-1"><span class="nav-number">11.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">11.2.</span> <span class="nav-text">解决方案</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">12.</span> <span class="nav-text">解决样本不均衡的方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LabelSmoothing"><span class="nav-number">13.</span> <span class="nav-text">LabelSmoothing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%AD%E5%B9%B3%E6%BB%91%E5%92%8C%E9%94%90%E5%8C%96%E6%93%8D%E4%BD%9C%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">14.</span> <span class="nav-text">图像处理中平滑和锐化操作是什么？</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5-2"><span class="nav-number">14.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">14.2.</span> <span class="nav-text">使用场景</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#batchsize"><span class="nav-number">15.</span> <span class="nav-text">batchsize</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">16.</span> <span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Smooth-L1-Loss"><span class="nav-number">16.1.</span> <span class="nav-text">Smooth L1 Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cross-Entropy-Loss"><span class="nav-number">16.2.</span> <span class="nav-text">Cross Entropy Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Binary-Cross-Entropy-Loss"><span class="nav-number">16.3.</span> <span class="nav-text">Binary Cross Entropy Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Focal-loss"><span class="nav-number">16.4.</span> <span class="nav-text">Focal loss</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">17.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sigmoid"><span class="nav-number">17.1.</span> <span class="nav-text">Sigmoid</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tanh"><span class="nav-number">17.2.</span> <span class="nav-text">tanh</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ReLU"><span class="nav-number">17.3.</span> <span class="nav-text">ReLU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Leaky-ReLU"><span class="nav-number">17.4.</span> <span class="nav-text">Leaky ReLU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ELU"><span class="nav-number">17.5.</span> <span class="nav-text">ELU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GELU"><span class="nav-number">17.6.</span> <span class="nav-text">GELU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ReLU%E6%AF%94Sigmoid%E6%95%88%E6%9E%9C%E5%A5%BD%E5%9C%A8%E5%93%AA%E9%87%8C%EF%BC%9F"><span class="nav-number">17.7.</span> <span class="nav-text">ReLU比Sigmoid效果好在哪里？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="nav-number">18.</span> <span class="nav-text">优化器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SGD"><span class="nav-number">18.1.</span> <span class="nav-text">SGD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SGDM"><span class="nav-number">18.2.</span> <span class="nav-text">SGDM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adagrad"><span class="nav-number">18.3.</span> <span class="nav-text">Adagrad</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RMSProp"><span class="nav-number">18.4.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adam"><span class="nav-number">18.5.</span> <span class="nav-text">Adam</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="nav-number">19.</span> <span class="nav-text">评价指标</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Top-1-Accuracy%E5%92%8CTop-5-Accuracy"><span class="nav-number">19.1.</span> <span class="nav-text">Top-1 Accuracy和Top-5 Accuracy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DICE"><span class="nav-number">19.2.</span> <span class="nav-text">DICE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IoU"><span class="nav-number">19.3.</span> <span class="nav-text">IoU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MIoU"><span class="nav-number">19.4.</span> <span class="nav-text">MIoU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Box-AP"><span class="nav-number">19.5.</span> <span class="nav-text">Box AP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mask-AP"><span class="nav-number">19.6.</span> <span class="nav-text">Mask AP</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="nav-number">20.</span> <span class="nav-text">集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Bagging"><span class="nav-number">20.1.</span> <span class="nav-text">Bagging</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="nav-number">20.1.1.</span> <span class="nav-text">随机森林</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">20.1.1.1.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E7%9A%84%E7%BB%88%E6%AD%A2%E6%9D%A1%E4%BB%B6"><span class="nav-number">20.1.1.1.1.</span> <span class="nav-text">决策树构建的终止条件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%89%AA%E6%9E%9D"><span class="nav-number">20.1.1.1.2.</span> <span class="nav-text">决策树剪枝</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95"><span class="nav-number">20.1.1.1.3.</span> <span class="nav-text">决策树生成算法</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Boosting"><span class="nav-number">20.2.</span> <span class="nav-text">Boosting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaBoost"><span class="nav-number">20.2.1.</span> <span class="nav-text">AdaBoost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT"><span class="nav-number">20.2.2.</span> <span class="nav-text">GBDT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95"><span class="nav-number">20.2.2.1.</span> <span class="nav-text">提升树算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient-Boosting-Decision-Tree%EF%BC%88%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%89"><span class="nav-number">20.2.2.2.</span> <span class="nav-text">Gradient Boosting Decision Tree（梯度提升决策树）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#XGBoost"><span class="nav-number">20.2.3.</span> <span class="nav-text">XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="nav-number">20.2.3.1.</span> <span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F%E5%B1%95%E5%BC%80"><span class="nav-number">20.2.3.2.</span> <span class="nav-text">泰勒公式展开</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%91%E7%9A%84%E5%8F%82%E6%95%B0%E5%8C%96"><span class="nav-number">20.2.3.3.</span> <span class="nav-text">树的参数化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%88%86%E8%A3%82"><span class="nav-number">20.2.3.4.</span> <span class="nav-text">特征分裂</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LightGBM"><span class="nav-number">20.2.4.</span> <span class="nav-text">LightGBM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B4%E6%96%B9%E5%9B%BEHistogram%E7%AE%97%E6%B3%95"><span class="nav-number">20.2.4.1.</span> <span class="nav-text">直方图Histogram算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%A6%E6%B7%B1%E5%BA%A6%E9%99%90%E5%88%B6%E7%9A%84Leaf-wise%E7%9A%84%E5%8F%B6%E5%AD%90%E7%94%9F%E9%95%BF%E7%AD%96%E7%95%A5"><span class="nav-number">20.2.4.2.</span> <span class="nav-text">带深度限制的Leaf-wise的叶子生长策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E7%B1%BB%E5%88%AB%E7%89%B9%E5%BE%81"><span class="nav-number">20.2.4.3.</span> <span class="nav-text">支持类别特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%8D%95%E8%BE%B9%E9%87%87%E6%A0%B7-GOSS"><span class="nav-number">20.2.4.4.</span> <span class="nav-text">基于梯度的单边采样(GOSS)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%92%E6%96%A5%E7%89%B9%E5%BE%81%E7%BB%91%E5%AE%9A"><span class="nav-number">20.2.4.5.</span> <span class="nav-text">互斥特征绑定</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CatBoost"><span class="nav-number">20.2.5.</span> <span class="nav-text">CatBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%B1%BB%E5%88%AB%E7%89%B9%E5%BE%81%E7%9A%84Ordered-Target-Statistics%E6%95%B0%E5%80%BC%E7%BC%96%E7%A0%81%E6%96%B9%E6%B3%95"><span class="nav-number">20.2.5.1.</span> <span class="nav-text">基于类别特征的Ordered Target Statistics数值编码方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E8%B4%AA%E5%BF%83%E7%AD%96%E7%95%A5%E7%9A%84%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89%E6%96%B9%E6%B3%95"><span class="nav-number">20.2.5.2.</span> <span class="nav-text">基于贪心策略的特征交叉方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%81%BF%E5%85%8D%E9%A2%84%E6%B5%8B%E5%81%8F%E7%A7%BB%E7%9A%84Ordered-Boosting%E6%96%B9%E6%B3%95"><span class="nav-number">20.2.5.3.</span> <span class="nav-text">避免预测偏移的Ordered Boosting方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91%E4%BD%9C%E4%B8%BA%E5%9F%BA%E6%A8%A1%E5%9E%8B"><span class="nav-number">20.2.5.4.</span> <span class="nav-text">使用对称二叉树作为基模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stacking"><span class="nav-number">20.3.</span> <span class="nav-text">Stacking</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SVM"><span class="nav-number">21.</span> <span class="nav-text">SVM</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CNN"><span class="nav-number">22.</span> <span class="nav-text">CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%B0%BA%E5%BA%A6"><span class="nav-number">22.1.</span> <span class="nav-text">多尺度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">22.2.</span> <span class="nav-text">卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%99%AE%E9%80%9A%E5%8D%B7%E7%A7%AF"><span class="nav-number">22.2.1.</span> <span class="nav-text">普通卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF-group-convolution"><span class="nav-number">22.2.2.</span> <span class="nav-text">分组卷积(group convolution)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%BB%93%E6%9E%9C"><span class="nav-number">22.2.3.</span> <span class="nav-text">卷积结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">22.3.</span> <span class="nav-text">池化层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#padding"><span class="nav-number">22.4.</span> <span class="nav-text">padding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN%E4%B8%AD%E7%9A%84%E7%AD%89%E5%8F%98%E5%92%8C%E4%B8%8D%E5%8F%98"><span class="nav-number">22.5.</span> <span class="nav-text">CNN中的等变和不变</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%89%E5%8F%98%E6%80%A7"><span class="nav-number">22.5.1.</span> <span class="nav-text">等变性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E5%8F%98%E6%80%A7"><span class="nav-number">22.5.2.</span> <span class="nav-text">不变性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E6%9D%83%E5%80%BC%E5%85%B1%E4%BA%AB%E7%9A%84%E7%90%86%E8%A7%A3%EF%BC%9F"><span class="nav-number">22.6.</span> <span class="nav-text">神经网络中权值共享的理解？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E5%BE%AE%E8%B0%83-fine-tuning-%E7%9A%84%E7%90%86%E8%A7%A3%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BF%AE%E6%94%B9%E6%9C%80%E5%90%8E%E5%87%A0%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E5%80%BC%EF%BC%9F"><span class="nav-number">22.7.</span> <span class="nav-text">对微调(fine-tuning)的理解，为什么要修改最后几层神经网络权值？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LeNet-5"><span class="nav-number">22.8.</span> <span class="nav-text">LeNet-5</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AlexNet"><span class="nav-number">22.9.</span> <span class="nav-text">AlexNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout"><span class="nav-number">22.9.1.</span> <span class="nav-text">Dropout</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4Dropout%E5%8F%AF%E4%BB%A5%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="nav-number">22.9.1.1.</span> <span class="nav-text">为什么说Dropout可以解决过拟合？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dropout%E7%BC%BA%E7%82%B9"><span class="nav-number">22.9.1.2.</span> <span class="nav-text">Dropout缺点</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VGG"><span class="nav-number">22.10.</span> <span class="nav-text">VGG</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GoogLeNet"><span class="nav-number">22.11.</span> <span class="nav-text">GoogLeNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ResNet"><span class="nav-number">22.12.</span> <span class="nav-text">ResNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%AE%E7%82%B9"><span class="nav-number">22.12.1.</span> <span class="nav-text">ResNet中的一些亮点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%87%E7%94%A8residual"><span class="nav-number">22.12.2.</span> <span class="nav-text">为什么采用residual?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#residual%E7%BB%93%E6%9E%84"><span class="nav-number">22.12.3.</span> <span class="nav-text">residual结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#residual%E7%9A%84%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F"><span class="nav-number">22.12.3.1.</span> <span class="nav-text">residual的计算方式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ResNet%E4%B8%AD%E4%B8%A4%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84residual"><span class="nav-number">22.12.3.2.</span> <span class="nav-text">ResNet中两种不同的residual</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BatchNormalization"><span class="nav-number">22.12.4.</span> <span class="nav-text">BatchNormalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LayerNormalization"><span class="nav-number">22.12.5.</span> <span class="nav-text">LayerNormalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%94%A8Dropout%EF%BC%9F"><span class="nav-number">22.12.6.</span> <span class="nav-text">ResNet为什么不用Dropout？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DenseNet"><span class="nav-number">22.13.</span> <span class="nav-text">DenseNet</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RNN"><span class="nav-number">23.</span> <span class="nav-text">RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-number">23.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81RNN%EF%BC%9F"><span class="nav-number">23.2.</span> <span class="nav-text">为什么需要RNN？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E4%B8%BB%E8%A6%81%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F"><span class="nav-number">23.3.</span> <span class="nav-text">RNN的主要应用领域</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="nav-number">23.4.</span> <span class="nav-text">RNN的计算过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E5%BB%BA%E6%A8%A1%E6%96%B9%E5%BC%8F"><span class="nav-number">23.5.</span> <span class="nav-text">RNN的建模方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E5%AF%B9%E5%A4%9A-vector-to-sequence"><span class="nav-number">23.5.1.</span> <span class="nav-text">一对多(vector-to-sequence)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%AF%B9%E4%B8%80-sequence-to-vector"><span class="nav-number">23.5.2.</span> <span class="nav-text">多对一(sequence-to-vector)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%AF%B9%E5%A4%9A-Encoder-Decoder"><span class="nav-number">23.5.3.</span> <span class="nav-text">多对多(Encoder-Decoder)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E4%B8%AD%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%87%BA%E7%8E%B0%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%EF%BC%9F%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="nav-number">23.6.</span> <span class="nav-text">RNN中为什么会出现梯度消失？如何解决？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">23.7.</span> <span class="nav-text">RNN的注意力机制</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LSTM"><span class="nav-number">24.</span> <span class="nav-text">LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9FRNN%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">24.1.</span> <span class="nav-text">传统RNN存在的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM-1"><span class="nav-number">24.2.</span> <span class="nav-text">LSTM</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer"><span class="nav-number">25.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="nav-number">25.1.</span> <span class="nav-text">词嵌入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">25.2.</span> <span class="nav-text">注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-attention"><span class="nav-number">25.2.1.</span> <span class="nav-text">Self-attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cross-attention"><span class="nav-number">25.2.2.</span> <span class="nav-text">Cross-attention</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-Encoder"><span class="nav-number">25.3.</span> <span class="nav-text">编码器(Encoder)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E6%8B%A9%E9%99%A4%E4%BB%A5-sqrt-d"><span class="nav-number">25.3.1.</span> <span class="nav-text">为什么选择除以$\sqrt{d}$</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8-Decoder"><span class="nav-number">25.4.</span> <span class="nav-text">解码器(Decoder)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BE%93%E5%87%BA"><span class="nav-number">25.5.</span> <span class="nav-text">输出</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-Positional-Encoding"><span class="nav-number">25.6.</span> <span class="nav-text">位置编码(Positional Encoding)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanilla-Transformer%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%84%E7%89%B9%E7%82%B9"><span class="nav-number">25.6.1.</span> <span class="nav-text">Vanilla Transformer的位置编码的特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E7%BC%96%E7%A0%81%E6%96%B9%E5%BC%8F"><span class="nav-number">25.6.2.</span> <span class="nav-text">其他编码方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanilla-Transformer%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%84%E7%BC%BA%E7%82%B9%E4%BB%A5%E5%8F%8A%E6%94%B9%E8%BF%9B"><span class="nav-number">25.6.3.</span> <span class="nav-text">Vanilla Transformer位置编码的缺点以及改进</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Vision-Transformer"><span class="nav-number">26.</span> <span class="nav-text">Vision Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%84%E6%88%90"><span class="nav-number">26.1.</span> <span class="nav-text">模型组成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Embedding%E5%B1%82%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3"><span class="nav-number">26.2.</span> <span class="nav-text">Embedding层结构详解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer-Encoder%E8%AF%A6%E8%A7%A3"><span class="nav-number">26.3.</span> <span class="nav-text">Transformer Encoder详解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MLP-Head%E8%AF%A6%E8%A7%A3"><span class="nav-number">26.4.</span> <span class="nav-text">MLP Head详解</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Swin-Transformer"><span class="nav-number">27.</span> <span class="nav-text">Swin Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84"><span class="nav-number">27.1.</span> <span class="nav-text">整体结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Patch-merging"><span class="nav-number">27.2.</span> <span class="nav-text">Patch merging</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#W-MSA"><span class="nav-number">27.3.</span> <span class="nav-text">W-MSA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SW-MSA"><span class="nav-number">27.4.</span> <span class="nav-text">SW-MSA</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="nav-number">28.</span> <span class="nav-text">推荐系统</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">28.1.</span> <span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84"><span class="nav-number">28.2.</span> <span class="nav-text">推荐系统架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84"><span class="nav-number">28.2.1.</span> <span class="nav-text">系统架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%9E%B6%E6%9E%84"><span class="nav-number">28.2.2.</span> <span class="nav-text">算法架构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B"><span class="nav-number">28.3.</span> <span class="nav-text">经典召回模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-number">28.3.1.</span> <span class="nav-text">评估指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E5%8F%AC%E5%9B%9E"><span class="nav-number">28.3.2.</span> <span class="nav-text">基于协同过滤的召回</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%89%A9%E5%93%81%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95"><span class="nav-number">28.3.2.1.</span> <span class="nav-text">基于物品的协同过滤算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span class="nav-number">28.3.2.1.1.</span> <span class="nav-text">算法步骤</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="nav-number">28.3.2.1.2.</span> <span class="nav-text">相似度计算</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C%E9%A2%84%E6%B5%8B"><span class="nav-number">28.3.2.1.3.</span> <span class="nav-text">结果预测</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95"><span class="nav-number">28.3.2.2.</span> <span class="nav-text">基于用户的协同过滤算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4-1"><span class="nav-number">28.3.2.2.1.</span> <span class="nav-text">算法步骤</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97-1"><span class="nav-number">28.3.2.2.2.</span> <span class="nav-text">相似度计算</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C%E9%A2%84%E6%B5%8B-1"><span class="nav-number">28.3.2.2.3.</span> <span class="nav-text">结果预测</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Swing-Graph-based"><span class="nav-number">28.3.2.3.</span> <span class="nav-text">Swing(Graph-based)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%89%8D%E8%BF%B0%E6%96%B9%E6%B3%95%E5%B1%80%E9%99%90%E6%80%A7"><span class="nav-number">28.3.2.3.1.</span> <span class="nav-text">前述方法局限性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Swing%E7%AE%97%E6%B3%95"><span class="nav-number">28.3.2.3.2.</span> <span class="nav-text">Swing算法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Surprise%E7%AE%97%E6%B3%95"><span class="nav-number">28.3.2.3.3.</span> <span class="nav-text">Surprise算法</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95"><span class="nav-number">28.3.2.4.</span> <span class="nav-text">基于模型的协同过滤算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3"><span class="nav-number">28.3.2.4.1.</span> <span class="nav-text">矩阵分解</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="nav-number">28.3.2.4.1.1.</span> <span class="nav-text">算法原理</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3%E6%96%B9%E6%B3%95"><span class="nav-number">28.3.2.4.1.2.</span> <span class="nav-text">求解方法</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E7%9A%84%E6%8B%93%E5%B1%95%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="nav-number">28.3.2.4.1.3.</span> <span class="nav-text">矩阵分解推荐算法的拓展与优化</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%90%91%E9%87%8F%E7%9A%84%E5%8F%AC%E5%9B%9E"><span class="nav-number">28.3.3.</span> <span class="nav-text">基于向量的召回</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#FM%E5%8F%AC%E5%9B%9E"><span class="nav-number">28.3.3.1.</span> <span class="nav-text">FM召回</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#item2vec%E5%8F%AC%E5%9B%9E%E7%B3%BB%E5%88%97"><span class="nav-number">28.3.3.2.</span> <span class="nav-text">item2vec召回系列</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Word2vec%E5%9F%BA%E7%A1%80"><span class="nav-number">28.3.3.2.1.</span> <span class="nav-text">Word2vec基础</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Word2vec%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">28.3.3.2.2.</span> <span class="nav-text">Word2vec模型结构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#item2Vec%E6%A8%A1%E5%9E%8B"><span class="nav-number">28.3.3.2.3.</span> <span class="nav-text">item2Vec模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Airbnb%E5%8F%AC%E5%9B%9E"><span class="nav-number">28.3.3.2.4.</span> <span class="nav-text">Airbnb召回</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#YouTubeDNN%E5%8F%AC%E5%9B%9E"><span class="nav-number">28.3.3.3.</span> <span class="nav-text">YouTubeDNN召回</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8C%E5%A1%94%E5%8F%AC%E5%9B%9E"><span class="nav-number">28.3.3.4.</span> <span class="nav-text">双塔召回</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8%E5%8F%8C%E5%A1%94"><span class="nav-number">28.3.3.4.1.</span> <span class="nav-text">经典双塔</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B"><span class="nav-number">28.3.3.4.1.1.</span> <span class="nav-text">经典双塔模型</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#SENet%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B"><span class="nav-number">28.3.3.4.1.2.</span> <span class="nav-text">SENet双塔模型</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%A4%9A%E7%9B%AE%E6%A0%87%E7%9A%84%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B"><span class="nav-number">28.3.3.4.1.3.</span> <span class="nav-text">多目标的双塔模型</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-number">28.3.3.4.1.4.</span> <span class="nav-text">模型的应用</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Youtube%E5%8F%8C%E5%A1%94"><span class="nav-number">28.3.3.4.2.</span> <span class="nav-text">Youtube双塔</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%B5%81%E7%A8%8B"><span class="nav-number">28.3.3.4.2.1.</span> <span class="nav-text">模型流程</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%9A%84%E5%8F%AC%E5%9B%9E"><span class="nav-number">28.3.4.</span> <span class="nav-text">基于图的召回</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#EGES"><span class="nav-number">28.3.4.1.</span> <span class="nav-text">EGES</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E7%89%A9%E5%93%81%E5%9B%BE"><span class="nav-number">28.3.4.1.1.</span> <span class="nav-text">构建物品图</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%BE%E5%B5%8C%E5%85%A5-BGE"><span class="nav-number">28.3.4.1.2.</span> <span class="nav-text">图嵌入(BGE)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8Eside-information%E7%9A%84%E5%9B%BE%E5%B5%8C%E5%85%A5%EF%BC%88GES%EF%BC%89"><span class="nav-number">28.3.4.1.3.</span> <span class="nav-text">基于side information的图嵌入（GES）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A2%9E%E5%BC%BA%E5%9E%8BEGS%EF%BC%88EGES%EF%BC%89"><span class="nav-number">28.3.4.1.4.</span> <span class="nav-text">增强型EGS（EGES）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PinSAGE"><span class="nav-number">28.3.4.2.</span> <span class="nav-text">PinSAGE</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#GraphSAGE%E5%8E%9F%E7%90%86"><span class="nav-number">28.3.4.2.1.</span> <span class="nav-text">GraphSAGE原理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GraphSAGE%E7%9A%84%E9%87%87%E6%A0%B7%E5%92%8C%E8%81%9A%E5%90%88"><span class="nav-number">28.3.4.2.2.</span> <span class="nav-text">GraphSAGE的采样和聚合</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#PinSAGE-1"><span class="nav-number">28.3.4.2.3.</span> <span class="nav-text">PinSAGE</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%BA%8F%E5%88%97%E7%9A%84%E5%8F%AC%E5%9B%9E"><span class="nav-number">28.3.5.</span> <span class="nav-text">基于序列的召回</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MIND"><span class="nav-number">28.3.5.1.</span> <span class="nav-text">MIND</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E5%8A%A8%E6%9C%BA"><span class="nav-number">28.3.5.1.1.</span> <span class="nav-text">背景与动机</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1%E6%9C%BA%E5%88%B6"><span class="nav-number">28.3.5.1.2.</span> <span class="nav-text">胶囊网络与动态路由机制</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9C%E5%88%9D%E8%AF%86"><span class="nav-number">28.3.5.1.2.1.</span> <span class="nav-text">胶囊网络初识</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1%E6%9C%BA%E5%88%B6%E5%8E%9F%E7%90%86"><span class="nav-number">28.3.5.1.2.2.</span> <span class="nav-text">动态路由机制原理</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#MIND%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E4%B8%8E%E7%BB%86%E8%8A%82%E5%89%96%E6%9E%90"><span class="nav-number">28.3.5.1.3.</span> <span class="nav-text">MIND模型的网络结构与细节剖析</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84"><span class="nav-number">28.3.5.1.3.1.</span> <span class="nav-text">网络整体结构</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1%E7%9B%AE%E6%A0%87"><span class="nav-number">28.3.5.1.3.2.</span> <span class="nav-text">任务目标</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Embedding-amp-Pooling%E5%B1%82"><span class="nav-number">28.3.5.1.3.3.</span> <span class="nav-text">Embedding &amp; Pooling层</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Multi-Interest-Extractor-Layer-%E6%A0%B8%E5%BF%83"><span class="nav-number">28.3.5.1.3.4.</span> <span class="nav-text">Multi-Interest Extractor Layer(核心)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Label-aware-Attention-Layer"><span class="nav-number">28.3.5.1.3.5.</span> <span class="nav-text">Label-aware Attention Layer</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%9C%8D%E5%8A%A1"><span class="nav-number">28.3.5.1.3.6.</span> <span class="nav-text">训练与服务</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SDM"><span class="nav-number">28.3.5.2.</span> <span class="nav-text">SDM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E5%8A%A8%E6%9C%BA-1"><span class="nav-number">28.3.5.2.1.</span> <span class="nav-text">背景与动机</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SDM%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E4%B8%8E%E7%BB%86%E8%8A%82%E5%89%96%E6%9E%90"><span class="nav-number">28.3.5.2.2.</span> <span class="nav-text">SDM的网络结构与细节剖析</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89"><span class="nav-number">28.3.5.2.2.1.</span> <span class="nav-text">问题定义</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Input-Embedding-with-side-Information"><span class="nav-number">28.3.5.2.2.2.</span> <span class="nav-text">Input Embedding with side Information</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%9F%AD%E6%9C%9F%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1"><span class="nav-number">28.3.5.2.2.3.</span> <span class="nav-text">短期用户行为建模</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%94%A8%E6%88%B7%E9%95%BF%E6%9C%9F%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1"><span class="nav-number">28.3.5.2.2.4.</span> <span class="nav-text">用户长期行为建模</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%9F%AD%E9%95%BF%E6%9C%9F%E5%85%B4%E8%B6%A3%E8%9E%8D%E5%90%88"><span class="nav-number">28.3.5.2.2.5.</span> <span class="nav-text">短长期兴趣融合</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A0%91%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%AC%E5%9B%9E"><span class="nav-number">28.3.6.</span> <span class="nav-text">基于树模型的召回</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TDM"><span class="nav-number">28.3.6.1.</span> <span class="nav-text">TDM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">28.3.6.1.1.</span> <span class="nav-text">模型结构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3"><span class="nav-number">28.3.6.1.2.</span> <span class="nav-text">算法详解</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B"><span class="nav-number">28.4.</span> <span class="nav-text">经典排序模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT-LR"><span class="nav-number">28.4.1.</span> <span class="nav-text">GBDT+LR</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="nav-number">28.4.1.1.</span> <span class="nav-text">逻辑回归模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT-1"><span class="nav-number">28.4.1.2.</span> <span class="nav-text">GBDT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GBDT-LR%E6%A8%A1%E5%9E%8B"><span class="nav-number">28.4.1.3.</span> <span class="nav-text">GBDT+LR模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89"><span class="nav-number">28.4.2.</span> <span class="nav-text">特征交叉</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#FM%E6%A8%A1%E5%9E%8B"><span class="nav-number">28.4.2.1.</span> <span class="nav-text">FM模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81FM%E6%A8%A1%E5%9E%8B"><span class="nav-number">28.4.2.1.1.</span> <span class="nav-text">为什么需要FM模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#FM%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">28.4.2.1.2.</span> <span class="nav-text">FM模型的应用场景</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#FM%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B7%E4%BD%93%E5%BD%A2%E5%BC%8F"><span class="nav-number">28.4.2.1.3.</span> <span class="nav-text">FM模型的具体形式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#FM%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="nav-number">28.4.2.1.4.</span> <span class="nav-text">FM模型的解决方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#FM%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="nav-number">28.4.2.1.5.</span> <span class="nav-text">FM模型的训练</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#FM%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%AB%98%E7%BB%B4%E6%89%A9%E5%B1%95"><span class="nav-number">28.4.2.1.6.</span> <span class="nav-text">FM模型的高维扩展</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#FFM%E6%A8%A1%E5%9E%8B"><span class="nav-number">28.4.2.2.</span> <span class="nav-text">FFM模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#FFM%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E7%BB%84%E5%90%88%E6%96%B9%E5%BC%8F"><span class="nav-number">28.4.2.2.1.</span> <span class="nav-text">FFM模型的特征组合方式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#FFM%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">28.4.2.2.2.</span> <span class="nav-text">FFM模型的应用场景</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PNN"><span class="nav-number">28.4.2.3.</span> <span class="nav-text">PNN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%8F%8A%E5%8E%9F%E7%90%86"><span class="nav-number">28.4.2.3.1.</span> <span class="nav-text">模型结构及原理</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DCN"><span class="nav-number">28.4.2.4.</span> <span class="nav-text">DCN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%8F%8A%E5%8E%9F%E7%90%86-1"><span class="nav-number">28.4.2.4.1.</span> <span class="nav-text">模型结构及原理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Embedding%E5%92%8CStacking-%E5%B1%82"><span class="nav-number">28.4.2.4.2.</span> <span class="nav-text">Embedding和Stacking 层</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Cross-Network"><span class="nav-number">28.4.2.4.3.</span> <span class="nav-text">Cross Network</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Deep-Network"><span class="nav-number">28.4.2.4.4.</span> <span class="nav-text">Deep Network</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%84%E5%90%88%E8%BE%93%E5%87%BA%E5%B1%82"><span class="nav-number">28.4.2.4.5.</span> <span class="nav-text">组合输出层</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AutoInt"><span class="nav-number">28.4.2.5.</span> <span class="nav-text">AutoInt</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA%E5%92%8C%E5%8E%9F%E7%90%86"><span class="nav-number">28.4.2.5.1.</span> <span class="nav-text">动机和原理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#AutoInt%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E5%90%91%E8%BF%87%E7%A8%8B%E6%A2%B3%E7%90%86"><span class="nav-number">28.4.2.5.2.</span> <span class="nav-text">AutoInt模型的前向过程梳理</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Input-Layer"><span class="nav-number">28.4.2.5.2.1.</span> <span class="nav-text">Input Layer</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Embedding-Layer"><span class="nav-number">28.4.2.5.2.2.</span> <span class="nav-text">Embedding Layer</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Interacting-Layer"><span class="nav-number">28.4.2.5.2.3.</span> <span class="nav-text">Interacting Layer</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Output-Layer"><span class="nav-number">28.4.2.5.2.4.</span> <span class="nav-text">Output Layer</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#AutoInt%E7%9A%84%E5%88%86%E6%9E%90"><span class="nav-number">28.4.2.5.3.</span> <span class="nav-text">AutoInt的分析</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#FiBiNet"><span class="nav-number">28.4.2.6.</span> <span class="nav-text">FiBiNet</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E5%8F%8A%E7%BB%86%E8%8A%82"><span class="nav-number">28.4.2.6.1.</span> <span class="nav-text">模型原理及细节</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Embedding-Layer-1"><span class="nav-number">28.4.2.6.1.1.</span> <span class="nav-text">Embedding Layer</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#SENET-Layer"><span class="nav-number">28.4.2.6.1.2.</span> <span class="nav-text">SENET Layer</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Bilinear-Interaction-Layer"><span class="nav-number">28.4.2.6.1.3.</span> <span class="nav-text">Bilinear-Interaction Layer</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Combination-Layer"><span class="nav-number">28.4.2.6.1.4.</span> <span class="nav-text">Combination Layer</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#DNN%E5%92%8C%E8%BE%93%E5%87%BA%E5%B1%82"><span class="nav-number">28.4.2.6.1.5.</span> <span class="nav-text">DNN和输出层</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Wide-amp-Deep%E7%B3%BB%E5%88%97"><span class="nav-number">28.4.3.</span> <span class="nav-text">Wide&amp;Deep系列</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Wide-amp-Deep"><span class="nav-number">28.4.3.1.</span> <span class="nav-text">Wide&amp;Deep</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA"><span class="nav-number">28.4.3.1.1.</span> <span class="nav-text">动机</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%8F%8A%E5%8E%9F%E7%90%86-2"><span class="nav-number">28.4.3.1.2.</span> <span class="nav-text">模型结构及原理</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NFM"><span class="nav-number">28.4.3.2.</span> <span class="nav-text">NFM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA-1"><span class="nav-number">28.4.3.2.1.</span> <span class="nav-text">动机</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E4%B8%8E%E5%8E%9F%E7%90%86"><span class="nav-number">28.4.3.2.2.</span> <span class="nav-text">模型结构与原理</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Input-%E5%92%8CEmbedding%E5%B1%82"><span class="nav-number">28.4.3.2.2.1.</span> <span class="nav-text">Input 和Embedding层</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Bi-Interaction-Pooling-layer"><span class="nav-number">28.4.3.2.2.2.</span> <span class="nav-text">Bi-Interaction Pooling layer</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82"><span class="nav-number">28.4.3.2.2.3.</span> <span class="nav-text">隐藏层</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E5%B1%82"><span class="nav-number">28.4.3.2.2.4.</span> <span class="nav-text">预测层</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AFM"><span class="nav-number">28.4.3.3.</span> <span class="nav-text">AFM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#AFM%E6%8F%90%E5%87%BA%E7%9A%84%E5%8A%A8%E6%9C%BA"><span class="nav-number">28.4.3.3.1.</span> <span class="nav-text">AFM提出的动机</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#AFM%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86"><span class="nav-number">28.4.3.3.2.</span> <span class="nav-text">AFM模型原理</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Pair-wise-Interaction-Layer"><span class="nav-number">28.4.3.3.2.1.</span> <span class="nav-text">Pair-wise Interaction Layer</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Attention-based-Pooling"><span class="nav-number">28.4.3.3.2.2.</span> <span class="nav-text">Attention-based Pooling</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#AFM%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">28.4.3.3.2.3.</span> <span class="nav-text">AFM模型训练</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DeepFM"><span class="nav-number">28.4.3.4.</span> <span class="nav-text">DeepFM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA-2"><span class="nav-number">28.4.3.4.1.</span> <span class="nav-text">动机</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%93%E6%9E%84%E4%B8%8E%E5%8E%9F%E7%90%86"><span class="nav-number">28.4.3.4.2.</span> <span class="nav-text">模型的结构与原理</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#FM"><span class="nav-number">28.4.3.4.2.1.</span> <span class="nav-text">FM</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Deep"><span class="nav-number">28.4.3.4.2.2.</span> <span class="nav-text">Deep</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#xDeepFM"><span class="nav-number">28.4.3.5.</span> <span class="nav-text">xDeepFM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#xDeepFM%E7%9A%84%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90"><span class="nav-number">28.4.3.5.1.</span> <span class="nav-text">xDeepFM的架构剖析</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#CIN%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%86%E8%8A%82"><span class="nav-number">28.4.3.5.2.</span> <span class="nav-text">CIN网络的细节</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="nav-number">28.4.4.</span> <span class="nav-text">序列模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%A8%A1%E5%9E%8B"><span class="nav-number">28.4.5.</span> <span class="nav-text">多任务模型</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">木霈玖</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/06/%E9%9D%A2%E7%BB%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="木霈玖">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="木霈玖的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          面经
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-03-06 22:04:43" itemprop="dateCreated datePublished" datetime="2023-03-06T22:04:43+08:00">2023-03-06</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2023-08-10 15:39:08" itemprop="dateModified" datetime="2023-08-10T15:39:08+08:00">2023-08-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">总结</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>119k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:48</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>记录一些自己整理的面经。</p>
<a id="more"></a>
<h1 id="训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些"><a href="#训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些" class="headerlink" title="训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些?"></a>训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些?</h1><p>在训练过程中，如果模型不收敛并不能说明该模型时无效的。</p>
<p>导致模型不收敛的原因包括：</p>
<ol>
<li>没有对数据做归一化处理。</li>
<li>没有使用正则化。</li>
<li><code>Batch Size</code>设的太大。</li>
<li>学习率设置的太大容易产生震荡，太小会导致不收敛。</li>
<li>没有做数据预处理。</li>
<li>没有检查过预处理结果和最终的训练测试结果。</li>
<li>网络存在坏梯度，比如当<code>ReLU</code>对负值的梯度为 $0$ ，反向传播时，梯度为 $0$ 表示不传播。</li>
<li>网络设定不合理，网络太浅或者太深。</li>
<li>最后一层的激活函数错误。</li>
<li>参数初始化错误。</li>
<li>隐藏层神经元数量错误。</li>
<li>数据集标签的设置有错误。</li>
</ol>
<h1 id="特征归一化"><a href="#特征归一化" class="headerlink" title="特征归一化"></a>特征归一化</h1><ul>
<li><p>训练数据集归一化很好理解，用于训练模型。</p>
</li>
<li><p>不能直接对测试数据集按公式进行归一化，而是要使用训练数据集的均值和方差对测试数据集归一化。</p>
<ul>
<li>原因1：真实的环境中，数据会源源不断输出进模型，无法求取均值和方差的。</li>
<li>原因2：训练数据集是模拟真实环境中的数据，不能直接使用自身的均值和方差。</li>
<li>原因3：真实环境中，无法对单个数据进行归一化。</li>
</ul>
</li>
</ul>
<h1 id="加快模型训练速度的方法"><a href="#加快模型训练速度的方法" class="headerlink" title="加快模型训练速度的方法"></a>加快模型训练速度的方法</h1><ul>
<li>合理的超参数设计</li>
<li>权值共享</li>
<li>升级相关软件包</li>
<li>多卡训练、数据并行</li>
<li>混合精度训练</li>
</ul>
<h1 id="Zero-Shot-One-Shot-Few-Shot"><a href="#Zero-Shot-One-Shot-Few-Shot" class="headerlink" title="Zero-Shot,One-Shot,Few-Shot"></a>Zero-Shot,One-Shot,Few-Shot</h1><p>Zero-Shot Learning（零样本学习）：利用训练集数据训练模型，使得模型能够对测试集的对象进行分类，但是训练集类别和测试集类别之间没有交集；期间需要借助类别的描述，来建立训练集和测试集之间的联系，从而使得模型有效。</p>
<p>Few-Shot Learning（小样本学习）：我们只通过看几张鸭嘴兽的照片，就能认识鸭嘴兽，这个过程就称作小样本学习。</p>
<p>One-Shot Learning（单样本学习）：Few-Shot Learning的特殊情况，即只看了一张鸭嘴兽的照片，就认识了鸭嘴兽。（人脸识别）</p>
<h1 id="PyTorch和TensorFlow的区别"><a href="#PyTorch和TensorFlow的区别" class="headerlink" title="PyTorch和TensorFlow的区别"></a>PyTorch和TensorFlow的区别</h1><h2 id="动态图与静态图"><a href="#动态图与静态图" class="headerlink" title="动态图与静态图"></a>动态图与静态图</h2><p><code>TensorFlow</code>最初选择使用静态图，这样的设计带来了较高的性能，但在构建网络时较为烦琐，用户需要专门学习<code>TensorFlow</code>的语法架构才能搭建网络，同时很难调试。<code>PyTorch</code>选择使用动态图，动态图的设计模式更加符合人类的思考过程，方便查看、修改中间变量的值，用户可以轻松地搭建网络进行训练。目前，<code>TensorFlow 2.0</code>之后的版本已经支持动态图的构建，并且提供动态图与静态图的转换功能。</p>
<ul>
<li>静态图</li>
</ul>
<p>静态图的生成与执行采用先编译后执行的方式，该模式将计算图的定义和执行进行分离。</p>
<p>静态图只建一次，然后不断复用它，容易在图上做优化，图的效率更高（比如Add操作和ReLU结合在一起优化）。</p>
<p>静态图可以在磁盘中序列化，可以保存整个网络的结构，可以重载，在部署中很实用。</p>
<p>静态图中条件和循环需要特定的语法（tf.condition和tf.while_loop）。</p>
<ul>
<li>动态图</li>
</ul>
<p>动态图采用解析式的执行方式，其核心特点是编译与执行同时发生。</p>
<p>动态图每次使用时建立，不容易优化。</p>
<p>动态图需要重复之前的代码。</p>
<p>只用Python的语法就可以实现条件和循环。</p>
<h2 id="部署和可扩展性"><a href="#部署和可扩展性" class="headerlink" title="部署和可扩展性"></a>部署和可扩展性</h2><p>如果您的项目范围很大，需要大规模部署，或者项目涉及跨平台，那么您的选择应该是<code>TensorFlow</code>，<code>TensorFlow</code>提供了<code>TensorFlow Serving</code>和<code>TensorFlow Lite</code>，可以便捷地将训练好的模型部署到集群以及移动设备上。如果它只是一个较小规模的研究项目的原型设计或类似的东西，那么<code>PyTorch</code>更好。</p>
<h2 id="资源优化和利用率"><a href="#资源优化和利用率" class="headerlink" title="资源优化和利用率"></a>资源优化和利用率</h2><p>如果您正在寻找更好的资源利用率和优化，如<code>GPU</code>，那么<code>PyTorch</code>肯定是首选。但是，当涉及到<code>TensorFlow</code>时，它使用当时可用的所有<code>GPU</code>容量，因此，功能略微缓慢。</p>
<h2 id="学术研究和开源代码"><a href="#学术研究和开源代码" class="headerlink" title="学术研究和开源代码"></a>学术研究和开源代码</h2><p>在学术界<code>PyTorch</code>有很多好评，其中四分之三的论文使用它。而且最早使用<code>TensorFlow</code>的研究人员中，很多人已经迁移到了<code>PyTorch</code>。正因如此，研究会影响教学，从而决定学生学到的是什么。所以如今，大学生对<code>PyTorch</code>了解的相对多一些。</p>
<h1 id="Pytorch多卡训练"><a href="#Pytorch多卡训练" class="headerlink" title="Pytorch多卡训练"></a>Pytorch多卡训练</h1><p>对于pytorch而言，有两种方式进行并行：数据并行（DataParallel，DP）和分布式数据并行（DistributedDataParallel，DDP）。</p>
<p>在多卡训练的实现上，DP与DDP的思路是相似的：</p>
<ol>
<li>每张卡都复制一个有相同参数的模型副本。</li>
<li>每次迭代，每张卡分别输入不同批次数据，分别计算梯度。</li>
<li>DP与DDP的主要不同在于接下来的多卡通信：</li>
</ol>
<p>DP的多卡交互实现在一个进程之中，它将一张卡视为主卡，维护单独模型优化器。所有卡计算完梯度后，主卡汇聚其它卡的梯度进行平均并用优化器更新模型参数，再将模型参数更新至其它卡上。</p>
<p>DDP则分别为每张卡创建一个进程，每个进程相应的卡上都独立维护模型和优化器。在每次每张卡计算完梯度之后，进程之间以NCLL（NVIDIA GPU通信）为通信后端，使各卡获取其它卡的梯度。各卡对获取的梯度进行平均，然后执行后续的参数更新。由于每张卡上的模型与优化器参数在初始化时就保持一致，而每次迭代的平均梯度也保持一致，那么即使没有进行参数复制，所有卡的模型参数也是保持一致的。</p>
<h1 id="提升模型泛化能力的方法"><a href="#提升模型泛化能力的方法" class="headerlink" title="提升模型泛化能力的方法"></a>提升模型泛化能力的方法</h1><ul>
<li>从数据角度上来说。可以通过数据增强、扩充训练集等方法提高泛化能力。</li>
<li>在训练策略上，可以增加每个<code>Batch Size</code>的大小，进而让模型每次迭代时见到更多数据，防止过拟合。</li>
<li>调整数据分布，做训练数据集的类别均衡。</li>
<li>调整网络结构。如果数据集较小，可以降低模型复杂度防止过拟合。如果数据集较大，可以尝试更加复杂的模型。</li>
<li>减少过拟合的方法也可以提升模型的泛化能力。</li>
</ul>
<h1 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h1><p>偏差：模型预测值的期望与真实值之间的差异，反应的是模型的拟合能力。<br>方差：反应的是训练集的变化所导致的学习性能的变化，即刻画了<strong>数据扰动</strong>所造成的影响，模型过拟合时会出现较大的方差。</p>
<h1 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>过拟合就是随着模型的训练，模型在训练集上的表现越来越好，但是在验证集上的表现却越来越差，也就是说对训练集的拟合程度过高，导致模型的泛化能力降低。<br>欠拟合就是模型在训练集上也无法达到满意的精度。</p>
<h2 id="过拟合的解决方案"><a href="#过拟合的解决方案" class="headerlink" title="过拟合的解决方案"></a>过拟合的解决方案</h2><p>过拟合通常是由于训练数据过少、模型复杂度过大等问题导致的，因此相应的解决方案也是从这两个角度考虑。</p>
<ul>
<li>使用更多的数据进行训练，并且数据集尽量均匀，做一些数据增强。</li>
<li>降低模型复杂度。并不是说模型越复杂越好，如果模型过于复杂，而训练集又较少，那么参数就很容易拟合到一个过于适配训练集的参数空间中，自然就会导致过拟合的出现。</li>
<li><code>Early Stopping</code>，简单来说就是减少迭代次数。随着训练次数的增加，模型对训练数据的拟合程度也会随之增高，所以也可以通过减少训练时间的方法避免过拟合，提高模型泛化能力。</li>
<li><code>L1</code>、<code>L2</code>正则化方法，限制模型权重。</li>
<li>在数据中增加一些噪声，从而通过影响损失函数的优化方向避免过拟合。</li>
<li><code>Dropout</code>，目的也是降低模型的复杂度。</li>
<li><code>ResNet</code>。是的，<code>ResNet</code>也可以解决过拟合问题，因为<code>ResNet</code>的跳线结构可以让部分参数权重归零，进而达到类似于<code>Dropout</code>的效果。</li>
</ul>
<h2 id="欠拟合的解决方案"><a href="#欠拟合的解决方案" class="headerlink" title="欠拟合的解决方案"></a>欠拟合的解决方案</h2><p>欠拟合通常是由于模型表征能力不足、数据量过大导致的，刚好和过拟合相反。</p>
<ul>
<li>使用更复杂的模型。</li>
<li>增加迭代次数。</li>
<li>减少数据中的噪声。</li>
</ul>
<h1 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h1><h2 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h2><p>梯度消失就是指在网络反向传播过程中由于链式求导法则不断的累积，如果每一层的梯度都小于 $1$ ，由于累乘效应，出现了某些参数的梯度非常小的现象。在使用这些梯度更新梯度的时候参数值基本没有发生变化，因此就出现了网络训练停滞、模型无法继续优化的问题。</p>
<p>梯度爆炸与之刚好相反，在网络反向传播过程中由于链式求导法则的累乘效应，在每一层梯度都大于 $1$ 的时候，就可能会出现某些参数的梯度非常大。在使用这些梯度更新参数的时候就会导致参数变化过大，就会出现损失函数震荡的现象。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol>
<li>预训练和<code>fine-tuning</code>就是将一些在公开训练集上训练好的模型参数加载到自己对应的模型中，这样损失函数通常就能稳定的优化。</li>
<li>梯度裁剪：梯度裁剪是一个针对梯度爆炸的解决方案，也就是说将梯度限制在某个阈值范围内，如果梯度超过的这个阈值，那么就将其设置为这个阈值。</li>
<li>正则化：正则化也是一种限制梯度爆炸的解决方案，同时也有限制过拟合的作用。</li>
<li>使用<code>ReLU</code>、<code>Leaky ReLU</code>、<code>ELU</code>等激活函数：梯度消失通常是因为损失函数选择 <code>Sigmoid</code> 导致的，而<code>ReLU</code>激活函数在正数部分梯度是恒等于 $1$ 的，由于 $1$ 不会累积加权的特性，自然就可以避免梯度消失或梯度爆炸现象。但是<code>ReLU</code>同样有缺点，作为分段函数，<code>ReLU</code>在负数部分恒为 $0$ ，导致一些神经元无法被激活。而<code>Leaky ReLU</code>、<code>ELU</code>就可以避免这个问题。</li>
<li><code>BN(Batch Normalization)</code>：<code>BN</code>可以加速网络收敛提升训练的稳定性，它把每一层神经网络的任意神经元输入值的分布规范为正态分布，如果采用<code>Sigmoid</code>激活函数，那么就可以使得激活函数的输入落在梯度较大的区域，因此就能一定程度解决梯度消失的问题。</li>
<li>使用类似<code>ResNet</code>的跳线结构：由于离输出近的层学习效果好，而由于链式求导法则的影响可能会导致梯度消失或者梯度爆炸，因此可以模仿<code>ResNet</code>在网络的中间增加跳线结构，这样对应层求导梯度时候由于跳线的连接可以增加一个让梯度无损传播的通路，从而避免梯度消失或者梯度爆炸。</li>
<li>采用<code>LSTM</code>等结构：在<code>NLP</code>领域中，<code>LSTM</code>有时也会被用于对抗梯度现象，这是由于其具有复杂的门结构来控制梯度更新。</li>
</ol>
<h1 id="解决样本不均衡的方法"><a href="#解决样本不均衡的方法" class="headerlink" title="解决样本不均衡的方法"></a>解决样本不均衡的方法</h1><ul>
<li>以多种数据组合形式训练模型并做模型融合。顾名思义，这种方法就是将全部的小样本数据和等量的大样本数据分别组合成几批训练数据集，并以此训练出几个不同的模型并做模型融合，这种方法能够有效的解决样本不均衡问题。</li>
<li>在计算损失函数时改变数据的权重，增加小样本数据的权重，减少大样本的权重。这种方法实际上参考了<code>focal loss</code>的思想，只不过解决的不是难易样本不均衡，而是样本数据量不均衡，同样能保证模型的泛化性能。</li>
<li>过采样小样本，欠采样大样本。也就是对于训练数据，把那些样本量较小的数据多在<code>load</code>数据时重复几遍，对于那些样本量较大的数据，则尽量减少<code>load</code>它们，以此来达到样本均衡的目的。例如：SMOTE、ADASYN、bSMOTE算法通过插值生成合成样本进行过采样；使用Tomek Links、Cluster Centroids、NearMiss进行欠采样。</li>
<li>将分类问题看成异常检测问题。</li>
</ul>
<h1 id="LabelSmoothing"><a href="#LabelSmoothing" class="headerlink" title="LabelSmoothing"></a>LabelSmoothing</h1><p>传统做图像分类采用的损失是交叉熵损失，具体形式是： $\mathcal{L}=-\sum_{i=1}^{m}t_{i}\log(y_{i})$ 。其中， $m$ 表示类别数， $y_{i}$ 表示<code>softmax</code>之后每个类的预测概率， $t_{i}$ 表示样本的真实标签值。然而，神经网络有一个坏习惯，就是在训练过程中对预测变得”过于自信“，这可能会降低它们的泛化能力，从而在新的、看不见的未来数据上表现得同样出色。此外，大型数据集通常会包含标签错误的数据，这意味着神经网络在本质上应该对“正确答案”持怀疑态度，以减少一定程度上围绕错误答案的极端情况下的建模。</p>
<p>因此，标签平滑所做的就是通过训练网络向<code>1-adjustment</code>目标移动，然后在其余的类上除以这个<code>adjustment</code>，从而使它对自己的答案不那么自信，而不是简单的设为 $1$ 。新标签的表现形式为：$T^{\prime}=(1-\varepsilon)*T+\frac{\varepsilon}{N}$。其中， $N$ 表示类别的个数， $T$ 表示真实标签值， $T^{\prime}$ 表示平滑后的标签。</p>
<p>例如，原来的标签 $T$ 为 $[0,0,1,0,0]$ ， $\varepsilon=0.1$ ，经过<code>LabelSmoothing</code>之后的标签 $T^{\prime}$ 为 $[0.02,0.02,0.92,0.02,0.02]$ 。</p>
<h1 id="图像处理中平滑和锐化操作是什么？"><a href="#图像处理中平滑和锐化操作是什么？" class="headerlink" title="图像处理中平滑和锐化操作是什么？"></a>图像处理中平滑和锐化操作是什么？</h1><h2 id="概念-2"><a href="#概念-2" class="headerlink" title="概念"></a>概念</h2><p>锐化就是通过增强图像的高频信息，也就是纹理边缘来减少图像中的模糊细节，但是在增强纹理的时候也引入了图像噪声。</p>
<p>平滑处理<code>(smoothing)</code>也称模糊处理<code>(bluring)</code>，主要用于消除图像中的噪声部分，平滑处理常用的用途是用来减少图像上的噪点或失真，平滑主要使用图像滤波。在这里，我个人认为可以把图像平滑和图像滤波联系起来，因为图像平滑常用的方法就是图像滤波器。</p>
<h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><ul>
<li>在计算机视觉的一些任务中，涉及到图像重建的、如高精度的深度估计、医学图像分割、三维重建等任务，最终需要得到原始图像分辨率大小的输出，同时对图像的边缘清晰度也有较高的要求，这时候可以通过增强特征图中的高频分量，在计算损失函数的时候放大这些区域的损失，进而放大对应参数的梯度，使得网络往更突出边缘的方向上优化。</li>
<li>与上相反，如果是一些希望输出更加平滑的任务，则可以考虑对特征图进行平滑操作，进而减小高频区域的损失，减小对应参数的梯度，使得网络往更平滑的方向上优化，通常这种技术都会用在<code>smooth loss</code>中。</li>
</ul>
<h1 id="batchsize"><a href="#batchsize" class="headerlink" title="batchsize"></a>batchsize</h1><ul>
<li><code>batchsize</code>：批大小。在深度学习中，一般采用<code>SGD</code>训练，即每次训练在训练集中取<code>batchsize</code>个样本训练。</li>
<li><code>iteration</code>：1个<code>iteration</code>等于使用<code>batchsize</code>个样本训练一次。</li>
<li><code>epoch</code>：1个<code>epoch</code>等于使用训练集中的全部样本训练一次。</li>
</ul>
<p>如果数据集比较小，则完全可以采用全数据集的形式。这样做的好处有两点：</p>
<ol>
<li>全数据集的方向能够更好的代表样本总体，确定其极值所在。</li>
<li>由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。</li>
</ol>
<p>增大<code>batchsize</code>的好处有三点：</p>
<ol>
<li>内存的利用率提高了，大矩阵乘法的并行化效率提高。</li>
<li>跑完一次<code>epoch</code>（全数据集）所需迭代次数减少，对于相同的数据量的处理速度进一步加快。</li>
<li>一定范围内，<code>batchsize</code>越大，其确定的下降方向就越准，引起训练震荡越小。</li>
</ol>
<p>盲目增大<code>batchsize</code>的坏处有三点：</p>
<ol>
<li>当数据集太大时，内存撑不住。</li>
<li>跑完一次<code>epoch</code>（全数据集）所需迭代次数减少了，但要想达到相同的精度，时间开销太大，参数的修正更加缓慢。</li>
<li><code>batchsize</code>增大到一定的程度，其确定的下降方向已经基本不再变化。</li>
</ol>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="Smooth-L1-Loss"><a href="#Smooth-L1-Loss" class="headerlink" title="Smooth L1 Loss"></a>Smooth L1 Loss</h2><ol>
<li>当预测框与<code>ground truth</code>差别过大时，梯度值不至于过大；</li>
<li>当预测框与<code>ground truth</code>差别很小时，梯度值足够小。</li>
</ol>
<p>为啥要做这两方面的限制呢？</p>
<ol>
<li>差距大时，梯度过于大，可能会导致梯度爆炸；</li>
<li>差距很小时，梯度足够小，能够接近最优点，避免大幅横跳。</li>
</ol>
<p><code>L2</code>、<code>L1</code>、<code>Smooth L1</code>损失函数分别定义为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222347197.png" alt=""></p>
<p>损失函数对 $x$ 的导数分别为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222349037.png" alt=""></p>
<p><code>L2</code>对 $x$ 的导数，与 $x$ 成<strong>正比关系</strong>。也就是当 $x$ 增大时，对 $x$ 的导数也线性增大。这就导致在训练初期，预测值与 <code>groud truth</code> 差异过于大时，损失函数对预测值的梯度十分大，<strong>训练初期不稳定</strong>。</p>
<p><code>L1</code>对 $x$ 的导数为常数，始终为 $1$ 或 $-1$ 。这就导致训练后期，预测值与<code>ground truth</code>差异很小时，损失对预测值的导数的绝对值仍然为 $1$ ，而 <code>learning rate</code>如果不变，损失函数将在稳定值附近波动，难以继续收敛以达到更高精度。</p>
<p><code>Smooth L1</code> 在 $x$ 较小时，也就是 $[-1,1]$ 区间，对 $x$ 的梯度也会变小；而在 $x$ 很大时，对 $x$ 的梯度的绝对值达到上限 $1$ ，也不会太大以至于破坏网络参数。 完美地避开了<code>L1</code> 和 <code>L2</code>损失的缺陷。</p>
<h2 id="Cross-Entropy-Loss"><a href="#Cross-Entropy-Loss" class="headerlink" title="Cross Entropy Loss"></a>Cross Entropy Loss</h2><p>既可用于多分类任务（只有一个能胜出），也可用于二分类任务。设标签为 $y$ （多分类中是one-hot编码），网络预测结果为 $\hat{y}$ ，<code>CE</code>损失函数为（这里只考虑了一个样本，c指的是多分类的类别数目）：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211132940.png" alt=""></p>
<h2 id="Binary-Cross-Entropy-Loss"><a href="#Binary-Cross-Entropy-Loss" class="headerlink" title="Binary Cross Entropy Loss"></a>Binary Cross Entropy Loss</h2><p>二值交叉熵损失，虽然总是用来学习0/1分布，即二分类问题，但不是0/1两个数，只要在0~1之间的数也都能学习。设标签为 $y$ ，网络预测结果为 $\hat{y}$ ，<code>BCE</code>损失函数为（这里考虑了N个样本，每个样本都是二分类）： </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211055521.png" alt=""></p>
<p>某些情况下需要加上权重：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211059473.png" alt=""></p>
<h2 id="Focal-loss"><a href="#Focal-loss" class="headerlink" title="Focal loss"></a>Focal loss</h2><p>Focal Loss的引入主要是为了解决one-stage目标检测中正负样本数量极不平衡问题。当易区分负样本超级多时，整个训练过程将会围绕着易区分负样本进行，进而淹没正样本，造成大损失。所以这里引入了一个调制因子 ，用来聚焦难分样本，公式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306211251919.png" alt=""></p>
<p>当 $p_{t}$ 趋向于1，即说明该样本是易区分样本，此时调制因子是趋向于0，说明对损失的贡献较小，即减低了易区分样本的损失比例。当 $p_{t}$ 很小，也就是假如某个样本被分到正样本，但是该样本为前景的概率特别小，即被错分到正样本了，此时调制因子是趋向于1，对loss也没有太大的影响。</p>
<p>我们在实验中采用了如下的$\alpha$-平衡变体形式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202308101255449.png" alt=""></p>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><p><code>Sigmoid</code>是最基础的激活函数，可以将任意数值转换为概率（缩放到 $0 \thicksim 1$ 之间），在分类等场景中有广泛的应用。<code>Sigmoid</code>函数的形式是 $\sigma(z)=\frac{1}{1+e^{-z}}$ 。其对应的函数图像如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101955821.png" alt=""></p>
<h2 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h2><p>激活函数<code>tanh</code>和<code>Sigmoid</code>类似，都是<code>S</code>形曲线，输出范围是$[-1, 1]$。<code>tanh</code>函数的形式为 $g(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$ 。其对应的函数图像如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102004442.png" alt=""></p>
<h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p><code>ReLU(Rectified Linear Unit)</code>，是一种人工神经网络中常用的激活函数。通常意义下，其指代数学中的斜坡函数，即<br> $f(x)=\max(0,x)$ 。其对应的函数图像如下所示：<br><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101950246.png" alt=""></p>
<h2 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h2><p>为了解决<code>dead ReLU</code>问题（<code>ReLU</code>在训练的时很“脆弱”。在 $x&lt;0$ 时，梯度为 $0$ ，这个神经元及之后的神经元梯度永远为 $0$ ，不再对任何数据有所响应，导致相应参数永远不会被更新）。<code>Leaky ReLU</code>用一个类似 $0.01$ 的小值来初始化神经元，从而使得<code>ReLU</code>在负数区域更偏向于激活而不是坏死，这里的斜率都是确定的。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102008358.png" alt=""></p>
<h2 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h2><p><code>ELU</code>的提出也解决了<code>ReLU</code>的问题。与<code>ReLU</code>相比，<code>ELU</code>有负值，这会使激活的平均值接近零，让模型学习得更快。当 $x&lt;0$ 时，<code>ELU</code>的函数形式为 $f(x)=\alpha(e^{x}-1)$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102010115.png" alt=""></p>
<h2 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h2><p>激活函数<code>GELU</code>的灵感来源于<code>ReLU</code>和<code>Dropout</code>，在激活中引入了<strong>随机正则</strong>的思想。<code>GELU</code>通过输入自身的概率分布情况，决定抛弃还是保留当前的神经元。<code>GELU</code>函数的形式是 $GELU(x)=0.5x(1+\tanh(\sqrt\frac{2}{\pi}(x+0.044715x^{3})))$ 。其对应的函数图像如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102019772.png" alt=""></p>
<p>可以理解为，对于输入的值，根据它的情况乘上 $1$ 或 $0$ 。更「数学」一点的描述是，对于每一个输入 $x$ ，其服从于标准正态分布 $\mathcal{N}(0, 1)$ ，它会乘上一个伯努利分布 $Bernoulli(\Phi(x))$，其中 $\Phi(x) = P(X \leq x)$ 。随着 $x$ 的降低，它被归零的概率会升高。对于<code>ReLU</code>来说，这个界限就是 $0$ ，输入少于零就会被归零。这一类激活函数，不仅保留了概率性，同时也保留了对输入的依赖性。<code>GELU</code>在最近的<code>Transformer</code>模型中（包括<code>BERT</code>，<code>RoBertA</code>和<code>GPT2</code>等）得到了广泛的应用。</p>
<h2 id="ReLU比Sigmoid效果好在哪里？"><a href="#ReLU比Sigmoid效果好在哪里？" class="headerlink" title="ReLU比Sigmoid效果好在哪里？"></a>ReLU比Sigmoid效果好在哪里？</h2><p><code>ReLU</code>的输出要么是 $0$ , 要么是输入本身。虽然方程简单，但实际上效果更好。</p>
<ol>
<li><code>ReLU</code>函数计算简单，可以减少很多计算量。反向传播求误差梯度时，涉及除法，计算量相对较大，采用<code>ReLU</code>激活函数，可以节省很多计算量。</li>
<li>避免梯度消失问题。对于深层网络，<code>Sigmoid</code>函数反向传播时，很容易就会出现梯度消失问题（在<code>Sigmoid</code>接近饱和区时，变换太缓慢，导数趋于 $0$ ，这种情况会造成信息丢失），从而无法完成深层网络的训练。例如在<code>RNN</code>当中，随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于 $0$ ，这就是“梯度消失“现象。此时采用<code>ReLU</code>激活函数就避免了“梯度消失“的发生。</li>
<li>可以缓解过拟合问题的发生，<code>ReLU</code>会使一部分神经元的输出为 $0$ ，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。</li>
</ol>
<h1 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h1><p>记号： $\theta_{t}$ 表示第 $t$ 轮的参数， $\eta$ 表示学习率， $g_{t}$ 表示第 $t$ 轮的梯度即 $\triangledown\hat{\mathcal{L}}(\theta_{t})$ ， $m_{t}$ 表示第 $t$ 轮的一阶动量， $v_{t}$ 表示第 $t$ 轮的二阶动量， $\hat{m}_{t}$ 为偏差纠正后的一阶矩估计， $\hat{v}_{t}$ 为偏差纠正后的二阶矩估计。</p>
<h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p><code>SGD(Stochastic Gradient Descent)</code>，随机梯度下降。每次选择一个<code>mini-batch</code>，而不是全部样本，使用梯度下降来更新模型参数。它解决了随机小批量样本的问题，但仍然有自适应学习率、容易卡在梯度较小点等问题。</p>
<p>$m_{t}=g_{t}, \ v_{t}=1$</p>
<p>$\theta_{t+1}=\theta_{t}-\eta \frac{m_{t}}{\sqrt{v_{t}}}=\theta_{t}-\eta g_{t} $</p>
<p><strong>缺点：</strong>下降速度慢，而且可能会在沟壑的两边持续振荡，停留在一个局部最优点</p>
<h2 id="SGDM"><a href="#SGDM" class="headerlink" title="SGDM"></a>SGDM</h2><p><code>SGDM(SGD with momentum)</code>，在<code>SGD</code>基础上增加一阶动量。  参数更新时以上一个时刻的一阶动量为主，其中 $\beta$ 通常取 $0.9$。</p>
<p>$m_{t} = \beta m_{t-1} + (1-\beta)  g_{t}, \ v_{t} = 1$</p>
<p>$\theta_{t+1} = \theta_{t} - \eta \frac{m_{t}}{\sqrt{v_{t}}} = \theta_{t} - \eta (\beta m_{t-1} + (1-\beta) g_{t}) $</p>
<h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>在<code>SGD</code>基础上增加二阶动量，可以对模型中的每个参数分配自适应学习率。</p>
<p>$m_{t}=g_{t}$</p>
<p>$v_{t}=\sum_{\tau=1}^{t}g_{\tau}^{2}$</p>
<p>$\theta_{t+1}=\theta_{t}-\eta \frac{m_{t}}{\sqrt{v_{t}+\epsilon}}=\theta_{t}-\eta \frac{g_{t}}{\sqrt{\sum_{\tau=1}^{t}g_{\tau}^{2}+\epsilon}}$</p>
<p><strong>优点：Adagrad在稀疏数据场景下表现最好</strong>，因为对于频繁出现的参数，学习率衰减快；对于稀疏的参数，学习率衰减的更慢</p>
<p><strong>缺点：</strong>在实际很多情况下，<strong>二阶动量呈单调递增，累积从训练开始的梯度，学习率会很快减至0，导致参数不再更新</strong>，训练过程提前结束</p>
<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p><code>SGD(Root Mean Square Prop)</code>在<code>SGD</code>基础上增加二阶动量，由于<code>Adagrad</code>的学习率衰减太过激进，改变二阶动量的计算策略：<strong>不累计全部梯度，只关注过去某一窗口内的梯度</strong>。<strong>指数移动平均值</strong>大约是过去一段时间的平均值，反映<strong>局部的</strong>参数信息，用这个方法来计算二阶累积动量。</p>
<p>$m_{t}=g_{t}$</p>
<p>$v_{t}=\beta v_{t-1}+(1-\beta)g_{t}^{2}$</p>
<p>$\theta_{t+1}=\theta_{t}-\eta\frac{g_{t}}{\sqrt{v_{t}}}=\theta_{t}-\eta\frac{g_{t}}{\sqrt{\beta v_{t-1}+(1-\beta)g_{t}^{2}}} $</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p><code>Adam(Adaptive Moment Estimation)</code>，自适应矩估计。是2014年提出的一种万金油式的优化器，使用起来非常方便，梯度下降速度快，但是容易在最优值附近震荡。竞赛中性能会略逊于<code>SGD</code>，毕竟最简单的才是最有效的。但是超强的易用性使得<code>Adam</code>被广泛使用。是<code>SGDM</code>和<code>RMSProp</code>的结合。</p>
<p>$m_{t}=\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}$</p>
<p>$v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}$</p>
<p>$\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}}$</p>
<p>$\hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}}$</p>
<p>$\theta_{t+1}=\theta_{t}-\eta\frac{\hat{m}_t}{\sqrt{\hat{v}_{t}}+\epsilon}$</p>
<p><strong>解释：</strong></p>
<p>第一项 $m_{t}$ 为t时刻，梯度在动量形式下的一阶矩估计。</p>
<p>第二项 $v_{t}$ 为梯度在动量形式下的二阶矩估计。</p>
<p>第三项 $\hat{m}_{t}$ 为偏差纠正后的一阶矩估计。</p>
<p>第四项 $\hat{v}_{t}$ 为偏差纠正后的二阶矩估计。</p>
<p>最后一项是更新公式，可以参考<code>RMSProp</code>以及之前的算法。</p>
<p><strong>为什么需要偏差纠正？</strong></p>
<p>拿梯度在动量形式下的二阶矩估计 $v_{t}$ 为例，各个 $v_{t}$ 的公式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071555276.png" alt=""></p>
<p>而我们实际上需要的是梯度的二阶矩估计，也就是 $E(g_{i}^{2})$ 。因此使用动量求出来的二阶矩估计是有偏的，需要纠正。我们对动量二阶矩估计 $v_{t}$ 求期望 $E(v_{t})$ ，可以通过等比数列公式得到 $E(v_{t})$ 与 $E(g_{i}^{2})$ 的关系：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071559636.png" alt=""></p>
<p>因此，要得到 $E(g_{i}^{2})$ ，就需要除掉前面的系数 $(1-\beta_{2}^{t})$ 是一个常数</p>
<h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><h2 id="Top-1-Accuracy和Top-5-Accuracy"><a href="#Top-1-Accuracy和Top-5-Accuracy" class="headerlink" title="Top-1 Accuracy和Top-5 Accuracy"></a>Top-1 Accuracy和Top-5 Accuracy</h2><p>Top-1：就是你预测的label取最后概率向量里面最大的那一个作为预测结果 ，如过预测结果中概率最大的那个分类正确，则预测正确，否则预测错误。<br>Top-5：就是最后概率向量最大的前五名中，只要出现了正确概率即为预测正确，否则预测错误。</p>
<h2 id="DICE"><a href="#DICE" class="headerlink" title="DICE"></a>DICE</h2><p>用来衡量预测结果<code>pred</code>和标签<code>label</code>的相似度（和评价指标<code>F1</code>是相同的）</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306191056073.png" alt=""></p>
<h2 id="IoU"><a href="#IoU" class="headerlink" title="IoU"></a>IoU</h2><p><code>IoU</code>全称<code>Intersection-over-Union</code>，即交并比，在目标检测领域中，定义为两个矩形框面积的交集和并集的比值，$IoU=\frac{A\cap B}{A\cup B}$。</p>
<ul>
<li>如果完全重叠，则<code>IoU</code>等于1，是最理想的情况。一般在检测任务中，<code>IoU</code>大于等于<code>0.5</code>就认为召回，如果设置更高的<code>IoU</code>阈值，则召回率下降，同时定位框也越更加精确。</li>
<li>在图像分割中也会经常使用<code>IoU</code>，此时就不必限定为两个矩形框的面积。比如对于二分类的前背景分割，那么$IoU=\frac{真实前景像素面积\cap预测前景像素面积}{真实前景像素面积\cup预测前景像素面积}$，这一个指标，通常比直接计算每一个像素的分类正确概率要低，也对错误分类更加敏感。</li>
</ul>
<h2 id="MIoU"><a href="#MIoU" class="headerlink" title="MIoU"></a>MIoU</h2><p>均交并比，语义分割的标准度量。计算两个集合的交集与并集之比，在语义分割中，这两个集合为真实值和预测值。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306191111890.png" alt=""></p>
<p>上述公式和下面的公式是等价的：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306191113911.png" alt=""></p>
<p>计算<code>MIoU</code>的三个步骤：</p>
<ol>
<li>计算混淆矩阵</li>
<li>计算每个类别的<code>IoU</code></li>
<li>对每个类别的<code>IoU</code>取平均</li>
</ol>
<h2 id="Box-AP"><a href="#Box-AP" class="headerlink" title="Box AP"></a>Box AP</h2><p>Box AP 即Box Average Precision，用于综合评价目标检测模型效能。要清楚的是AP的计算是先使用<strong>confidence threshold</strong>去除一些置信度过低的框；然后要使用NMS要用到 <strong>nms_boxiou threshold</strong>，去除和置信度最高的框IoU大于阈值的重叠框；接着会使用<strong>box_iou threshold</strong>，将每个框分为TP、FP，对于同一个GT，预测框根据置信度从高到低排列，只有IoU大于阈值且置信度最高的那个算是TP，其余IOU大于阈值的都算是FP；分配好TP、FP以后，所有框按照置信度从高到低排列，计算AP值。</p>
<ul>
<li>confidence threshold：置信度阈值，每个输出框都会给出置信度，只有置信度高于某个阈值的框才会被考虑。</li>
<li>nms_boxiou threshold：NMS方法的逻辑是先根据预测出的前景分数从高到低排序box，从分数最高的box开始遍历，与这个box的overlap大于某个threshold的box就会被去掉，例如我们以score=0.8的box为基准，overlap_threshold=0.2，那么与score=0.8的box的overlap大于0.2的框，都会被丢弃。如果框X与score=0.8的框的overlap小于0.2，那么我们会认为框X属于另一个物体，不用抑制。</li>
<li>box_iou threshold：例如我们以IoU为30%作为评估的阈值，如果检测结果和GT的IoU超过30%，则此次检测为TP，否则就是FP。</li>
<li>AP：AP的值就是PR曲线下方围成的面积的值。</li>
</ul>
<h2 id="Mask-AP"><a href="#Mask-AP" class="headerlink" title="Mask AP"></a>Mask AP</h2><p>Mask AP用于综合评价实例分割模型效能。Mask AP和Box AP的区别仅仅在于box_iou threshold作用的对象不相同，Box AP里面作用的是标准普通的GT和预测框的IoU值，Mask AP里面作用的是GT mask和预测mask的mask IoU，即像素点之间的mask IoU。</p>
<h1 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h1><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p><code>Bagging(Bootstrap aggregating)</code>，引导聚集算法，又称装袋算法，是机器学习领域的一种团体学习算法。<code>Bagging</code>算法可与其他分类、回归算法结合，提高其准确率、稳定性的同时，通过降低结果的方差，避免过拟合的发生。</p>
<p><strong>随机采样（bootstrap sample）</strong>从 $n$ 个数据点中<strong>有放回地重复随机</strong>抽取一个样本（即同一个样本可被多次抽取），共抽取 $n$ 次。创建一个与原数据大小相同得数据集，但有些数据点会缺失（大约 $1/3$ ），有些会重复。</p>
<p><code>Bagging</code>对于弱学习器没有限制，这和<code>Adaboost</code>一样。但是最常用的一般也是<strong>决策树</strong>和<strong>神经网络</strong>。</p>
<p><code>Bagging</code>的集合策略也比较简单，对于分类问题，通常使用简单投票法，得到最多票数的类别或者类别之一为最终的模型输出。对于回归问题，通常使用简单平均法，对 $T$ 个弱学习器得到的回归结果进行算术平均得到最终的模型输出。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071107979.png" alt=""></p>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p>随机森林以决策树为基本单元，通过集成大量的决策树，就构成了随机森林。其构造过程如下：</p>
<ol>
<li>$T$ 中共有 $N$ 个样本，有放回的随机选择 $N$ 个样本（因为有放回，所以虽然是 $N$ 但是不可能遍历所有样本）。这选择好了的 $N$ 个样本用来训练一个决策树，作为决策树根节点处的样本。</li>
<li>当每个样本有 $M$ 个属性时，在决策树的每个节点需要分裂时，随机从这 $M$ 个属性中选取出 $m$ 个属性，满足条件 $m &lt;&lt; M$ 。然后从这 $m$ 个属性中采用某种策略来选择某个属性作为该节点的分裂属性。</li>
<li>决策树形成过程中每个节点都要按照上述步骤来分裂，一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。</li>
<li>重复建立大量的决策树，这样就构成了随机森林了。</li>
</ol>
<h4 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h4><h5 id="决策树构建的终止条件"><a href="#决策树构建的终止条件" class="headerlink" title="决策树构建的终止条件"></a>决策树构建的终止条件</h5><ol>
<li><p>样本进来属于同一个类别，直接输出结果是 $C$ 类返回。</p>
</li>
<li><p>没法划分了，特征向量中所有属性都用完了；或者 $D$ 样本中的属性 $A$ 都相同，然后将此节点标记为叶子节点，将样本 $D$ 中数目最多的类当做类别返回。</p>
</li>
<li>当对数据进行划分成多个分支，如果存在分支中没有数据（分支为空），将划分前类别数目多的当做类别返回。</li>
</ol>
<h5 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h5><p>各种准则虽然对决策树的尺寸有较大影响,但<strong>对泛化性能的影响很有限</strong>，<strong>剪枝方法和程度对决策树泛化性能的影响更为显著；剪枝是决策树防止过拟合的手段</strong>。</p>
<p><strong>预剪枝：</strong>在决策树构造时就进行剪枝。在决策树构造过程中，对节点进行评估，如果对其划分并不能再验证集中提高准确性，那么该节点就不要继续往下划分。这时就会把当前节点作为叶节点。</p>
<p><strong>后剪枝：</strong>在生成决策树之后再剪枝。通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉该节点，带来的验证集中准确性差别不大或有明显提升，则可以对它进行剪枝，用叶子节点来代填该节点。</p>
<p><strong>预剪枝vs后剪枝</strong></p>
<p>时间开销：</p>
<ul>
<li>预剪枝：训练时间开销降低，测试时间开销降低</li>
<li>后剪枝：训练时间开销增加，测试时间开销降低</li>
</ul>
<p>过/欠拟合风险:</p>
<ul>
<li>预剪枝：过拟合风险降低，欠拟合风险增加</li>
<li>后剪枝：过拟合风险降低，欠拟合风险基本不变</li>
</ul>
<p>泛化性能：后剪枝通常优于预剪枝</p>
<h5 id="决策树生成算法"><a href="#决策树生成算法" class="headerlink" title="决策树生成算法"></a>决策树生成算法</h5><p><strong>ID3</strong></p>
<p>使用信息熵增益作为特征选择的标准。</p>
<p>数据集 $D$ 的经验熵定义为： $Ent(D)=-\sum_{k=1}^{K}\frac{|C_{k}|}{|D|}\log_{2}\frac{|C_{k}|}{|D|}$ ，其中 $|C_{k}|$ 为第 $k$ 类样本的数目， $|D|$ 为数据集 $D$ 中样本的数目。</p>
<p>计算特征 $A$ 对数据集 $D$ 的经验条件熵： $Ent(D|A)=\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}Ent(D_{i})=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_{i}|}\log_{2}\frac{|D_{ik}|}{|D_{i}|}$  。意思就是对划分后的新的 $n$ 个小数据集的经验熵加权，权重为每一个小数据集中的样本数目占未划分前的大数据集的比例。</p>
<p>计算信息熵增益： $gain(D,A)=Ent(D)-Ent(D|A)$ ，选择信息熵增益最大的特征。</p>
<p><strong>C4.5</strong></p>
<p>与<code>ID3</code>算法的最大不同在于使用信息熵增益率代替信息熵增益。信息熵增益率的定义如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304211122841.png" alt=""></p>
<p>其中 $IV(A)=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}\log_{2}\frac{|D_{i}|}{|D|}$ 称为数据集 $D$ 关于 $A$ 的取值熵。</p>
<p>这种方法对可能取值少的属性有所偏好，因此<code>C4.5</code>算法也不是直接使用增益率最大的来划分属性，而是使用了一种“启发式”的方法，先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。</p>
<p><strong>CART</strong></p>
<p><code>CART</code>决策树使用“基尼指数”来选择划分属性，选取那个使划分后基尼指数<strong>最小</strong>的属性。数据集 $D$ 的纯度可用基尼值来度量。 $Gini(D)$ 越小，则数据集的纯度越高。 $Gini(D)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p_{k}^{2}$ ，则属性 $A$ 的基尼指数表达式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304211122760.png" alt=""></p>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p><code>Bagging</code>在随机森林的构建过程中，各棵树之间是相互独立的，在构建第 $m$ 棵树的时候，不会考虑前面的 $m-1$ 棵树。<code>Boosting</code>在构建第 $m$ 棵子树的时候，会考虑到前 $m-1$ 棵子树的结果。</p>
<p><strong>提升学习(Boosting)</strong>是一种机器学习技术，通过从训练数据构建模型，然后创建第二个模型来尝试纠正第一个模型中的错误来完成的。添加模型直到完美预测训练集或添加最大数量的模型。提升学习的每一步产生弱预测模型（如决策树），并加权累加到总模型中；如果每一步的弱预测模型的生成都是依据损失函数的梯度方式，就称为梯度提升<code>(Gradient Boosting)</code>。</p>
<h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071138731.png" alt=""></p>
<p><code>Adaptive Boosting(AdaBoost)</code>是第一个为二进制分类开发的真正成功的提升算法。这是理解<code>Boosting</code>的最佳起点，现代提升方法建立在<code>AdaBoost</code>之上。</p>
<p><code>AdaBoost</code>流程：</p>
<ol>
<li><p>训练数据集中的每个实例都被加权。初始权重设置为： $Weight(X_{i})=\frac{1}{N_{}}$ </p>
</li>
<li><p>使用加权之后的样本作为训练数据，以弱分类器（决策树桩）进行训练。</p>
</li>
<li><p>为训练后的模型计算当前分类器的分类误差。传统的计算方式如下：</p>
<p>分类误差： $error_t = \sum_{i=1}^{N}(W_{t,i}\times terror_{i})$ 。其中 $terror_{i}=I(G_{t}(X_{i})\neq y_{i})$ 。</p>
<p>例如：如果我们有三个训练实例，权重分别为 $0.01$ 、 $0.5$ 和 $0.2$ 。预测值为 $-1$ 、 $-1$ 和 $-1$ ，实例中的真实输出变量为 $-1$ 、 $1$ 和 $-1$ ，则  $terror$ 为 $0$ 、 $1$ 和 $0$ 。误分类率将计算为：<strong>error = (0.01*0 + 0.5*1 + 0.2*0) or error = 0.5</strong>。</p>
</li>
<li><p>为经过训练的模型计算阶段权值，该值为模型做出的任何预测提供权重。训练模型的阶段值计算公式： $\alpha_{t}=\frac{1}{2}\times\ln\frac{1-error_{t}}{error_{t}}$</p>
</li>
<li><p>更新训练权重，为错误预测的实例提供更多的权重，为正确预测的实例提供更少权重。计算公式： $W_{t+1,i}=\frac{W_{t,i}}{Z_{t}}\exp(-\alpha_{t}G_{t}(X_{i})y_{i})$ ， $Z_{t}$ 是规范因子， $Z_{t}=\sum_{i=1}^{N}W_{t,i}\exp(-\alpha_{t}G_{t}(X_{i})y_{i})$</p>
</li>
<li><p>最终的分类器为： $G(X)=sign(\sum_{m=1}^{K}\alpha_{m}G_{m}(X))$</p>
</li>
</ol>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p><code>GBDT(Gradient Boosting Decision Tree)</code>在数据分析和预测中的效果很好。它是一种基于决策树的集成算法。其中<code>Gradient Boosting</code>是集成方法<code>Boosting</code>中的一种算法，通过梯度下降来对新的学习器进行迭代。将表现一般的数个模型（通常是深度固定的决策树）组合在一起来集成一个表现较好的模型。抽象地说，模型的训练过程是对一任意可导目标函数的优化过程。通过反复地选择一个指向负梯度方向的函数，该算法可被看做在函数空间里对目标函数进行优化。因此可以说<code>Gradient Boosting = Gradient Descent + Boosting</code>。</p>
<p>模型的结果是一组回归分类树组合<code>(CART Tree Ensemble)</code>： $T_{1},\cdots,T_{K}$ 。其中 $T_{j}$ 学习的是之前 $j-1$ 棵树预测结果的残差，这种思想就像准备考试前的复习，先做一遍习题册，然后把做错的题目挑出来，再做一次，然后把做错的题目挑出来再做一次，经过反复多轮训练，取得最好的成绩。而模型最后的输出，是一个样本在各个树中输出的结果的和： $\bar{y}=\sum_{k=1}^{K}f_{k}(x)$ 。</p>
<p>和<code>AdaBoost</code>一样，<code>Gradient Boosting</code>也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，<code>AdaBoost</code>是通过提升错分数据点的权重来定位模型的不足而<code>Gradient Boosting</code>是通过算梯度<code>(gradient)</code>来定位模型的不足。因此相比<code>AdaBoost</code>，<code>Gradient Boosting</code>可以使用更多种类的目标函数。</p>
<h4 id="提升树算法"><a href="#提升树算法" class="headerlink" title="提升树算法"></a>提升树算法</h4><p>提升树是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差=真实值-预测值。提升树即是整个迭代过程生成的回归树的累加。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101613543.jpg" alt=""></p>
<p>具体的算法步骤：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101613198.png" alt=""></p>
<h4 id="Gradient-Boosting-Decision-Tree（梯度提升决策树）"><a href="#Gradient-Boosting-Decision-Tree（梯度提升决策树）" class="headerlink" title="Gradient Boosting Decision Tree（梯度提升决策树）"></a>Gradient Boosting Decision Tree（梯度提升决策树）</h4><p>提升树利用加法模型和前向分步算法实现学习的优化过程。当损失函数时平方损失和指数损失函数时，每一步的优化很简单，如平方损失函数学习残差回归树。</p>
<p>但对于一般的损失函数，往往每一步优化没那么容易，如上图中的绝对值损失函数和<code>Huber</code>损失函数。针对这一问题，<code>Freidman</code>提出了梯度提升算法：利用最速下降的近似方法，即利用损失函数的负梯度在当前模型的值，作为回归问题中提升树算法的残差的近似值，拟合一个回归树。</p>
<h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p><strong>原始目标函数</strong></p>
<p>目标函数，可以分为两个部分，一部分是损失函数，一部分是正则（用于控制模型的复杂度）。</p>
<p>对于第 $t$ 颗树，第 $i$ 个样本的，模型的预测值是这样的：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101650959.png" alt=""></p>
<p>进一步，我们可以得到我们的原始目标函数，如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101651061.png" alt=""></p>
<p><strong>损失函数化简</strong></p>
<p><code>XGBoost</code>是前向迭代，我们的重点在于第 $t$ 个树，所以涉及到前 $t-1$ 个树变量或者说参数我们是可以<strong>看做常数</strong>的。所以我们的损失函数进一步可以化为如下，其中一个变化是我们对正则项进行了拆分，变成可前 $t-1$ 项和第 $t$ 项：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101656177.png" alt=""></p>
<h4 id="泰勒公式展开"><a href="#泰勒公式展开" class="headerlink" title="泰勒公式展开"></a>泰勒公式展开</h4><p>使用泰勒公式进行近似展开的核心目标是对目标函数进行化简，将常数项抽离出来。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101658315.png" alt=""></p>
<p>这里 $\Delta x$ 对应的是第 $t$ 棵树的模型 $f_{t}(x_{i})$ ， $x$ 对应的是 $\hat{y}_{i}^{(t-1)}$ ，相应的 $f(x)$ 对应到损失函数应该是 $l(y_{i},\hat{y}_{i}^{(t-1)})+f_{t}(x_{i})$ 。</p>
<p>所以原有公式进行泰勒公式二阶展开，结果为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101704500.png" alt=""></p>
<p>进而我们可以得到目标函数展开公式为如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101704670.png" alt=""></p>
<h4 id="树的参数化"><a href="#树的参数化" class="headerlink" title="树的参数化"></a>树的参数化</h4><p><strong>树模型参数化</strong></p>
<ul>
<li>每棵树每个叶子节点的值（或者说每个叶子节点的权重） $w$ ：这是一个向量，因为每个树有很多叶子节点</li>
<li>样本到叶子节节点的映射关系 $q$ ：告诉每个样本落在当前这个树的哪一个叶子节点上</li>
<li>叶子节点样本归属集合 $I$ ：告诉每个叶子节点包含哪些样本</li>
</ul>
<p><strong>树复杂度参数化</strong></p>
<p>树的复杂度定义如下，其中 $T$ 参数表示当前这棵树叶子节点的个数； $w_{j}^{2}$ 是叶子节点值的 $L_{2}$ 范数：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101716405.png" alt=""></p>
<p>进而我们可以对树进行了参数化，带入到目标函数我们可以得到如下式子：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101727004.png" alt=""></p>
<p>最后一步的转化思路是从在这个树中，每个样本落在哪个节点转为了每个节点上有哪些样本。</p>
<p>叶子节点 $j$ 所包含的样本的一阶导数累加之和为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101743580.png" alt=""></p>
<p>叶子节点 $j$ 所包含的样本的二阶导数累加之和为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101744292.png" alt=""></p>
<p>进而我们可以进一步化简为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101744829.png" alt=""></p>
<p>对目标函数对 $w_{j}$ 进行求导就能得出极值点：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101746573.png" alt=""></p>
<h4 id="特征分裂"><a href="#特征分裂" class="headerlink" title="特征分裂"></a>特征分裂</h4><p>对于上述的目标函数，我们仍存在问题，即 $T$ 的取值，也就是如何做特征分裂。</p>
<p><strong>贪心算法</strong></p>
<p>本质上是做两次循环，第一个循环是针对每个特征的每个分割点做一次循环，计算收益，从而选择此特征的最佳分割点。分裂收益使用的是分裂之后的目标函数的变化差值。第二个循环是对样本所有特征的循环，从中挑选出收益最大的特征。</p>
<p>简单说就是首先找到基于每个特征找到收益最大的分割点，然后基于所有特征找到收益最大的特征。</p>
<p><strong>近似算法-分位数候选点</strong></p>
<p>对于每个特征，不去暴力搜索每个值，而是使用分位点</p>
<ul>
<li>根据样本数量选择三分位点或者四分位点等</li>
<li>或者根据二阶导数（也就是梯度）作为权重进行划分</li>
</ul>
<p>也就是说原来是某个特征的所有取值作为候选点，现在是某个特征的分位点作为候选点。</p>
<h3 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h3><p><code>LightGBM(Light Gradient Boosting Machine)</code>是一种梯度提升框架，它使用决策树作为基学习器。<code>LightGBM</code>为高效并行计算而生，它的<code>Light</code>体现在以下几个点上：</p>
<ul>
<li>更快的训练速度</li>
<li>更低的内存使用</li>
<li>支持单机多线程，多机并行计算，以及<code>GPU</code>训练</li>
<li>能够处理大规模数据</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102112494.png" alt=""></p>
<p>概括来说，<code>lightGBM</code>主要有以下特点：</p>
<ul>
<li>基于<code>Histogram</code>的决策树算法</li>
<li>带深度限制的<code>Leaf-wise</code>的叶子生长策略</li>
<li>直方图做差加速</li>
<li>直接支持类别特征<code>(Categorical Feature)</code></li>
<li><code>Cache</code>命中率优化</li>
<li>基于直方图的稀疏特征优化</li>
<li>多线程优化</li>
</ul>
<h4 id="直方图Histogram算法"><a href="#直方图Histogram算法" class="headerlink" title="直方图Histogram算法"></a>直方图Histogram算法</h4><p><strong>寻找最佳分类点</strong></p>
<p>将连续型特征值放入离散化的箱子<code>(bin)</code>中，然后用这些箱子构建特征直方图。然后模型基于特征直方图寻找最佳分裂点，构建直方图的时间复杂度是 $O(data \times feature)$ ，但寻找最佳分裂点的时间复杂度为 $O(bin \times feature)$ 。模型训练速度会因此而提高，而且因为不需要存储排序索引，内存压力也变小了。<code>LGBM</code>采用的就是直方图算法（现在<code>XGBoost</code>开源代码也支持直方图算法）。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102149708.png" alt=""></p>
<p><strong>直方图差加速</strong></p>
<p>直方图做差是指：“一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的 $k$ 个桶。利用这个方法，<code>LightGBM</code>可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。” 示意图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102152516.png" alt=""></p>
<h4 id="带深度限制的Leaf-wise的叶子生长策略"><a href="#带深度限制的Leaf-wise的叶子生长策略" class="headerlink" title="带深度限制的Leaf-wise的叶子生长策略"></a>带深度限制的Leaf-wise的叶子生长策略</h4><p><code>GBDT</code>与<code>XGBoost</code>模型在叶子生长策略上均采用按层<code>level-wise</code>分裂的方式，这种方式在分裂时会针对同一层的每一个节点，即每次迭代都要遍历整个数据集中的全部数据，这种方式虽然可以使每一层的叶子节点并行完成，并控制模型的复杂度，但也会产生许多不必要搜索或分裂，从而消耗更多的运行内存，增加计算成本。</p>
<p>而<code>LightGBM</code>算法对其进行了改进，使用了按叶子节点<code>leaf-wise</code>分裂的生长方式，即每次是对所有叶子中<strong>分裂增益最大的叶子节点进行分裂</strong>，其他叶子节点则不会分裂。这种分裂方式比按层分裂会带来更小的误差，并且加快算法的学习速度，但由于没有对其他叶子进行分裂，会使得分裂结果不够细化，并且在每层中只对一个叶子不断进行分裂将增大树的深度，造成模型过拟合。因此，<code>LightGBM</code>算法在按叶子节点生长过程中会限制树的深度来避免过拟合。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102156269.png" alt=""></p>
<h4 id="支持类别特征"><a href="#支持类别特征" class="headerlink" title="支持类别特征"></a>支持类别特征</h4><p>实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化<code>one-hotting</code>特征，降低了空间和时间的效率。而类别特征的使用是在实践中很常用的。基于这个考虑，<code>LightGBM</code>优化了对类别特征的支持，可以直接输入类别特征，不需要额外的 $0/1$ 展开。并在决策树算法上增加了类别特征的决策规则。决策树在学习节点分裂时，是一种<code>one-vs-rest</code>模式，每次只能根据一个类别做分类，如下图。这种模式效率比较低，而且不利于决策树学习。<code>LightGBM</code>对此进行了优化，采用<code>many-vs-many</code>模式分裂节点，如下图。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102208969.png" alt=""></p>
<h4 id="基于梯度的单边采样-GOSS"><a href="#基于梯度的单边采样-GOSS" class="headerlink" title="基于梯度的单边采样(GOSS)"></a>基于梯度的单边采样(GOSS)</h4><p>在<code>AdaBoost</code>中，样本权重指示了数据样本的重要性，而在<code>GBDT</code>上并没有样本权重这一说，可作者发现：在<code>GBDT</code>中，梯度对于每个样本是个很有用的信息，它可以用来帮助采样。为什么这么说呢？让我们打个比方，如果某样本得到的一个小的梯度值，那么说明该样本的训练误差也小，模型在该样本上表现得就很好，那这些小梯度的样本其实是不是不用参与训练了？就好像准备考试时刷题不刷简单题，这样可以吗？不可以！因为如果真的直接剔除它们，数据分布会改变，从而损害模型的准确率。为了处理这个问题，作者提出了<code>GOSS</code>，<code>GOSS</code>全称是<code>Gradient-based One-Side Sampling</code>单边梯度采样，它保留所有大梯度的样本，然后小梯度样本采用随机采样，在不改变原始数据分布的同时，减小了样本数量，提升了模型的训练速度。</p>
<h4 id="互斥特征绑定"><a href="#互斥特征绑定" class="headerlink" title="互斥特征绑定"></a>互斥特征绑定</h4><p>从特征角度来看，稀疏特征会包含很多 $0$ 元素；从样本角度来看，一个样本的多个稀疏特征经常同时为 $0$ 。<code>EFB(Exclusive Feature Bundling)</code>基于这种想法，对<strong>互斥特征</strong>进行了<strong>捆绑</strong>，整体过程有点类似于<code>One-Hot</code>逆过程。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102216271.png" alt=""></p>
<p><strong>捆哪些特征</strong></p>
<p>尝试每种组合，是<code>NP</code>难问题，现有算力做不到。所以只能采用贪心算法找。具体过程如下：</p>
<ol>
<li>遍历特征，先把第一个特征拿出来作为一个组合</li>
<li>第二个特征往这个组合里放，冲突比例小就放进去合并成一个特征，冲突比例大就单拿出来作为另一个组合</li>
<li>第三个特征继续往已有的组合里放，能放就放，不能放就单成一个新组合</li>
<li>以此类推对所有特征做同样的操作。</li>
</ol>
<p><strong>如何捆绑特征</strong></p>
<p>因为不同特征下的值有不同的量纲，比如：特征<code>A</code>的值范围为 $[0, 10)$ ，特征<code>B</code>的值范围为 $[0, 20)$ ，将特征<code>A</code>和特征<code>B</code>的直方图加起来捆绑一起后，<code>bundle</code>的值范围变为 $[0, 20)$ ，但是我们无法从中辨别哪些是特征<code>A</code>，哪些是特征<code>B</code>。这样对模型是不利的，因为模型这样就没法根据<code>bundle</code>直方图的值范围去很好区分特征，对树生成会带来误差。对于该问题的解决办法就是加偏移量，如果我们对特征<code>B</code>加偏移量 $10$ ，特征<code>B</code>的值范围变为 $[10, 30)$ ，合并后的<code>bundle</code>值范围变为 $[0, 30)$ 。</p>
<h3 id="CatBoost"><a href="#CatBoost" class="headerlink" title="CatBoost"></a>CatBoost</h3><p><code>CatBoost</code>主要是在类别特征上的处理上做了很多的改进。从用户使用角度来看，相比<code>XGBoost</code>和<code>LightGBM</code>，<code>CatBoost</code>具有如下特点。</p>
<ul>
<li><strong>模型精度：</strong><code>XGBoost</code>和<code>LightGBM</code>相当，<code>CatBoost</code>往往略好一些，无需调参即可获取很好的结果。</li>
<li><strong>训练速度：</strong><code>LightGBM</code>远快于<code>XGBoost</code>，<code>CatBoost</code>快于<code>XGBoost</code>但比<code>LightGBM</code>慢。</li>
<li><strong>预测速度：</strong><code>LightGBM</code>与<code>XGBoost</code>相当，<code>CatBoost</code>远快于<code>LightGBM</code>与<code>XGBoost</code>，是它们的几十分之一。</li>
<li><strong>内存消耗：</strong><code>LightGBM</code>远小于<code>XGBoost</code>，<code>CatBoost</code>小于<code>XGBoost</code>，但大于<code>LightGBM</code>。</li>
<li><strong>类别特征：</strong><code>XGBoost</code>不支持类别特征，需要<code>One-Hot</code>编码预处理。<code>LightGBM</code>支持类别特征，需转换成整数编码。<code>CatBoost</code>提供更强大的对类别特征的支持，直接支持字符串类型的类别特征，无需预处理。</li>
<li><strong>缺失值特征：</strong><code>XGBoost</code>和<code>LightGBM</code>都可以自动处理特征缺失值，<code>CatBoost</code>不能自动处理缺失值（或者将缺失值视为最小值/最大值）。</li>
<li><strong>GPU支持：</strong><code>LightGBM</code>与<code>CatBoost</code>支持<code>GPU</code>训练，<code>XGBoost</code>也支持<code>GPU</code>训练。</li>
<li><strong>可视化：</strong><code>CatBoost</code>还自带一套可视化工具，可以在<code>Jupyter Notebook</code>或者<code>TensorBoard</code>中实时看到指标变化。</li>
</ul>
<h4 id="基于类别特征的Ordered-Target-Statistics数值编码方法"><a href="#基于类别特征的Ordered-Target-Statistics数值编码方法" class="headerlink" title="基于类别特征的Ordered Target Statistics数值编码方法"></a>基于类别特征的Ordered Target Statistics数值编码方法</h4><p>对于类别特征，如果类别数目不多，可以使用<code>One-Hot</code>编码。但如果类别数量成百上千，使用<code>One-Hot</code>编码会导致特征数量爆炸。<strong>CatBoost设计了一种基于预测目标统计值的方法可以将类别特征转化为数值特征。</strong>先将样本随机打乱，然后每个样本只使用它排序在它前面的样本来计算其类别特征的数值编码。这样就防止了<code>label</code>的泄露，并且能够较为合理地评估这个特征的真实有效性。具体公式表达为： $i \rightarrow \frac{Current\ Count+a\star P}{Max\ Count + a}$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111538347.png" alt=""></p>
<p>对上述例子来说，我们要计算第 $i$ 条数据的<code>label</code>，计算结果就为 $\frac{2+a \star P}{3+a}$ ，如果将第三行的<code>label</code>改为 $1$ ，那么结果就变成了 $\frac{3+a\star P}{3+a}$ 。</p>
<h4 id="基于贪心策略的特征交叉方法"><a href="#基于贪心策略的特征交叉方法" class="headerlink" title="基于贪心策略的特征交叉方法"></a>基于贪心策略的特征交叉方法</h4><p>使用<code>Ordered Target Statistics</code>方法将类别特征转化成为数值特征以后，会影响到特征交叉，因为数值特征无法有效地进行交叉。为了有效地利用特征交叉，<code>CatBoost</code>在将类别特征转换为数值编码的同时，会自动生成交叉特征。但如果让全部的类别特征之间都进行交叉，两两交叉，三三交叉，四四交叉，这个复杂度是指数级的，特征维度一定会爆炸。<code>CatBoost</code>使用一种贪心的策略来进行特征交叉。生成<code>tree</code>的第一次分裂，<code>CatBoost</code>不使用任何交叉特征。在后面的分裂中，<code>CatBoost</code>会使用生成<code>tree</code>所用到的全部原始特征和交叉特征跟数据集中的全部类别特征进行交叉。</p>
<h4 id="避免预测偏移的Ordered-Boosting方法"><a href="#避免预测偏移的Ordered-Boosting方法" class="headerlink" title="避免预测偏移的Ordered Boosting方法"></a>避免预测偏移的Ordered Boosting方法</h4><p>使用<code>XGBoost</code>或者<code>LightGBM</code>做模型时，我们可能经常会发现模型在训练集上拟合的很好，<code>train_auc</code>甚至达到了 $1.0$ ，但是在验证集上却差了很多，<code>val_auc</code>可能只有 $0.7$ 。这当然有可能是因为<code>tree</code>的数量太多了，或者是每棵<code>tree</code>的<code>leaves</code>太多了，总之模型太复杂了造成了过拟合。</p>
<p>但也有一些<code>XGBoost</code>和<code>LightGBM</code>自身算法的缺陷因素。我们知道<code>LightGBM</code>在训练下一棵<code>tree</code>的时候，需要计算前面这些<code>tree</code>构成的加法模型在所有样本上的一阶梯度和二阶梯度（<code>Loss</code>对模型预测结果的导数），然后用这些梯度来决定下一棵树的结构和叶子节点取值。</p>
<p>但是我们计算的这些一阶梯度和二阶梯度值是有问题的。前面的这些<code>tree</code>都是在这些样本上训练的，现在我们又在这些样本上估计模型预测结果的一阶和二阶梯度。我们应该换一些新的样本才更合理。但是我们从哪里找这些新的样本呢？</p>
<p><code>CatBoost</code>的作者故伎重演。先将样本随机打乱，然后每个样本只使用<strong>排序在它前面的样本</strong>来训练模型。用这样的模型来估计这个样本预测结果的一阶和二阶梯度。然后用这些梯度构建一棵<code>tree</code>的结构，最终<code>tree</code>的每个叶子节点的取值，是使用全体样本进行计算的。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111601603.png" alt=""></p>
<h4 id="使用对称二叉树作为基模型"><a href="#使用对称二叉树作为基模型" class="headerlink" title="使用对称二叉树作为基模型"></a>使用对称二叉树作为基模型</h4><p><code>XGBoost</code>和<code>LightGBM</code>采用的基模型是普通的二叉树，但是<code>CatBoost</code>采用的是对称的二叉树。这种对树结构上的约束有一定的<strong>正则作用</strong>。更为重要的是，它可以让<code>CatBoost</code>模型的推断过程极快。对于<code>CatBoost</code>的<code>tree</code>的预测过程来说，每个特征的分裂都是独立的，不分先后顺序，多个样本可以一起预测。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111619137.jpg" alt=""></p>
<h2 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h2><p>待总结</p>
<h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><p>支持向量机<code>(support vector machines，SVM)</code>是一种二分类模型，它将实例的特征向量映射为空间中的一些点，<code>SVM</code>的目的就是想要画出一条线，以 “最好地” 区分这两类点，以至如果以后有了新的点，这条线也能做出很好的分类。<code>SVM</code>适合中小型数据样本、非线性、高维的分类问题。</p>
<p>将实例的特征向量（以二维为例）映射为空间中的一些点，如下图的实心点和空心点，它们属于不同的两类。<code>SVM</code>的目的就是想要画出一条线，以“最好地”区分这两类点，以至如果以后有了新的点，这条线也能做出很好的分类。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071255957.png" alt=""></p>
<p><strong>Q1：能够画出多少条线对样本点进行区分？</strong><br>答：线是有无数条可以画的，区别就在于效果好不好，每条线都可以叫做一个划分超平面。比如上面的绿线就不好，蓝线还凑合，红线看起来就比较好。我们所希望找到的这条效果最好的线就是具有 “最大间隔的划分超平面”。</p>
<p><strong>Q2：为什么要叫作“超平面”呢？</strong><br>答：因为样本的特征很可能是高维的，此时样本空间的划分就不是一条线了。</p>
<p><strong>Q3：画线的标准是什么？/ 什么才叫这条线的效果好？/ 哪里好？</strong><br>答：<code>SVM</code>将会寻找可以区分两个类别并且能使间隔<code>(margin)</code>最大的划分超平面。比较好的划分超平面，样本局部扰动时对它的影响最小、产生的分类结果最鲁棒、对未见示例的泛化能力最强。</p>
<p><strong>Q4：间隔margin是什么？</strong><br>答：对于任意一个超平面，其两侧数据点都距离它有一个最小距离（垂直距离），这两个最小距离的和就是间隔。比如下图中两条虚线构成的带状区域就是<code>margin</code>，虚线是由距离中央实线最近的两个点所确定出来的（也就是由支持向量决定）。但此<code>margin</code>比较小，如果用第二种方式画，<code>margin</code>明显变大也更接近我们的目标。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071300267.png" alt=""></p>
<p><strong>Q5：为什么要让margin尽量大？</strong><br>答：因为大<code>margin</code>犯错的几率比较小，也就是更鲁棒啦。</p>
<p><strong>Q6：支持向量是什么？</strong><br>答：从上图可以看出，虚线上的点到划分超平面的距离都是一样的，实际上只有这几个点共同确定了超平面的位置，因此被称作 “支持向量<code>(support vectors)</code>”，“支持向量机” 也是由此来的。</p>
<p><strong>Q7：SVM 算法特性</strong></p>
<ol>
<li>训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以<code>SVM</code>不太容易产生<code>overfitting</code>。</li>
<li><code>SVM</code>训练出来的模型完全依赖于支持向量，即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果仍然会得到完全一样的模型。</li>
<li>一个<code>SVM</code>如果训练得出的支持向量个数比较少，那么<code>SVM</code>训练出的模型比较容易被泛化。</li>
<li><code>SVM</code>算法对大规模训练样本难以实施。</li>
<li>用<code>SVM</code>解决多分类问题存在困难。</li>
</ol>
<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><h2 id="多尺度"><a href="#多尺度" class="headerlink" title="多尺度"></a>多尺度</h2><p>多尺度，实际上就是对信号的不同粒度的采样。粒度小，说明是一个很密集的采样，能看到更多更多的细节；而粒度大，说明是一个很稀疏的采样，但是点与点之间隔得远了，就容易看到趋势了。通常在不同尺度下我们可以观察到不同的特征，从而完成不同的任务。</p>
<p>例如，我们判断一张图片中是否有前景，那么<code>12×8</code>的图像尺度就够了；如果我们要识别图中的水果种类，那么<code>64×48</code>的图像尺度勉强够用；如果我们要后期合成该图像的景深，则需要更高分辨率的图像，例如<code>640×480</code>。</p>
<h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>卷积的作用是<strong>提取特征</strong>，图像的空间联系是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。同时卷积通过权重共享降低参数量。</p>
<p>常见的卷积核选择都是<code>3x3</code>、<code>5x5</code>、<code>7x7</code>的，为什么很少见到偶数的卷积核呢？</p>
<ul>
<li>其主要原因是为了保护位置信息，使用奇数的卷积核，保证了中心点刚好在中间，避免了位置信息发生偏移。在需要使用位置信息的任务，如目标检测、目标识别、三维重建、图像重建等任务中非常有价值。</li>
<li>另一个就是因为<code>padding</code>时候能够保证左右对称，实际上也是为了位置信息。</li>
</ul>
<h3 id="普通卷积"><a href="#普通卷积" class="headerlink" title="普通卷积"></a>普通卷积</h3><p>在卷积神经网络中我们通常需要输入<code>in_channels</code>和<code>out_channels</code>，即输入通道数和输出通道数。</p>
<p>对于最初输入图片样本的通道数<code>in_channels</code>取决于图片的类型，如果是彩色的，即<code>RGB</code>类型，这时候通道数固定为 $3$ ，如果是灰度图，通道数为 $1$ 。<br>卷积完成之后，输出的通道数<code>out_channels</code>取决于过滤器的数量。从这个方向理解，这里的<code>out_channels</code>设置的就是过滤器的数目。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20230106231925.png" alt=""></p>
<p>如上图，输入的通道数为 $3$ ，所以卷积的时候每个需要对每个通道有个卷积核；输出的通道数为 $4$ ，输出的通道数就是我们设置的过滤器的数目。</p>
<h3 id="分组卷积-group-convolution"><a href="#分组卷积-group-convolution" class="headerlink" title="分组卷积(group convolution)"></a>分组卷积(group convolution)</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303212301932.png" alt=""></p>
<p><strong>我们用同等的参数量运算量生成了g个feature map！！！</strong></p>
<p>所以<code>group convolution</code>常用在轻量型高效网络中，因为它用少量的参数量和运算量就能生成大量的<code>feature map</code>，大量的<code>feature map</code>意味着能够编码更多的信息！</p>
<p>从分组卷积的角度来看，分组数 $g$ 就像一个控制旋钮，最小值是 $1$ ，此时 $g=1$ 的卷积就是普通卷积；最大值是输入<code>feature map</code>的通道数 $C$ ，此时 $g=C$ 的卷积就是<strong>depthwise sepereable convolution</strong>，即深度分离卷积，又叫逐通道卷积。</p>
<h3 id="卷积结果"><a href="#卷积结果" class="headerlink" title="卷积结果"></a>卷积结果</h3><p>假设 $W_{1}$ 、 $H_{1}$ 表示输入的宽度和长度， $W_{2}$ 、 $H_{2}$ 表示输出特征图的的宽度和长度， $F$ 表示卷积核长和宽的大小， $S$ 表示滑动窗口的步长， $P$ 表示边界填充。那么输出特征图的宽度和长度分别为： $W_{2}=\frac{W_{1}-F_{W}+2P}{S}+1$ ， $H_{2}=\frac{H_{1}-F_{H}+2P}{S}+1$ 。</p>
<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化的作用是进行<strong>特征压缩（下采样）</strong>，池化层是当前卷积神经网络中常用组件之一，它最早见于<code>LeNet</code>一文，称之为<code>Subsample</code>。自<code>AlexNet</code>之后采用<code>Pooling</code>命名。池化层是模仿人的视觉系统对数据进行降维，用更高层次的特征表示图像。</p>
<p>实施池化的目的：(1) 降低信息冗余；(2) 提升模型的尺度不变性、旋转不变性；(3) 降低特征维数，防止过拟合。</p>
<h2 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h2><p>padding的主要作用是使得图像边界的特征也能够被充分利用。</p>
<h2 id="CNN中的等变和不变"><a href="#CNN中的等变和不变" class="headerlink" title="CNN中的等变和不变"></a>CNN中的等变和不变</h2><p>简单的来说，CNN中的卷积操作中的参数共享使得它对平移操作有等变性，而一些池化操作对平移有近似不变性。先来说前者， 我们举个很简单的例子，我们都知道CNN的第一层往往可以解释为一些简单的线条处理，比如竖直/水平线条检测等等，那么如果图像平移，显然并不会影响到这一层线条检测的功能，但是其输出也会做相应平移。后者的之所以说是近似不变性，是因为池化层并非能保持完全不变，例如我们使用max池化，只要变换不影响到最大值，我们的池化结果不会收到影响，对于一个NxN的filter，只有一个值的变动会影响到输出， 其他的变换都不会造成扰动。 平均池化的近似不变性就稍弱些。这里池化的其实是一个非常强的先验，等于是忽视了这一步维数约简带来的信息损失而保证了近似不变性。</p>
<p>我们换个角度来说，CNN是既具有不变性，又具有等变性。 可以这么理解，如果我们的输出是给出图片中猫的位置，那么我们将图片中的猫从左边移到右边，这种平移也会反应在输出上，我们输出的位置也是从左边到右边，那么我们则可以说CNN有等变性；如果我们只是输出图片中是否有猫，那么我们无论把猫怎么移动，我们的输出都保持”有猫”的判定，因此体现了CNN的不变性。</p>
<h3 id="等变性"><a href="#等变性" class="headerlink" title="等变性"></a>等变性</h3><p>等变性 equivariant，对于一个函数，如果你对其输入施加的变换也会同样反应在输出上，那么这个函数就对该变换具有等变性。</p>
<h3 id="不变性"><a href="#不变性" class="headerlink" title="不变性"></a>不变性</h3><p>不变性 invraiant，对于一个函数，如果对其输入施加的某种操作丝毫不会影响到输出，那么这个函数就对该变换具有不变性。</p>
<h2 id="神经网络中权值共享的理解？"><a href="#神经网络中权值共享的理解？" class="headerlink" title="神经网络中权值共享的理解？"></a>神经网络中权值共享的理解？</h2><p>所谓权值共享就是说给定一张输入图片，用一个卷积核来卷积这张图，卷积核里的值叫做权重，这张图的每个位置是被同一个卷积核扫的，即卷积的时候所用的权重是一样的。其实权值共享这个词说全了就是整张图片在使用同一个卷积核内的参数，比如一个 $3\times 3 \times 1$ 的卷积核，这个卷积核内 $9$ 个的参数被整张图共享，而不会因为图像内位置的不同而改变卷积核内的权系数。说的再直白一些，就是用一个卷积核不改变其内权系数的情况下卷积处理整张图片（当然<code>CNN</code>中每一层不会只有一个卷积核的，这样说只是为了方便解释而已）。<br>作用：大大减少网络训练参数的同时，还可以实现并行训练。</p>
<h2 id="对微调-fine-tuning-的理解，为什么要修改最后几层神经网络权值？"><a href="#对微调-fine-tuning-的理解，为什么要修改最后几层神经网络权值？" class="headerlink" title="对微调(fine-tuning)的理解，为什么要修改最后几层神经网络权值？"></a>对微调(fine-tuning)的理解，为什么要修改最后几层神经网络权值？</h2><p>使用预训练模型的好处，在于利用训练好的<code>SOTA</code>模型权重去做特征提取，可以节省我们训练模型和调参的时间。</p>
<p>为什么只微调最后几层神经网络权重，是因为：</p>
<ol>
<li><code>CNN</code>中更靠近底部的层（定义模型时先添加到模型中的层）编码的是更加通用的可复用特征，而更靠近顶部的层（最后添加到模型中的层）编码的是更专业化的特征。微调这些更专业化的特征更加有用，它更代表了新数据集上的有用特征。</li>
<li>训练的参数越多，过拟合的风险越大。很多<code>SOTA</code>模型拥有超过千万的参数，在一个不大的数据集上训练这么多参数是有过拟合风险的，除非你的数据集像<code>ImageNet</code>那样大。</li>
</ol>
<h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><p>优点：这种网络当时提出来成为了CNN标准的“模板”——叠加卷积层和池化层，并以一个全连接层结束网络。</p>
<p>缺点：当时一段时间并未火起来，原因在于当时历史背景，这个简单的网络仅有 $6000$ 多个参数，但训练起来费时且没有<code>GPU</code>加速，相比较于传统的<code>SVM</code>等算法，效率还是差了许多，所以并没有大放异彩。</p>
<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><ul>
<li>使用<code>ReLU</code>激活函数，不容易发生梯度消失问题。</li>
<li>对输入的数据进行了数据增强处理。（水平变换、光照增强、随机裁剪、平移变换等等）</li>
<li>首次使用<code>Dropout</code>防止过拟合。</li>
<li>采用两块<code>GPU</code>并行计算，每一层分两块进行计算，所以看着比较繁琐，这里也是由于<code>GPU</code>不是很好，所以要两块并行。</li>
</ul>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p><code>2012</code>年，<code>Hinton</code>在其论文中提出<code>Dropout</code>。当一个复杂的前馈神经网络被训练在小的数据集时，容易造成过拟合。为了防止过拟合，可以通过阻止特征检测器的共同作用来提高神经网络的性能。<br><code>Droupout</code>是一种针对深度学习广泛应用的<strong>正则化技术</strong>。在每次迭代时随机关闭一些神经单元，随着迭代的进行，由于其他神经元可能在任何时候都被关闭，因此神经元对其他特定神经元的激活变得不那么敏感。</p>
<p>经过上面屏蔽掉某些神经元，使其激活值为 $0$ 以后，我们还需要对向量 $y_{1},\cdots,y_{n}$ <strong>进行缩放</strong>，也就是乘以 $1/(1-p)$ ，此方法为<code>invert Dropout</code>；如果你在训练的时候，经过置 $0$ 后，没有对 $y_{1}\cdots y_{n}$ 进行<code>rescale</code>，那么在测试的时候，就需要对权重进行缩放，即对每个神经元的权重都乘以一个 $p$ ，这样在“总体上”使得测试数据和训练数据是大致一样的，此方法为<code>vanilla Dropout</code>。比如一个神经元的输出是 $x$ ，那么在训练的时候它有 $p$ 的概率参与训练， $(1-p)$ 的概率丢弃，那么它输出的期望是 $p\cdot x+ (1-p)\cdot 0=p\cdot x$。因此测试的时候把这个神经元 $d$ 的权重乘以 $p$ 可以得到同样的期望。</p>
<h4 id="为什么说Dropout可以解决过拟合？"><a href="#为什么说Dropout可以解决过拟合？" class="headerlink" title="为什么说Dropout可以解决过拟合？"></a>为什么说Dropout可以解决过拟合？</h4><ol>
<li>取平均的作用。<code>Dropout</code>掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个<code>Dropout</code>过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</li>
<li>减少神经元之间复杂的共适应关系：因为<code>Dropout</code>导致两个神经元不一定每次都在一个<code>Dropout</code>网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。</li>
<li><code>Dropout</code>类似于性别在生物进化中的角色：物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。</li>
</ol>
<h4 id="Dropout缺点"><a href="#Dropout缺点" class="headerlink" title="Dropout缺点"></a>Dropout缺点</h4><p>明确定义的损失函数每一次迭代都会下降，而<code>Dropout</code>每一次都会随机删除节点，也就是说每一次训练的网络都是不同的，损失函数不再被明确地定义，在某种程度上很难计算，我们失去了调试工具。</p>
<h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><p>全部使用<code>3×3</code>卷积核的堆叠，来模拟更大的感受野，并且网络层数更深。<code>VGG</code>有五段卷积，每段卷积后接一层最大池化。卷积核数目逐渐增加。</p>
<p><strong>作者用的是多个3×3卷积叠加，而不是例如7×7、11×11的单个卷积，原因如下：</strong></p>
<ul>
<li><code>2</code>个<code>3×3</code>卷积叠加得到的理论感受野和一个<code>5×5</code>卷积的理论感受野是相同的，<code>3</code>个<code>3×3</code>卷积叠加得到的理论感受野和一个<code>7×7</code>卷积的理论感受野是相同的。</li>
<li>因为每个卷积层后面都会跟着一个<code>ReLU</code>，<code>3</code>个<code>3×3</code>卷积就会有<code>3</code>个<code>ReLU</code>，但是一个<code>7×7</code>的卷积只有一个，所以这么做可以使得模型的非线性拟合能力更强。</li>
<li>减少了参数数量。假设<code>3</code>个<code>3×3</code>卷积的输入和输出都是<code>C</code>个通道，那么参数数量为 $3 \times 3 \times 3 \times C \times C = 27C^{2}$  ，而<code>7×7</code>卷积的参数数量为 $7 \times 7 \times C \times C = 49C^{2}$ 。</li>
</ul>
<p><strong>1×1卷积的作用是什么？</strong></p>
<ul>
<li>为了在不影响卷积层感受野的前提下，增加模型的非线性。</li>
<li>可以压缩通道数，即减少特征的维度。</li>
</ul>
<h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p><code>GoogLeNet</code>专注于加深网络结构，与此同时引入了新的基本结构——<code>Inception</code>模块，从而来增加网络的宽度。每个原始<code>Inception</code>模块由<code>previous layer</code>、并行处理层及<code>filter concatenation</code>层组成。并行处理层包含 $4$ 个分支，即<code>1×1</code>卷积分支，<code>3×3</code>卷积分支，<code>5×5</code>卷积分支和<code>3×3</code>最大池化分支。一个关于原始<code>Inception</code>模块的最大问题是，<code>5×5</code>卷积分支即使采用中等规模的卷积核个数，在计算代价上也可能是无法承受的。这个问题在混合池化层之后会更为突出，很快的出现计算量的暴涨。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222128650.png" alt=""></p>
<p>为了克服原始<code>Inception</code>模块上的困难，<code>GoogLeNet</code>推出了一个新款，即采用<code>1×1</code>的卷积层来降低输入层的维度，使网络参数减少，因此减少网络的复杂性。因此得到降维<code>Inception</code>模块，称为<code>inception V1</code>。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222129537.png" alt=""></p>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><h3 id="ResNet中的一些亮点"><a href="#ResNet中的一些亮点" class="headerlink" title="ResNet中的一些亮点"></a>ResNet中的一些亮点</h3><ol>
<li>超深的网络结构（超过 $1000$ 层）。</li>
<li>提出<code>residual</code>（残差结构）模块。</li>
<li>使用<code>Batch Normalization</code>加速训练（丢弃<code>Dropout</code>）。</li>
</ol>
<h3 id="为什么采用residual"><a href="#为什么采用residual" class="headerlink" title="为什么采用residual?"></a>为什么采用residual?</h3><p>人们认为卷积层和池化层的层数越多，获取到的图片特征信息越全，学习效果也就越好。但是在实际的试验中发现，随着卷积层和池化层的叠加，不但没有出现学习效果越来越好的情况，反而两种问题：</p>
<ol>
<li>梯度消失和梯度爆炸<br>梯度消失：若每一层的误差梯度小于 $1$ ，反向传播时，网络越深，梯度越趋近于 $0$<br>梯度爆炸：若每一层的误差梯度大于 $1$ ，反向传播时，网络越深，梯度越来越大</li>
<li>退化问题<br>随着层数的增加，预测效果反而越来越差。</li>
</ol>
<ul>
<li>为了解决梯度消失或梯度爆炸问题，<code>ResNet</code>论文提出通过数据的预处理以及在网络中使用 <code>BN(Batch Normalization)</code>层来解决。</li>
<li>为了解决深层网络中的退化问题，可以人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系。这种神经网络被称为残差网络<code>(ResNets)</code>。<code>ResNet</code>论文提出了<code>residual</code>结构（残差结构）来减轻退化问题，随着网络的不断加深，效果并没有变差，而是变的更好了。</li>
</ul>
<h3 id="residual结构"><a href="#residual结构" class="headerlink" title="residual结构"></a>residual结构</h3><h4 id="residual的计算方式"><a href="#residual的计算方式" class="headerlink" title="residual的计算方式"></a>residual的计算方式</h4><p><code>residual</code>结构使用了一种<code>shortcut</code>的连接方式，也可理解为捷径。让特征矩阵隔层相加，注意 $\mathcal{F}(\mathbb{x})$ 和 $\mathbb{x}$ 形状要相同，所谓相加是特征矩阵相同位置上的数字进行相加。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317819.png" alt=""></p>
<h4 id="ResNet中两种不同的residual"><a href="#ResNet中两种不同的residual" class="headerlink" title="ResNet中两种不同的residual"></a>ResNet中两种不同的residual</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317983.png" alt=""></p>
<ol>
<li>左侧残差结构称为<code>BasicBlock</code></li>
<li>右侧残差结构称为<code>Bottleneck</code><ul>
<li>其中第一层的 $1\times1$ 的卷积核的作用是对特征矩阵进行降维操作，将特征矩阵的深度由 $256$ 降为 $64$ ；</li>
<li>第三层的 $1\times1$ 的卷积核是对特征矩阵进行升维操作，将特征矩阵的深度由 $64$ 升成 $256$ 。<br>降低特征矩阵的深度主要是为了减少参数的个数。<br>如果采用<code>BasicBlock</code>，参数的个数应该是： $256\times256\times3\times3\times2=1179648$<br>采用<code>Bottleneck</code>，参数的个数是： $1\times1\times256\times64+3\times3\times64\times64+1\times1\times256\times64=69632$</li>
<li>先降后升为了主分支上输出的特征矩阵和捷径分支上输出的特征矩阵形状相同，以便进行加法操作。</li>
</ul>
</li>
</ol>
<h3 id="BatchNormalization"><a href="#BatchNormalization" class="headerlink" title="BatchNormalization"></a>BatchNormalization</h3><p><strong>BatchNormalization就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布。</strong></p>
<p>在神经网络中, 数据分布对训练会产生影响。比如某个神经元 $x$ 的值为 $1$ ，某个 <code>Weights</code> 的初始值为 $0.1$ ，这样后一层神经元计算结果就是 $Wx = 0.1$ ；又或者 $x = 20$ ，这样 $Wx$ 的结果就为 $2$ 。现在还不能看出什么问题,，但是，当我们加上一层激活函数，激活这个  $Wx$ 值的时候，问题就来了。如果使用像 <code>tanh</code> 的激活函数， $Wx$ 的激活值就变成了 $\approx 0.1$ 和 $\approx 1$, 接近于 $1$ 的部已经处在了激活函数的饱和阶段, 也就是 $x$ 无论再怎么扩大， <code>tanh</code> 激励函数输出值也还是接近 $1$ 。</p>
<p>我们为了避免这种情况，就会对数据进行归一化，<strong>对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0​，方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。</strong>同时了为了恢复出原始的某一层所学到的特征，我们引入了这个可学习重构参数 $\gamma$ 、$\beta$ ，让我们的网络可以学习恢复出原始网络所要学习的特征分布。</p>
<h3 id="LayerNormalization"><a href="#LayerNormalization" class="headerlink" title="LayerNormalization"></a>LayerNormalization</h3><ul>
<li>BN在mini-batch较小的情况下不太适用。BN是对整个mini-batch的样本统计均值和方差，当训练样本数很少时，样本的均值和方差不能反映全局的统计分布信息，从而导致效果下降。</li>
<li>BN无法应用于RNN。<ul>
<li>RNN实际是共享的MLP，在时间维度上展开，每个step的输出是(bsz, hidden_dim)。由于不同句子的同一位置的分布大概率是不同的，所以应用BN来约束是没意义的。注：而BN应用在CNN可以的原因是同一个channel的特征图都是由同一个卷积核产生的。</li>
<li>LN原文的说法是：在训练时，对BN来说需要保存每个step的统计信息（均值和方差）。在测试时，由于变长句子的特性，测试集可能出现比训练集更长的句子，所以对于后面位置的step，是没有训练的统计量使用的。（不过实践中的话都是固定了max len，然后padding的。）</li>
<li>当然还有一种说法是，不同句子的长度不一样，对所有的样本统计均值是无意义的，因为某些样本在后面的timestep时其实是padding。</li>
<li>还有一种说法是（Normalization helps training of quantized lstm.）：应用BN层的话，每个timestep都需要去保存和计算batch统计量，耗时又耗力，后面就有人提出across timestep去shared BN的统计量，这明显不对，因为不同timestep的分布明显是不同的。</li>
<li>最后，大家发现LN的效果还很不错，比BN好，所以就变成NLP data里面的default config了。</li>
</ul>
</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306151052109.png" alt=""></p>
<h3 id="ResNet为什么不用Dropout？"><a href="#ResNet为什么不用Dropout？" class="headerlink" title="ResNet为什么不用Dropout？"></a>ResNet为什么不用Dropout？</h3><p><code>Dropout</code>与<code>BN</code>不兼容。<code>BN</code>在训练过程对每个单个样本的<code>forward</code>均引入多个样本的统计信息，相当于自带一定噪音，起到正则效果，所以也就基本消除了<code>Dropout</code>的必要。</p>
<h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p>每个层从前面的所有层获得额外的输入，并将自己的特征映射传递到后续的所有层，使用级联<code>(Concatenation)</code>方式，每一层都在接受来自前几层的“集体知识”<code>(collective knowledge)</code>。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222332494.png" alt=""></p>
<p>如图所示，第<code>i</code>层的输入不仅与<code>i-1</code>层的输出相关，还有所有之前层的输出有关，记作： $X_{l}=H_{l}([X_{0},\cdots,X_{l-1}])$ 。第<code>l</code>层产生 $k_{0}+(l-1)k$ 个<code>feature maps</code>，其中 $k$ 称为网络的增长率。</p>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>循环神经网络<code>(Recurrent Neural Network, RNN)</code>是一类以序列<code>(sequence)</code>数据为输入，在序列的演进方向进行递归<code>(recursion)</code>且所有节点（循环单元）按链式连接的递归神经网络<code>(recursive neural network)</code> 。</p>
<p>对循环神经网络的研究始于二十世纪八九十年代，并在二十一世纪初发展为深度学习算法之一 ，其中双向循环神经网络<code>(Bidirectional RNN, Bi-RNN)</code>和长短期记忆网络<code>(Long Short-Term Memory networks, LSTM)</code>是常见的循环神经网络。</p>
<h2 id="为什么需要RNN？"><a href="#为什么需要RNN？" class="headerlink" title="为什么需要RNN？"></a>为什么需要RNN？</h2><p>在<code>CNN</code>网络中的训练样本的数据为<code>IID</code>数据（独立同分布数据），所解决的问题也是分类问题或者回归问题或者是特征表达问题。<strong>但更多的数据是不满足IID的</strong>，如语言翻译，自动文本生成。它们是一个序列问题，包括时间序列和空间序列。比如时间序列数据，这类数据是在不同时间点上收集到的数据，反映了某一事物、现象等随时间的变化状态或程度。一般的神经网络，在训练数据足够、算法模型优越的情况下，给定特定的 $x$ ，就能得到期望 $y$ 。其一般处理单个的输入，前一个输入和后一个输入完全无关，但实际应用中，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。 这时就要用到<code>RNN</code>网络，<code>RNN</code>的结构图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071328562.png" alt=""></p>
<h2 id="RNN的主要应用领域"><a href="#RNN的主要应用领域" class="headerlink" title="RNN的主要应用领域"></a>RNN的主要应用领域</h2><p>可以说只要考虑时间先后顺序的问题都可以使用<code>RNN</code>来解决，这里主要说一下几个常见的应用领域：</p>
<ul>
<li>自然语言处理<code>(NLP)</code>：主要有视频处理，文本生成，语言模型，图像处理。</li>
<li>机器翻译，机器写文章。</li>
<li>语音识别。</li>
<li>图像描述生成。</li>
<li>文本相似度计算。</li>
<li>推荐系统。例如：音乐推荐、网易考拉商品推荐、<code>Youtube</code>视频推荐等新的应用领域。</li>
</ul>
<h2 id="RNN的计算过程"><a href="#RNN的计算过程" class="headerlink" title="RNN的计算过程"></a>RNN的计算过程</h2><p><code>RNN</code>引入了隐状态 $h$ ， $h$ 可对序列数据提取特征，接着再转换为输出。首先我们计算 $h_{1}$ ：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071332166.jpg" alt=""></p>
<p><code>RNN</code>中，每个步骤使用的参数 $U,W,b$ 相同， $h_{2},h_{3},h_{4}$ 的计算方式和 $h_{1}$ 类似，其计算结果如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071333924.jpg" alt=""></p>
<p>接下来，计算<code>RNN</code>的输出 $y_1$ ，采用<code>Softmax</code>作为激活函数，根据 $y_n=f(Wx+b)$ ，得到 $y_1$ :</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071335563.jpg" alt=""></p>
<p>使用和 $y_1$ 相同的参数 $V,c$ ，得到 $y_2,y_3,y_4$ 的输出结构：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071336003.jpg" alt=""></p>
<h2 id="RNN的建模方式"><a href="#RNN的建模方式" class="headerlink" title="RNN的建模方式"></a>RNN的建模方式</h2><h3 id="一对多-vector-to-sequence"><a href="#一对多-vector-to-sequence" class="headerlink" title="一对多(vector-to-sequence)"></a>一对多(vector-to-sequence)</h3><p>输入是一个单独的值，输出是一个序列。此时，有两种主要建模方式：</p>
<p>方式一：可只在其中的某一个序列进行计算，比如序列第一个进行输入计算，其建模方式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339814.jpg" alt=""></p>
<p>方式二：把输入信息 $X$ 作为每个阶段的输入，其建模方式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339457.jpg" alt=""></p>
<h3 id="多对一-sequence-to-vector"><a href="#多对一-sequence-to-vector" class="headerlink" title="多对一(sequence-to-vector)"></a>多对一(sequence-to-vector)</h3><p>输入是一个序列，输出是一个单独的值，此时通常在最后的一个序列上进行输出变换，其建模如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071340668.jpg" alt=""></p>
<h3 id="多对多-Encoder-Decoder"><a href="#多对多-Encoder-Decoder" class="headerlink" title="多对多(Encoder-Decoder)"></a>多对多(Encoder-Decoder)</h3><p><strong>步骤一</strong>：将输入数据编码成一个上下文向量 $c$ ，这部分称为<code>Encoder</code>，得到 $c$ 有多种方式，最简单的方法就是把<code>Encoder</code>的最后一个隐状态赋值给 $c$ ，还可以对最后的隐状态做一个变换得到 $c$ ，也可以对所有的隐状态做变换。其示意如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071341013.jpg" alt=""></p>
<p><strong>步骤二</strong>：用另一个<code>RNN</code>网络（我们将其称为<code>Decoder</code>）对其进行编码。</p>
<p>方法一是将步骤一中的 $c$ 作为初始状态输入到<code>Decoder</code>，示意图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071342703.jpg" alt=""></p>
<p>方法二是将 $c$ 作为<code>Decoder</code>的每一步输入，示意图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071343619.jpg" alt=""></p>
<h2 id="RNN中为什么会出现梯度消失？如何解决？"><a href="#RNN中为什么会出现梯度消失？如何解决？" class="headerlink" title="RNN中为什么会出现梯度消失？如何解决？"></a>RNN中为什么会出现梯度消失？如何解决？</h2><p><strong>梯度消失的原因：</strong><code>Sigmoid</code>函数的导数范围是 $(0,0.25]$，<code>tanh</code>函数的导数范围是 $(0,1]$ ，他们的导数最大都不大于 $1$ ，如果取<code>tanh</code>或<code>Sigmoid</code>函数作为激活函数嵌套到<code>RNN</code>中，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于 $0$ ，这就是“梯度消失“现象。实际使用中，会优先选择<code>tanh</code>函数，原因是<code>tanh</code>函数相对于<code>Sigmoid</code>函数来说梯度较大，收敛速度更快且引起梯度消失更慢。</p>
<p> <strong>解决RNN中的梯度消失方法主要有：</strong></p>
<ol>
<li>选取更好的激活函数，如<code>ReLU</code>激活函数。<code>ReLU</code>函数的左侧导数为 $0$ ，右侧导数恒为 $1$ ，这就避免了“梯度消失“的发生。但恒为 $1$ 的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。</li>
<li>加入<code>BN</code>层，其优点包括可加速收敛、控制过拟合，可以少用或不用<code>Dropout</code>和正则、降低网络对初始化权重不敏感，且能允许使用较大的学习率等。</li>
<li>改变传播结构，选择更高级的模型，例如：<code>LSTM</code>结构可以有效解决这个问题。</li>
</ol>
<h2 id="RNN的注意力机制"><a href="#RNN的注意力机制" class="headerlink" title="RNN的注意力机制"></a>RNN的注意力机制</h2><p>在上述的<code>Encoder-Decoder</code>结构中，<code>Encoder</code>把所有的输入序列都编码成一个统一的语义特征 $c$ 再解码。因此， $c$ 中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。如机器翻译问题，当要翻译的句子较长时，一个 $c$ 可能存不下那么多信息，就会造成翻译精度的下降。<code>Attention</code>机制通过在每个时间输入不同的 $c$ 来解决此问题。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071356215.png" alt=""></p>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><h2 id="传统RNN存在的问题"><a href="#传统RNN存在的问题" class="headerlink" title="传统RNN存在的问题"></a>传统RNN存在的问题</h2><p><strong>长期依赖(Long Term Dependencies)</strong></p>
<p>在深度学习领域中（尤其是<code>RNN</code>），“长期依赖”问题是普遍存在的。长期依赖产生的原因是当神经网络的节点经过许多阶段的计算后，之前比较长的时间片的特征已经被覆盖，例如：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101147762.png" alt=""></p>
<p>我们想预测<code>full</code>之前系动词的单复数情况，显然<code>full</code>是取决于第二个单词<code>cat</code>的单复数情况，而非其前面的单词<code>food</code>。随着数据时间片的增加，<code>RNN</code>丧失了学习连接如此远的信息的能力。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101148082.png" alt=""></p>
<p><strong>梯度消失/爆炸</strong></p>
<p>梯度消失和梯度爆炸是困扰<code>RNN</code>模型训练的关键原因之一，产生梯度消失和梯度爆炸是由于<code>RNN</code>的权值矩阵循环相乘导致的，相同函数的多次组合会导致极端的非线性行为。梯度消失和梯度爆炸主要存在<code>RNN</code>中，因为<code>RNN</code>中每个时间片使用相同的权值矩阵。对于一个<code>DNN</code>，虽然也涉及多个矩阵的相乘，但是通过精心设计权值的比例可以避免梯度消失和梯度爆炸的问题。</p>
<p>处理梯度爆炸可以采用梯度截断的方法。所谓梯度截断是指将梯度值超过阈值 $\theta$ 的梯度手动降到 $\theta$ 。虽然梯度截断会一定程度上改变梯度的方向，但梯度截断的方向依旧是朝向损失函数减小的方向。</p>
<p>对比梯度爆炸，梯度消失不能简单的通过类似梯度截断的阈值式方法来解决，因为长期依赖的现象也会产生很小的梯度。在上面例子中，我们希望 $t_{9}$ 时刻能够读到 $t_{1}$ 时刻的特征，在这期间内我们自然不希望隐层节点状态发生很大的变化，所以 $[t_{2},t_{8}]$ 时刻的梯度要尽可能的小才能保证梯度变化小。很明显，如果我们刻意提高小梯度的值将会使模型失去捕捉长期依赖的能力。</p>
<h2 id="LSTM-1"><a href="#LSTM-1" class="headerlink" title="LSTM"></a>LSTM</h2><p><code>LSTM</code>的全称是<code>Long Short Term Memory</code>，顾名思义，它具有记忆长短期信息的能力的神经网络。<code>LSTM</code>提出的动机是为了解决上面我们提到的长期依赖问题。传统的<code>RNN</code>节点输出仅由权值，偏置以及激活函数决定。<code>RNN</code>是一个链式结构，每个时间片使用的是相同的参数。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101504342.png" alt=""></p>
<p>而<code>LSTM</code>之所以能够解决<code>RNN</code>的长期依赖问题，是因为<code>LSTM</code>引入了门<code>(gate)</code>机制用于控制特征的流通和损失。对于上面的例子，<code>LSTM</code>可以做到在 $t_{9}$ 时刻将 $t_{2}$ 时刻的特征传过来，这样就可以非常有效的判断 $t_{9}$ 时刻使用单数还是复数了。<code>LSTM</code>是由一系列<code>LSTM</code>单元<code>(LSTM Unit)</code>组成，其链式结构如下图。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101504156.png" alt=""></p>
<p><code>LSTM</code>的核心部分是在上图最上边类似于传送带的部分，这一部分一般叫做单元状态<code>(cell state)</code>它自始至终存在于<code>LSTM</code>的整个链式系统中。其中 $C_{t}=f_{t} \times C_{t-1} + i_{t} \times \tilde{C}_{t}$ ，其中 $f_{t}$ 叫做遗忘门，表示 $C_{t-1}$ 的哪些特征被用于计算 $C_{t}$ 。 $f_{t}$ 是一个向量，向量的每个元素均位于 $[0,1]$ 范围内。通常我们使用<code>Sigmoid</code>作为激活函数，<code>Sigmoid</code>的输出是一个介于 $[0,1]$ 区间内的值，但是当你观察一个训练好的<code>LSTM</code>时，你会发现门的值绝大多数都非常接近 $0$ 或者 $1$ ，其余的值少之又少。其中 $\otimes$ 是<code>LSTM</code>最重要的门机制，表示 $f_{t}$ 和 $C_{t-1}$ 之间的单位乘的关系。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101539655.png" alt=""></p>
<p> $\tilde{C}_{t}$ 表示单元状态更新值，由输入数据 $x_{t}$ 和隐节点 $h_{t-1}$ 经由一个神经网络层得到，单元状态更新值的激活函数通常使用<code>tanh</code>。 $i_{t}$ 叫做输入门，同 $f_{t}$ 一样也是一个元素介于 $[0,1]$ 区间内的向量，同样由 $x_{t}$ 和 $h_{t-1}$ 经由<code>Sigmoid</code>激活函数计算而成。 $i_{t}$ 用于控制 $\tilde{C}_{t}$ 的哪些特征用于更新 $C_{t}$ ，使用方式和 $f_t$ 相同。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101545859.png" alt=""></p>
<p>最后，为了计算预测值 $\hat{y}_{t}$ 和生成下个时间片完整的输入，我们需要计算隐节点的输出 $h_{t}$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101553212.png" alt=""></p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p><code>Transformer</code>是一个利用注意力机制来提高模型训练速度的模型。<code>Transformer</code>可以说是完全基于自注意力机制的一个深度学习模型，因为它适用于并行化计算，和它本身模型的复杂程度导致它在精度和性能上都要高于之前流行的<code>RNN</code>循环神经网络。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302227637.png" alt=""></p>
<p>当我输入一个文本的时候，该文本数据会先经过一个叫<code>Encoders</code>的模块，对该文本进行编码，然后将编码后的数据再传入一个叫<code>Decoders</code>的模块进行解码，解码后就得到了翻译后的文本，对应的我们称<code>Encoders</code>为编码器，<code>Decoders</code>为解码器。一般情况下，<code>Encoders</code>里边有 $6$ 个小编码器<code>(Encoder)</code>，<code>Decoders</code>里边有 $6$ 个小解码器<code>(Decoder)</code>。我们看到，在编码部分，每一个的小编码器的输入是前一个小编码器的输出，而每一个小解码器的输入不光是它的前一个解码器的输出，还包括了整个编码部分的输出。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302042116.png" alt=""></p>
<h2 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h2><p>Encoder输入源语言序列，Decoder里面输入需要被翻译的语言文本。一个文本常有许多序列组成，常见操作为将序列进行一些预处理（如词切分等）变成列表，一个序列的列表的元素通常为词表中不可切分的最小词，整个文本就是一个大列表，元素为一个个由序列组成的列表。如一个序列经过切分后变为[“am”, “##ro”, “##zi”, “meets”, “his”, “father”]，接下来按照它们在词表中对应的索引进行转换，假设结果如[23, 94, 13, 41, 27, 96]。假如整个文本一共100个句子，那么就有100个列表为它的元素，因为每个序列的长度不一，需要设定最大长度，这里不妨设为128，那么将整个文本转换为数组之后，形状即为100 x 128，这就对应着batch_size和seq_length。</p>
<p>输入之后，紧接着进行词嵌入处理，词嵌入就是将每一个词用预先训练好的向量进行映射。词嵌入在torch里基于<code>torch.nn.Embedding</code>实现，实例化时需要设置的参数为词表的大小和被映射的向量的维度比如<code>embed = nn.Embedding(10,8)</code>。向量的维度通俗来说就是向量里面有多少个数。注意，第一个参数是词表的大小，如果你目前最多有8个词，通常填写10（多一个位置留给unk和pad），你后面万一进入与这8个词不同的词就映射到unk上，序列padding的部分就映射到pad上。</p>
<p>假如我们打算映射到8维（num_features或者embed_dim），那么整个文本的形状变为100 x 128 x 8。接下来举个小例子解释一下：假设我们词表一共有10个词（算上unk和pad），文本里有2个句子，每个句子有4个词，每个词在词表中都有一个index，然后我们通过index取embed中对应的向量，就可以把每个词映射到8维的向量。</p>
<h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><p>Self-Attention属于Attention，要求QKV必须同源，依然代表X，本质上可以看作是相等的，只是对同一个词向量X乘上了参数矩阵，作了空间上的变换。</p>
<h3 id="Cross-attention"><a href="#Cross-attention" class="headerlink" title="Cross-attention"></a>Cross-attention</h3><ul>
<li>Transformer架构中混合两种不同嵌入序列的注意机制</li>
<li>两个序列<strong>必须具有相同的维度</strong></li>
<li>两个序列可以是不同的模式形态（如：文本、声音、图像）</li>
<li>一个序列作为输入的Q，定义了输出的序列长度，另一个序列提供输入的K&amp;V</li>
</ul>
<h2 id="编码器-Encoder"><a href="#编码器-Encoder" class="headerlink" title="编码器(Encoder)"></a>编码器(Encoder)</h2><p>我们放大一个<code>Encoder</code>，发现里边的结构是一个多头自注意力机制加上一个前馈神经网络。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302053360.png" alt=""></p>
<p>像大部分<code>NLP</code>应用一样，我们首先将每个输入单词通过词嵌入算法转换为词向量，每个单词都被嵌入为 $512$ 维的向量。</p>
<ul>
<li>计算自注意力的第一步就是从每个编码器的输入向量（每个单词的词向量）中生成三个向量。也就是说对于每个单词，我们创造一个查询向量、一个键向量和一个值向量。这三个向量是通过词嵌入与三个权重矩阵后相乘创建的。可以发现这些新向量在维度上比词嵌入向量更低。他们的维度是 $64$ ，而词嵌入和编码器的输入/输出向量的维度是 $512$ 。但实际上不强求维度更小，这只是一种基于架构上的选择，它可以使多头注意力<code>(Multi-Head Attention)</code>的大部分计算保持不变。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302112845.png" alt=""></p>
<ul>
<li>计算自注意力的第二步是计算得分。这个得分是通过计算<code>Q</code>与各个单词的<code>K</code>向量的点积得到的。我们以<code>X1</code>为例，分别将<code>Q1</code>和<code>K1</code>、<code>K2</code>进行点积运算，假设分别得到得分 $112$ 和 $96$ 。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302133753.png" alt=""></p>
<ul>
<li>将得分分别除以一个特定数值 $8$ （<code>K</code>向量的维度的平方根，通常<code>K</code>向量的维度是 $64$ ）这能让梯度更加稳定，则得到结果如下：</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302134012.png" alt=""></p>
<ul>
<li>将上述结果进行<code>softmax</code>运算得到，<code>softmax</code>主要将分数标准化，使他们都是正数并且加起来等于 $1$ 。这个<code>softmax</code>分数决定了每个单词对编码当下位置的贡献。显然，已经在这个位置上的单词将获得最高的<code>softmax</code>分数，但有时关注另一个与当前单词相关的单词也会有帮助。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302137784.png" alt=""></p>
<ul>
<li>将<code>V</code>向量乘上<code>softmax</code>的结果，这个思想主要是为了保持我们想要关注的单词的值不变，而掩盖掉那些不相关的单词（例如将他们乘上很小的数字）。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302138098.png" alt=""></p>
<ul>
<li>将带权重的各个<code>V</code>向量加起来，至此，产生在这个位置上（第一个单词）的<code>self-attention</code>层的输出，其余位置的<code>self-attention</code>输出也是同样的计算方式。（注：自注意力的另一种解释就是在编码某个单词时，就是将所有单词的表示（值向量<code>V</code>）进行加权求和，而权重是通过该词的表示（键向量<code>K</code>）与被编码词表示（查询向量<code>Q</code>）的点积并通过<code>softmax</code>得到。）</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302139598.png" alt=""></p>
<p>论文为了进一步细化自注意力机制层，增加了<strong>多头注意力机制</strong>的概念，这从两个方面提高了自注意力层的性能。</p>
<ol>
<li>扩展了模型关注不同位置的能力，比如<code>Apple</code>可能和<code>Banana</code>比较相关，但是如果使用单个自注意力机制可能这种关系就被<code>Apple</code>自己支配了（体现在<code>softmax</code>之后的权重最大），而采用多头自注意力机制则可以缓解这种现象。</li>
<li>他给了自注意力层多个表示子空间。对于多头自注意力机制，我们不止有一组权重矩阵，而是有多组（论文中使用 $8$ 组），所以每个编码器/解码器使用 $8$ 个头（可以理解为 $8$ 个互不干扰自的注意力机制运算），每一组的<code>Q/K/V</code>都不相同。然后，得到 $8$ 个不同的权重矩阵 <code>Z</code>，每个权重矩阵被用来将输入向量投射到不同的表示子空间。论文中说到这样的好处是可以允许模型在不同的表示子空间里学习到相关的信息。输出矩阵的维度是（序列长度×单词向量长度）</li>
</ol>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302156146.png" alt=""></p>
<p>为了解决梯度消失的问题，在<code>Encoder</code>和<code>Decoder</code>中都是用了<strong>残差神经网络</strong>的结构，即每一个前馈神经网络的输入不光包含上述<code>self-attention</code>的输出，还包含最原始的输入。</p>
<h3 id="为什么选择除以-sqrt-d"><a href="#为什么选择除以-sqrt-d" class="headerlink" title="为什么选择除以$\sqrt{d}$"></a>为什么选择除以$\sqrt{d}$</h3><ol>
<li><p>防止softmax输入值过大，当embedding的维度越大，矩阵乘法的数值越大，所以防止softmax输入值过大，偏导数趋于0，有益于训练稳定；</p>
</li>
<li><p>$\frac{qk}{\sqrt{d}}$服从均值为0，方差为1的分布，作归一化；</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202306151023478.png" alt=""></p>
</li>
<li><p>类似softmax加温度系数，温度系数根号d越大，softmax输出越平滑（而非尖锐），如果不除以$\sqrt{d}$，相当于softmax输出更尖锐，进而导致梯度稀疏。</p>
</li>
</ol>
<h2 id="解码器-Decoder"><a href="#解码器-Decoder" class="headerlink" title="解码器(Decoder)"></a>解码器(Decoder)</h2><p>同样的，在<code>Decoder</code>中使用的也是类似的结构。不同的地方在于，<code>Decoder</code>不是并行的，而是像<code>RNN</code>一样是一个一个产生的，有时序概念，在这个前提之下，<code>Encoder</code>的第一个模块是一个<code>Masked Multi-Head Attention</code>（就是生产第一步只有一个词，自己做<code>self-attention</code>，第二步生成两个词的时候，就做两个词的<code>self-attetion</code>,那么每一步在该层有一个输出，假设为<code>Q</code>，那么送入到中间的<code>Multi-Head Attention</code>层，和<code>encncoder</code>部分的<code>K</code>，<code>V</code>做<code>attention</code>）。进行过自注意力机制后，将<code>self-attention</code>的输出再与<code>Encoder</code>模块的输出计算一遍注意力机制得分之后，再进入前馈神经网络模块。</p>
<h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p>解码器输出本来是一个浮点型的向量，怎么转化成这两个词呢？解决办法是在最后的线性层接上一个<code>softmax</code>，其中线性层是一个简单的全连接神经网络，它将解码器产生的向量投影到一个更高维度的向量<code>(logits)</code>上，假设我们模型的词汇表是 $10000$ 个词，那么<code>logits</code>就有 $10000$ 个维度，每个维度对应一个唯一的词的得分。之后的<code>softmax</code>层将这些分数转换为概率。选择概率最大的维度，并对应地生成与之关联的单词作为此时间步的输出就是最终的输出。</p>
<h2 id="位置编码-Positional-Encoding"><a href="#位置编码-Positional-Encoding" class="headerlink" title="位置编码(Positional Encoding)"></a>位置编码(Positional Encoding)</h2><p><code>Transformer</code>中没有考虑顺序信息，那怎么办呢，我们可以在输入中做手脚，把输入变得有位置信息。我们可以给每个词向量加上一个有顺序特征的向量，发现<code>sin</code>函数和<code>cos</code>函数能够很好的表达这种特征，所以通常位置向量用以下公式来表示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310947858.png" alt=""></p>
<p>也即第 $t$ 个位置的位置编码为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310948367.png" alt=""></p>
<p>对编码的可视化：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310952548.png" alt=""></p>
<h3 id="Vanilla-Transformer的位置编码的特点"><a href="#Vanilla-Transformer的位置编码的特点" class="headerlink" title="Vanilla Transformer的位置编码的特点"></a>Vanilla Transformer的位置编码的特点</h3><ol>
<li>只要位置小于 $10000$ ，每一个位置的编码都是不同的。</li>
<li>奇数维度之间或者偶数维度之间周期不同。</li>
<li>也可以很好的表示相对位置信息。给定k,存在一个固定的与 $k$ 相关的线性变换矩阵，从而由 <code>pos</code> 的位置编码线性变换而得到 <code>pos+k</code> 的位置编码。这个相对位置信息可能可以被模型发现而利用。因为绝对位置信息只保证了各个位置不一样，但是并不是像 $0,1,2$ 这样的有明确前后关系的编码。</li>
</ol>
<p>我们拿出位置编码的两个维度出来做个例子，其他维度也是一样的，可以拼接起来变成完整的位置编码：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311001584.png" alt=""></p>
<p>其中 $M$ 为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311002558.png" alt=""></p>
<p>上面的操作也只可以看到线性关系，怎么可以更直白地知道每个<code>token</code>的距离关系？我们将两个位置编码对应相乘，可以发现发现相乘后的结果为一个余弦的加和。这里影响值的因素就是 $k$ 。如果两个<code>token</code>的距离越大，也就是 $k$ 越大，根据余弦函数的性质可以知道，两个位置编码相乘结果越小。这样的关系可以得到，如果两个<code>token</code>距离越远则乘积的结果越小。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311016054.png" alt=""></p>
<h3 id="其他编码方式"><a href="#其他编码方式" class="headerlink" title="其他编码方式"></a>其他编码方式</h3><p><strong>用[0,1]范围标记位置</strong></p>
<p>产生的问题是，当序列长度不同时，<code>token</code>间的相对距离是不一样的。例如在序列长度为 $3$ 时，<code>token</code>间的相对距离为 $0.5$ ；在序列长度为 $4$ 时，<code>token</code>间的相对距离就变为 $0.33$ 。</p>
<p><strong>用整型值标记位置</strong></p>
<p>存在的问题是：模型可能遇见比训练时所用的序列更长的序列，不利于模型的泛化；模型的位置表示是无界的，随着序列长度的增加，位置值会越来越大。</p>
<p><strong>用二进制向量标记位置</strong></p>
<p>这种编码方式也存在问题是编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的。</p>
<h3 id="Vanilla-Transformer位置编码的缺点以及改进"><a href="#Vanilla-Transformer位置编码的缺点以及改进" class="headerlink" title="Vanilla Transformer位置编码的缺点以及改进"></a>Vanilla Transformer位置编码的缺点以及改进</h3><p>看一个序列中，第 $i$ 个单词和第 $j$ 个单词的<code>attention score</code>的计算：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311035374.png" alt=""></p>
<p>其中 $W_{q}$ ， $W_{k}$ 分别是<code>Multi-Head Attention</code>给每个头加的<code>Query</code>和<code>Key</code>参数， $E_{x_{i}}$ 和 $E_{x_{j}}$ 是 $x_{i}$ 和 $x_{j}$ 的词嵌入， $U_{i}$ 和 $U_{j}$ 是第 $i$ 个位置和第 $j$ 个位置的位置向量。因式分解得到下式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311039224.png" alt=""></p>
<p>实际上，按照<code>Vanilla Transformer</code>的位置编码方法，如果没有 $W_{q}$ 和 $W_{k}$ 那么它是包含相对位置信息的，证明见上上小节。但是中间加入一个“不可知”的线性变换以后，就没有相对位置信息了，这个可以使用实验证明，具体如下图：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311047793.png" alt=""></p>
<p><strong>Transformer中加入相对位置信息的改进方法</strong></p>
<ul>
<li><p>既然相对位置信息是在<code>self-attention</code>计算时候丢失的，那么最直接的想法就是在计算<code>self-attention</code>的时候再加回来。该工作出自<code>Transformer</code>的原班人马，具体做法是在计算<code>attention score</code>和<code>weighted value</code>时各加入一个可训练的表示相对位置的参数，并且<code>multi-head</code>之间可以共享。</p>
</li>
<li><p>改写<code>self-attention</code>的计算公式</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311102674.png" alt=""></p>
</li>
<li><p>前面两个方法都是基于”词向量+位置向量”的模式，说白了是在“亡羊补牢”，而没有一开始就修建一个牢固的羊圈。而一种新的角度是推导出一个<strong>复数域</strong>的词向量方法，理论十分优美。</p>
</li>
</ul>
<h1 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a>Vision Transformer</h1><h2 id="模型组成"><a href="#模型组成" class="headerlink" title="模型组成"></a>模型组成</h2><p>模型由三个模块组成：</p>
<ul>
<li><code>Linear Projection of Flattened Patches</code>（<code>Embedding</code>层）</li>
<li><code>Transformer Encoder</code></li>
<li><code>MLP Head</code>（最终用于分类的层结构）</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101646509.png" alt=""></p>
<h2 id="Embedding层结构详解"><a href="#Embedding层结构详解" class="headerlink" title="Embedding层结构详解"></a>Embedding层结构详解</h2><p>对于标准的<code>Transformer</code>模块，要求输入的是<code>token</code>（向量）序列，即二维矩阵<code>[num_token, token_dim]</code>，<code>0-9</code>对应的<code>token</code>都是向量，以<code>ViT-B/16</code>为例，每个<code>token</code>向量长度为 $768$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101836412.png" alt=""></p>
<p>对于图像数据而言，其数据格式为<code>[H, W, C]</code>是三维矩阵明显不是<code>Transformer</code>想要的。所以需要先通过一个<code>Embedding</code>层来对数据做个变换。如上图所示，首先将一张图片按给定大小分成一堆<code>Patches</code>。以<code>ViT-B/16</code>为例，将输入图片<code>(224x224)</code>按照<code>16x16</code>大小的<code>Patch</code>进行划分，划分后会得到 $( 224 / 16 )^{2} = 196$ 个<code>Patches</code>。接着通过线性映射将每个<code>Patch</code>映射到一维向量中，以<code>ViT-B/16</code>为例，每个<code>Patch</code>数据<code>shape</code>为 $[16, 16, 3]$ 通过映射得到一个长度为 $768$ 的向量（后面都直接称为<code>token</code>）。</p>
<p><strong>在输入Transformer Encoder之前注意需要加上[class]token以及Position Embedding。</strong> 在原论文中，作者说参考<code>BERT</code>，在刚刚得到的一堆<code>tokens</code>中插入一个专门用于分类的<code>[class]token</code>，这个<code>[class]token</code>是一个可训练的参数，数据格式和其他<code>token</code>一样都是一个向量，以<code>ViT-B/16</code>为例，就是一个长度为<code>768</code>的向量，与之前从图片中生成的<code>tokens</code>拼接在一起， $([1, 768], [196, 768]) \rightarrow [197, 768]$ 。然后关于<code>Position Embedding</code>就是之前<code>Transformer</code>中讲到的<code>Positional Encoding</code>，这里的<code>Position Embedding</code>采用的是一个可训练的参数，是直接叠加在<code>tokens</code>上的<code>(add)</code>，所以<code>shape</code>要一样。以<code>ViT-B/16</code>为例，刚刚拼接<code>[class]token</code>后<code>shape</code>是 $[197, 768]$ ，那么这里的<code>Position Embedding</code>的<code>shape</code>也是 $[197, 768]$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101836048.png" alt=""></p>
<h2 id="Transformer-Encoder详解"><a href="#Transformer-Encoder详解" class="headerlink" title="Transformer Encoder详解"></a>Transformer Encoder详解</h2><p><code>Transformer Encoder</code>其实就是重复堆叠<code>Encoder Block</code> $L$ 次，主要由以下几部分组成：</p>
<ul>
<li><code>Layer Norm</code>，这种<code>Normalization</code>方法主要是针对<code>NLP</code>领域提出的，这里是对每个<code>token</code>进行<code>Norm</code>处理。</li>
<li><code>Multi-Head Attention</code>，也就是<code>Transformer</code>中的多头注意力。</li>
<li><code>Dropout/DropPath</code>，在原论文的代码中是直接使用的<code>Dropout</code>层，但<code>rwightman</code>实现的代码中使用的是<code>DropPath(stochastic depth)</code>，可能后者会更好一点。</li>
<li><code>MLP Block</code>，就是全连接+<code>GELU</code>激活函数+<code>Dropout</code>组成也非常简单，需要注意的是第一个全连接层会把输入节点个数翻 $4$ 倍 $[197, 768] \rightarrow [197, 3072]$ ，第二个全连接层会还原回原节点个数 $[197, 3072] \rightarrow [197, 768]$ 。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101738369.png" alt=""></p>
<h2 id="MLP-Head详解"><a href="#MLP-Head详解" class="headerlink" title="MLP Head详解"></a>MLP Head详解</h2><p>上面通过<code>Transformer Encoder</code>后输出的<code>shape</code>和输入的<code>shape</code>是保持不变的，以<code>ViT-B/16</code>为例，输入的是 $[197, 768]$ 输出的还是 $[197, 768]$。注意，在<code>Transformer Encoder</code>后其实还有一个<code>Layer Norm</code>没有画出来。这里我们只是需要分类的信息，所以我们只需要提取出<code>[class]token</code>生成的对应结果就行，即 $[197, 768]$ 中抽取出 $[class]token$ 对应的 $[1, 768]$ 。接着我们通过<code>MLP Head</code>得到我们最终的分类结果。<code>MLP Head</code>原论文中说在训练<code>ImageNet21K</code>时是由<code>Linear</code>+<code>tanh</code>激活函数+<code>Linear</code>组成。但是迁移到<code>ImageNet1K</code>上或者自己的数据上时，只用一个<code>Linear</code>即可。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101841213.png" alt=""></p>
<h1 id="Swin-Transformer"><a href="#Swin-Transformer" class="headerlink" title="Swin Transformer"></a>Swin Transformer</h1><h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><p>每个窗口固定有 $7\ast7$ 个patch，所以会有 $8\ast8$个window。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202307131454285.png" alt=""></p>
<h2 id="Patch-merging"><a href="#Patch-merging" class="headerlink" title="Patch merging"></a>Patch merging</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202307131152042.png" alt=""></p>
<h2 id="W-MSA"><a href="#W-MSA" class="headerlink" title="W-MSA"></a>W-MSA</h2><p>MSA全称为Windows Multi-head Self-Attention也就是窗口化的Self-Attention机制，此处以在一个 $4\ast4$ 的特征图上做为例子，在Vision Transformer中的MSA模块， $4\ast4$ 中的每个像素都要去和其他像素进行关联度的计算，那么在W-MSA中，其将原 $4\ast4$ 的特征图首先分割成了 $4$ 个 $2\ast2$ 的Window窗口，然后再在每个窗口内部进行单独的Self-Attention的计算。也就是说，每个像素只需要和自己所属Window内部的像素进行关联度的计算即可。这样一来，确实大大减少了计算量，但是你会发现窗口之间的像素也无法进行通信了，导致我们的感受野变小，对于最终的结果产生影响。优劣势还是非常的明确的。</p>
<h2 id="SW-MSA"><a href="#SW-MSA" class="headerlink" title="SW-MSA"></a>SW-MSA</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202307131153000.png" alt=""></p>
<h1 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p><strong>CTR(Click-Through-Rate)</strong>：即点击通过率，是互联网广告常用的术语，指网络广告的点击到达率，即该广告的实际点击次数（严格的来说，可以是到达目标页面的数量）除以广告的展现量。</p>
<p>点击量过小时，是由两种原因造成的，展现量过小，或点击数偏低。</p>
<ol>
<li>展现量低，进而点击数也小。展现量过低说明潜在受众搜索需求发生的较少，也即推广结果展现在潜在受众前的机会较少，推广商户可以通过拓展关键词来提高展现量，即提高推广信息展现的机会。</li>
<li>展现量高，但是点击数偏低，造成点击率偏低。这种情况可能的原因是：<ul>
<li>关键词与文案的相关性不高，所以无法满足潜在受众的需求，进而点击数小。可以通过改善文案写作，提高关键词与文案的相关性来提高点击率。</li>
<li>推广结果的平均排名较低，不具有竞争力。可以通过调高平均点击价格来提高排名。</li>
<li>关键词匹配模式的问题。例如，推广商户购买了“葡萄”等相关关键词，用户在搜索“葡萄牙”时商户的推广结果也可能会出现，这时推广结果就是无效展现，即为推广结果信息没有展现在潜在受众前。此种情况就需通过“否定匹配”模式来解决，将“葡萄牙”设置为否定匹配，即为用户在搜索“葡萄牙”时，推广商户的推广结果不会展现，降低了无效展现的风险。</li>
</ul>
</li>
</ol>
<h2 id="推荐系统架构"><a href="#推荐系统架构" class="headerlink" title="推荐系统架构"></a>推荐系统架构</h2><p>从<strong>系统架构</strong>和<strong>算法架构</strong>两个角度出发解析推荐系统通用架构。系统架构设计思想是大数据背景下如何有效利用海量和实时数据，将推荐系统按照对数据利用情况和系统响应要求出发，将整个架构分为<strong>离线层、近线层、在线层</strong>三个模块。而算法架构是从我们比较熟悉的<strong>召回、粗排、排序、重排</strong>等算法环节角度出发的，重要的是要去理解每个环节需要完成的任务，每个环节的评价体系，以及为什么要那么设计。</p>
<h3 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h3><p>推荐系统架构，首先从数据驱动角度，对于数据，最简单的方法是存下来，留作后续离线处理，<strong>离线层</strong>就是我们用来管理离线作业的部分架构。<strong>在线层</strong>能更快地响应最近的事件和用户交互，但必须实时完成。这会限制使用算法的复杂性和处理的数据量。离线计算对于数据数量和算法复杂度限制更少，因为它以批量方式完成，没有很强的时间要求。不过，由于没有及时加入最新的数据，所以很容易过时。个性化架构的关键问题，就是如何以无缝方式结合、管理在线和离线计算过程。<strong>近线层</strong>介于两种方法之间，可以执行类似于在线计算的方法，但又不必以实时方式完成。</p>
<h3 id="算法架构"><a href="#算法架构" class="headerlink" title="算法架构"></a>算法架构</h3><p>一个通用的算法架构，设计思想就是对数据层层建模，层层筛选，帮助用户从海量数据中找出其真正感兴趣的部分。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192001842.png" alt=""></p>
<p><strong>召回</strong></p>
<p>召回层的主要目标时从推荐池中选取几千上万的item，送给后续的排序模块。由于召回面对的候选集十分大，且一般需要在线输出，故召回模块必须轻量快速低延迟。由于后续还有排序模块作为保障，召回不需要十分准确，但<strong>不可遗漏</strong>（特别是搜索系统中的召回模块）。</p>
<p>召回主要考虑的内容有：</p>
<ol>
<li><strong>考虑用户层面</strong>：用户兴趣的多元化，用户需求与场景的多元化：例如：新闻需求，重大要闻，相关内容沉浸阅读等等</li>
<li><strong>考虑系统层面</strong>：增强系统的鲁棒性；部分召回失效，其余召回队列兜底不会导致整个召回层失效；排序层失效，召回队列兜底不会导致整个推荐系统失效</li>
<li><strong>系统多样性内容分发</strong>：图文、视频、小视频；精准、试探、时效一定比例；召回目标的多元化，例如：相关性，沉浸时长，时效性，特色内容等等</li>
<li><strong>可解释性推荐一部分召回是有明确推荐理由的</strong>：很好的解决产品性数据的引入</li>
</ol>
<p><strong>粗排</strong></p>
<p>粗排的原因是有时候召回的结果还是太多，精排层速度还是跟不上，所以加入粗排。粗排可以理解为精排前的一轮过滤机制，减轻精排模块的压力。粗排介于召回和精排之间，要同时兼顾精准性和低延迟。目前粗排一般也都模型化了，其训练样本类似于精排，选取曝光点击为正样本，曝光未点击为负样本。但由于粗排一般面向上万的候选集，而精排只有几百上千，其解空间大很多。</p>
<p><strong>精排</strong></p>
<p>精排层，也是我们学习推荐入门最常常接触的层，我们所熟悉的算法很大一部分都来自精排层。这一层的任务是获取粗排模块的结果，对候选集进行打分和排序。精排需要在最大时延允许的情况下，保证打分的精准性，是整个系统中至关重要的一个模块，也是最复杂，研究最多的一个模块。</p>
<p>精排是推荐系统各层级中最纯粹的一层，他的目标比较单一且集中，一门心思的实现目标的调优即可。最开始的时候精排模型的常见目标是CTR,后续逐渐发展了CVR等多类目标。精排和粗排层的基本目标是一致的，都是对商品集合进行排序，但是和粗排不同的是，精排只需要对少量的商品(即粗排输出的商品集合的Top-N)进行排序即可。因此，精排中可以使用比粗排更多的特征，更复杂的模型和更精细的策略（用户的特征和行为在该层的大量使用和参与也是基于这个原因）。</p>
<p><strong>重排</strong></p>
<p>常见的有三种优化目标：Point Wise、Pair Wise 和 List Wise。重排序阶段对精排生成的Top-N个物品的序列进行重新排序，生成一个Top-K个物品的序列，作为排序系统最后的结果，直接展现给用户。重排序的原因是因为多个物品之间往往是相互影响的，而精排序是根据Point Wise得分，容易造成推荐结果同质化严重，有很多冗余信息。而重排序面对的挑战就是海量状态空间如何求解的问题，一般在精排层我们使用AUC作为指标，但是在重排序更多关注NDCG等指标。</p>
<p><strong>混排</strong></p>
<p>多个业务线都想在Feeds流中获取曝光，则需要对它们的结果进行混排。比如推荐流中插入广告、视频流中插入图文和banner等。可以基于规则策略（如广告定坑）和强化学习来实现。</p>
<h2 id="经典召回模型"><a href="#经典召回模型" class="headerlink" title="经典召回模型"></a>经典召回模型</h2><h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><ul>
<li>召回率：在模型召回预测的物品中，预测准确的物品占用户实际喜欢的物品的比例。对用户 $u$ 推荐 $N$ 个物品记为 $R(u)$ , 令用户 $u$ 在测试集上喜欢的物品集合为 $T(u)$ ， 那么召回率定义为：</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192101633.png" alt=""></p>
<ul>
<li>准确率：推荐的物品中，对用户准确推荐的物品占总物品的比例。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192102103.png" alt=""></p>
<ul>
<li>覆盖率：覆盖率反映了推荐算法发掘长尾的能力， 覆盖率越高， 说明推荐算法越能将长尾中的物品推荐给用户。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192115000.png" alt=""></p>
<ul>
<li>新颖度：用推荐列表中物品的平均流行度度量推荐结果的新颖度。 如果推荐出的物品都很热门， 说明推荐的新颖度较低。 由于物品的流行度分布呈长尾分布， 所以为了流行度的平均值更加稳定， 在计算平均流行度时对每个物品的流行度取对数。</li>
</ul>
<h3 id="基于协同过滤的召回"><a href="#基于协同过滤的召回" class="headerlink" title="基于协同过滤的召回"></a>基于协同过滤的召回</h3><p>协同过滤算法<code>(Collaborative Filtering)</code>是比较经典常用的推荐算法，它是一种完全依赖用户和物品之间行为关系的推荐算法。我们从它的名字“协同过滤”中，也可以窥探到它背后的原理，就是 “协同大家的反馈、评价和意见，一起对海量的信息进行过滤，从中筛选出用户可能感兴趣的信息”。协同过滤算法的主要分类：</p>
<ul>
<li>基于物品的协同过滤算法：给用户推荐与他之前喜欢的物品相似的物品。</li>
<li>基于用户的协同过滤算法：给用户推荐与他兴趣相似的用户喜欢的物品。</li>
</ul>
<h4 id="基于物品的协同过滤算法"><a href="#基于物品的协同过滤算法" class="headerlink" title="基于物品的协同过滤算法"></a>基于物品的协同过滤算法</h4><p><strong>基于物品的协同过滤(ItemCF)</strong>的基本思想是预先根据所有用户的历史偏好数据计算物品之间的相似性，然后把与用户喜欢的物品相类似的物品推荐给用户。比如物品<code>a</code>和<code>c</code>非常相似，因为喜欢<code>a</code>的用户同时也喜欢<code>c</code>，而用户<code>A</code>喜欢<code>a</code>，所以把<code>c</code>推荐给用户<code>A</code>。<code>ItemCF</code>算法并不利用物品的内容属性计算物品之间的相似度， 主要通过分析用户的行为记录计算物品之间的相似度， 该算法认为， 物品<code>a</code>和物品<code>c</code>具有很大的相似度是因为喜欢物品<code>a</code>的用户大都喜欢物品<code>c</code>。</p>
<h5 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h5><p>基于物品的协同过滤算法主要分为两步：</p>
<ul>
<li>计算物品之间的相似度；</li>
<li>根据物品的相似度和用户的历史行为给用户生成推荐列表（购买了该商品的用户也经常购买的其他商品）。</li>
</ul>
<h5 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h5><p><strong>购买了该商品的用户也经常购买的其他商品</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202215784.png" alt=""></p>
<p>分母 $\vert N(i) \vert$  是喜欢物品 $i$ 的用户数，而分子 $\vert N(i)\bigcap N(j)\vert$ 是同时喜欢物品 $i$ 和物品 $j$ 的用户数。因此，上述公式可以理解为喜欢物品 $i$ 的用户中有多少比例的用户也喜欢物品 $j$ 。<br>上述公式虽然看起来很有道理，但是却存在一个问题。如果物品 $j$ 很热门，很多人都喜欢，那么 $w_{ij}$ 就会很大，接近 $1$ 。因此，该公式会造成任何物品都会和热门的物品有很大的相似度，这对于致力于挖掘长尾信息的推荐系统来说显然不是一个好的特性。为了避免推荐出热门的物品，可以用下面的公式，这个公式惩罚了物品 $j$ 的权重，因此减轻了热门物品会和很多物品相似的可能性。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202218223.png" alt=""></p>
<p>除了在计算物品之间相似度时可以对热门物品进行惩罚外，可以在此基础上，进一步引入参数 $\alpha$ ，这样可以通过控制参数 $\alpha$ 来决定对热门物品的惩罚力度。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192140478.png" alt=""></p>
<p><strong>相似度算法改进</strong></p>
<p>在协同过滤中两个物品产生相似度是因为它们共同出现在很多用户的兴趣列表中。也就是说，每个用户的兴趣列表都对物品的相似度产生贡献。那么是不是每个用户的贡献都相同呢?</p>
<p>假设有这么一个用户，他是开书店的，并且买了当当网上<code>80%</code>的书准备用来自己卖。那么他的购物车里包含当当网<code>80%</code>的书。假设当当网有<code>100</code>万本书，也就是说他买了<code>80</code>万本。从前面对<code>ItemCF</code>的讨论可以看到，这意味着因为存在这么一个用户，有<code>80</code>万本书两两之间就产生了相似度，也就是说，内存里即将诞生一个<code>80</code>万乘<code>80</code>万的稠密矩阵。</p>
<p><code>John S. Breese</code>中提出了一个称为<code>IUF(Inverse User Frequence)</code>，即<strong>用户活跃度</strong>对数的倒数的参数，他也认为活跃用户对物品相似度的贡献应该小于不活跃的用户，他提出应该增加<code>IUF</code>参数来修正物品相似度的计算公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210945142.png" alt=""></p>
<p>上述公式对活跃用户做了一种软性的惩罚，但是对于很多过于活跃的用户，比如上面那位买了当当网<code>80%</code>图书的用户，为了避免相似度矩阵过于稠密，我们在实际计算中一般直接忽略他的兴趣列表，而不将其纳入到相似度计算的数据集中。</p>
<p><strong>相似度矩阵归一化处理</strong></p>
<p><code>Karypis</code>在研究中发现如果将<code>ItemCF</code>的相似度矩阵按最大值归一化，可以提高推荐的准确率。其研究表明，如果已经得到了物品相似度矩阵 $w$ ，那么可以用如下公式得到归一化之后的相似度矩阵 $w^{\prime}$ ：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210948508.png" alt=""></p>
<p>归一化的好处不仅仅在于增加推荐的准确度，它还可以提高推荐的覆盖率和多样性。</p>
<p><strong>实例讲解</strong></p>
<p>下表是一个简易的原始数据集，也称之为<code>User-Item</code>表，即用户-物品列表，记录了每个用户喜爱的物品，数据表格如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>用户</th>
<th>喜爱的物品</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>{a,b,d}</td>
</tr>
<tr>
<td>B</td>
<td>{b,c,e}</td>
</tr>
<tr>
<td>C</td>
<td>{c,d}</td>
</tr>
<tr>
<td>D</td>
<td>{b,c,d}</td>
</tr>
<tr>
<td>E</td>
<td>{a,d}</td>
</tr>
</tbody>
</table>
</div>
<p>接着，我们分别建立用户<code>A</code>-<code>E</code>的共现矩阵，并将他们累加起来。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202254827.png" alt=""></p>
<p>接下来我们计算最终的物品相似度矩阵，以物品<code>a</code>和物品<code>b</code>的相似度计算为例，通过上面计算的计算可知 $C[a][b]=1$ ，即同时喜欢物品<code>a</code>和物品<code>b</code>的用户有一位。根据<code>User-Item</code>表可以统计出 $N(a)=2$ ， $N(b)=3$ ，那么物品<code>a</code>和物品<code>b</code>的相似度 $w_{ab}$ 计算如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210936324.png" alt=""></p>
<h5 id="结果预测"><a href="#结果预测" class="headerlink" title="结果预测"></a>结果预测</h5><p>在得到物品之间的相似度后，<code>ItemCF</code>通过如下公式计算用户 $u$ 对一个物品 $j$ 的兴趣：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210950233.png" alt=""></p>
<p>这里 $N(u)$ 是用户喜欢的物品的集合， $S(j,K)$ 是和物品 $j$ 最相似的 $K$个物品的集合， $w_{ji}$ 是物品 $j$ 和 $i$ 的相似度， $r_{ui}$ 是用户 $u$ 对物品 $i$ 的兴趣。（对于隐反馈数据集，如果用户 $u$ 对物品 $i$ 有过行为，即可令 $r_{ui}=1$ 。）该公式的含义是，和用户历史上感兴趣的物品越相似的物品，越有可能在用户的推荐列表中获得比较高的排名。</p>
<p><strong>实例讲解</strong></p>
<p>下图是一个基于物品推荐的简单例子。该例子中，用户喜欢《C++ Primer中文版》和《编程之美》两本书。然后<code>ItemCF</code>会为这两本书分别找出和它们最相似的 $3$ 本书，然后根据公式的定义计算用户对每本书的感兴趣程度。比如，<code>ItemCF</code>给用户推荐《算法导论》，是因为这本书和《C++Primer中文版》相似，相似度为 $0.4$ ，而且这本书也和《编程之美》相似，相似度是 $0.5$ 。考虑到用户对《C++ Primer中文版》的兴趣度是 $1.3$ ，对《编程之美》的兴趣度是 $0.9$ ，那么用户对《算法导论》的兴趣度就是 $1.3 \times 0.4 + 0.9\times0.5 = 0.97$ 。<br><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210958573.png" alt=""></p>
<h4 id="基于用户的协同过滤算法"><a href="#基于用户的协同过滤算法" class="headerlink" title="基于用户的协同过滤算法"></a>基于用户的协同过滤算法</h4><p><strong>基于用户的协同过滤(UserCF)</strong>，思想其实比较简单，当一个用户<code>A</code>需要个性化推荐的时候， 我们可以先找到和他有相似兴趣的其他用户， 然后把那些用户喜欢的， 而用户<code>A</code>没有听说过的物品推荐给<code>A</code>。</p>
<h5 id="算法步骤-1"><a href="#算法步骤-1" class="headerlink" title="算法步骤"></a>算法步骤</h5><p>基于用户的协同过滤算法分为两步骤：</p>
<ul>
<li>找到与当前用户<code>A</code>相似的用户<code>B</code>；</li>
<li>将相似用户<code>B</code>喜欢的物品而用户<code>A</code>没有见过的物品推荐给用户<code>A</code>。</li>
</ul>
<h5 id="相似度计算-1"><a href="#相似度计算-1" class="headerlink" title="相似度计算"></a>相似度计算</h5><p><strong>欧式距离</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202054636.png" alt=""></p>
<p><strong>曼哈顿距离</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202055336.png" alt=""></p>
<p><strong>杰卡德(Jaccard)相似系数</strong></p>
<p>两个集合<code>A</code>和<code>B</code>交集元素的个数在<code>A</code>、<code>B</code>并集中所占的比例，称为这两个集合的杰卡德系数，用符号<code>J(A,B)</code>表示。杰卡德相似系数是衡量两个集合相似度的一种指标（余弦距离也可以用来衡量两个集合的相似度），<code>Jaccard</code>值越大说明相似度越高。由于杰卡德相似系数一般无法反映具体用户的评分喜好信息，所以常用来<strong>评估用户是否会对某物品进行打分， 而不是预估用户会对某物品打多少分</strong>。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202037061.png" alt=""></p>
<p><strong>余弦相似度</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202037832.png" alt=""></p>
<p><strong>皮尔逊相关系数</strong></p>
<p>相比余弦相似度，皮尔逊相关系数通过<strong>使用用户平均分对各独立评分进行修正，减小了用户评分偏置的影响</strong>。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202048519.png" alt=""></p>
<p>其中 $R_{i,p}$ 表示用户 $i$ 对物品 $p$ 的评分， $\overline{R}_{i}$ 表示用户 $i$ 对所有物品评分的平均值， $P$ 代表所有物品的集合。</p>
<h5 id="结果预测-1"><a href="#结果预测-1" class="headerlink" title="结果预测"></a>结果预测</h5><p>根据上面的几种方法， 我们可以计算出向量之间的相似程度， 也就是可以计算出<code>Alice</code>和其他用户的相近程度， 这时候我们就可以选出与<code>Alice</code>最相近的前 $n$ 个用户， 基于他们对某一物品的评价猜测出<code>Alice</code>的打分值。</p>
<p>这里最常用的方式是利用用户相似度和相似用户的评价的加权平均获得目标用户的评价预测。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202104133.png" alt=""></p>
<p>其中 $w_{u,s}$ 是用户 $u$ 与用户 $s$ 的相似度， $R_{s,p}$ 是用户 $s$ 对物品 $p$ 的评分， $S$ 是所有用户的集合。</p>
<p>还有一种方式打分方式， 这种方式考虑的更加全面， 依然是用户相似度作为权值， 但后面不单纯的是其他用户对物品的评分， 而是该物品的评分与此用户的所有评分的差值进行加权平均， 这时候考虑到了有的用户内心的评分标准不一的情况， 即有的用户喜欢打高分， 有的用户喜欢打低分的情况。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202135669.png" alt=""></p>
<p>其中， $w_{u,s}$ 是用户 $u$ 与用户 $s$ 的相似度， $R_{s,p}$ 是用户 $s$ 对物品 $p$ 的评分， $\overline{R}_{u}$ 是用户 $u$ 对所有物品打分的平均值， $\overline{R}_{s}$ 是用户 $s$ 对所有物品打分的平均值，其实就是对真实值与平均值的误差进行重要性加权。</p>
<h4 id="Swing-Graph-based"><a href="#Swing-Graph-based" class="headerlink" title="Swing(Graph-based)"></a>Swing(Graph-based)</h4><h5 id="前述方法局限性"><a href="#前述方法局限性" class="headerlink" title="前述方法局限性"></a>前述方法局限性</h5><ul>
<li>基于 Cosine, Jaccard, 皮尔逊相关性等相似度计算的协同过滤算法，在计算邻居关联强度的时候只关注于 Item-based (常用，因为item相比于用户变化的慢，且新Item特征比较容易获得)，Item-based CF 只关注于 Item-User-Item 的路径，把所有的User-Item交互都平等得看待，从而忽视了 User-Item 交互中的大量噪声，推荐精度存在局限性。</li>
<li>对互补性产品的建模不足，可能会导致用户购买过手机之后还继续推荐手机，但用户短时间内不会再继续购买手机，因此产生无效曝光。</li>
</ul>
<h5 id="Swing算法"><a href="#Swing算法" class="headerlink" title="Swing算法"></a>Swing算法</h5><p>Swing 通过利用 User-Item-User 路径中所包含的信息，考虑 User-Item 二部图中的鲁棒内部子结构计算相似性。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192157333.png" alt=""></p>
<ul>
<li>什么是内部子结构？ 以经典的啤酒尿布故事为例，张三同时购买了啤酒和尿布，这可能是一种巧合。但两个甚至多个顾客都同时购买了啤酒尿布，这就证明啤酒和尿布具有相关关系。这样共同购买啤酒和尿布的用户越多，啤酒和尿布的相关度就会越高。</li>
<li>通俗解释：若用户 $u$ 和用户 $v$ 之间除了购买过 $i$ 外，还购买过商品 $j$ ，则认为两件商品是具有某种程度上的相似的。也就是说，商品与商品之间的相似关系，是通过用户关系来传递的。为了衡量物品 $i$ 和 $j$ 的相似性，比较同时购买了物品 $i$ 和 $j$ 的用户 $u$ 和用户 $v$ ， 如果这两个用户共同购买的物品越少，即这两个用户原始兴趣不相似，但仍同时购买了两个相同的物品 $i$ 和 $j$ ， 则物品 $i$ 和 $j$ 的相似性越高。</li>
<li>计算公式如下，其中 $U_{i}$ 是点击过商品 $i$ 的用户集合，$I_{u}$ 是用户 $u$ 点击过的商品集合， $\alpha$ 是平滑系数。 $w_{u}=\frac{1}{\sqrt{\vert I_{u} \vert}}$ ，$w_{v}=\frac{1}{\sqrt{\vert I_{v} \vert}}$  是用户权重参数，来降低活跃用户的影响。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192205012.png" alt=""></p>
<h5 id="Surprise算法"><a href="#Surprise算法" class="headerlink" title="Surprise算法"></a>Surprise算法</h5><p>Surprise 算法利用商品分类信息和用户共同购买图上的聚类技术来建模产品之间的组合关系。</p>
<p>首先在行为相关性中引入连续时间衰减因子，然后引入基于交互数据的聚类方法解决数据稀疏的问题，旨在帮助用户找到互补商品。互补相关性主要从三个层面考虑，类别层面，商品层面和聚类层面。</p>
<ul>
<li>类别层面 首先通过商品和类别的映射关系，我们可以得到 user-category 矩阵。随后使用简单的相关性度量可以计算出类别 $i$ ， $j$ 的相关性。 $N(c_{i},j)$ 为在购买过 $i$ 之后购买 $j$ 类的数量， $N(c_{j})$ 为购买 $j$ 类的数量。（）</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192210225.png" alt=""></p>
<ul>
<li><p>商品层面 商品层面的相关性挖掘主要有两个关键设计：</p>
<ul>
<li>商品的购买顺序是需要被考虑的，例如在用户购买手机后推荐充电宝是合理的，但在用户购买充电宝后推荐手机是不合理的。</li>
<li>两个商品购买的时间间隔也是需要被考虑的，时间间隔越短越能证明两个商品的互补关系。</li>
</ul>
<p>最终商品层面的互补相关性被定义为如下，其中 $j$ 属于 $i$ 的相关类，且 $j$ 的购买时间晚于 $i$ 。</p>
</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305192217890.png" alt=""></p>
<ul>
<li>聚类层面<ul>
<li>如何聚类？传统的聚类算法（基于密度和k-means）在数十亿产品规模下的淘宝场景中不可行，所以作者采用了标签传播算法。</li>
<li>在哪里标签传播？Item-item 图，其中把Swing计算的排名靠前item为邻居，边的权重就是Swing分数。</li>
<li>表现如何？快速而有效，15分钟即可对数十亿个项目进行聚类。最终聚类层面的相关度计算同上面商品层面的计算公式。</li>
</ul>
</li>
<li>线性组合： $s(i,j)=\omega\ast s_{1}(i,j)+(1−\omega)\ast s_{2}(i,j)$ ，其中是 $\omega=0.8$ 作者设置的权重超参数。Surprise算法通过利用类别信息和标签传播技术解决了用户共同购买图上的稀疏性问题。</li>
</ul>
<h4 id="基于模型的协同过滤算法"><a href="#基于模型的协同过滤算法" class="headerlink" title="基于模型的协同过滤算法"></a>基于模型的协同过滤算法</h4><p>基于模型的协同过滤作为目前最主流的协同过滤类型，我们的问题是这样的： $m$ 个物品， $n$ 个用户的数据，只有部分用户和部分数据之间是有评分数据的，其它部分评分是空白。此时我们要用已有的部分稀疏数据来预测那些空白的物品和数据之间的评分关系，找到最高评分的物品推荐给用户。</p>
<p>对于这个问题，用机器学习的思想来建模解决，主流的方法可以分为：用关联算法，聚类算法，分类算法，回归算法，矩阵分解，神经网络，图模型以及隐语义模型来解决。</p>
<p><strong>用关联算法做协同过滤</strong></p>
<p>一般我们可以找出用户购买的所有物品数据里频繁出现的项集活序列，来做频繁集挖掘，找到满足支持度值的关联物品的频繁 $N$ 项集或者序列。如果用户购买了频繁 $N$ 项集或者序列里的部分物品，那么我们可以将频繁项集或序列里的其他物品按一定的评分准则推荐给用户，这个评分准则可以包括支持度，置信度和提升度等。</p>
<p><strong>用聚类算法做协同过滤</strong></p>
<p>用聚类算法做协同过滤就和前面的基于用户或者项目的协同过滤有些类似了。我们可以按照用户或者按照物品基于一定的距离度量来进行聚类。如果基于用户聚类，则可以将用户按照一定距离度量方式分成不同的目标人群，将同样目标人群评分高的物品推荐给目标用户。基于物品聚类的话，则是将用户评分高物品的相似同类物品推荐给用户。</p>
<p><strong>用分类算法做协同过滤</strong></p>
<p>如果我们根据用户评分的高低，将分数分成几段的话，则这个问题变成分类问题。比如最直接的，设置一份评分阈值，评分高于阈值的就是推荐，评分低于阈值就是不推荐，我们将问题变成了一个二分类问题。虽然分类问题的算法多如牛毛，但是目前使用最广泛的是逻辑回归。为啥是逻辑回归而不是看起来更加高大上的比如支持向量机呢？因为逻辑回归的解释性比较强，每个物品是否推荐我们都有一个明确的概率放在这，同时可以对数据的特征做工程化，得到调优的目的。</p>
<p><strong>用回归算法做协同过滤</strong></p>
<p>用回归算法做协同过滤比分类算法看起来更加的自然。我们的评分可以是一个连续的值而不是离散的值，通过回归模型我们可以得到目标用户对某商品的预测打分。</p>
<p><strong>用神经网络做协同过滤</strong></p>
<p>用神经网络乃至深度学习做协同过滤应该是以后的一个趋势。目前比较主流的用两层神经网络来做推荐算法的是限制玻尔兹曼机<code>(RBM)</code>。在目前的<code>Netflix</code>算法比赛中，<code>RBM</code>算法的表现很牛。当然如果用深层的神经网络来做协同过滤应该会更好，大厂商用深度学习的方法来做协同过滤应该是将来的一个趋势。</p>
<p><strong>用图模型做协同过滤</strong></p>
<p>用图模型做协同过滤，则将用户之间的相似度放到了一个图模型里面去考虑，常用的算法是<code>SimRank</code>系列算法和马尔科夫模型算法。对于<code>SimRank</code>系列算法，它的基本思想是被相似对象引用的两个对象也具有相似性。算法思想有点类似于大名鼎鼎的<code>PageRank</code>。而马尔科夫模型算法当然是基于马尔科夫链了，它的基本思想是基于传导性来找出普通距离度量算法难以找出的相似性。</p>
<p><strong>用隐语义模型做协同过滤</strong></p>
<p>隐语义模型主要是基于<code>NLP</code>的，涉及到对用户行为的语义分析来做评分推荐，主要方法有隐性语义分析<code>LSA</code>和隐含狄利克雷分布<code>LDA</code>。</p>
<p><strong>用矩阵分解做协同过滤</strong></p>
<p>用矩阵分解做协同过滤是目前使用也很广泛的一种方法。由于传统的奇异值分解<code>SVD</code>要求矩阵不能有缺失数据，必须是稠密的，而我们的用户物品评分矩阵是一个很典型的稀疏矩阵，直接使用传统的<code>SVD</code>到协同过滤是比较复杂的。目前主流的矩阵分解推荐算法主要是<code>SVD</code>的一些变种，比如<code>FunkSVD</code>，<code>BiasSVD</code>和<code>SVD++</code>。</p>
<h5 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h5><p>  矩阵分解是指将一个矩阵分解成两个或者多个矩阵的乘积，实际推荐计算时不再使用大矩阵，而是用分解得到的两个小矩阵：一个是由代表用户偏好的用户隐因子向量组成，另一个是由代表物品语义主题的隐因子向量组成。</p>
<p>对于下图的<code>User-Item</code>矩阵（评分矩阵），记为 $R_{n \times m}$ 。可以将其分解成两个或者多个矩阵的乘积，假设分解成两个矩阵 $U_{n \times k}$ 和 $V_{k \times m}$ ，我们要使得矩阵 $U_{n \times k}$ 和 $V_{k \times m}$ 的乘积能够还原原始的矩阵 $R$ ，即 $R_{n \times m}=U_{n \times k} \ast V_{k \times m}$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231131422.png" alt=""></p>
<p>在我们得到用户对每个标的物的评分后，从该评分中过滤掉用户已经操作过的标的物，针对剩下的标的物得分做降序排列取<code>topN</code>推荐给用户。矩阵分解算法的核心思想是将用户行为矩阵分解为两个低秩矩阵的乘积，通过分解，我们分别将用户和标的物嵌入到了同一个 $k$ 维的向量空间（ $k$ 一般很小，几十到上百），用户向量和标的物向量的内积代表了用户对标的物的偏好度。所以，矩阵分解算法本质上也是一种<strong>嵌入方法</strong>。上面提到的k维向量空间的每一个维度是<strong>隐因子(latent factor)</strong>，之所以叫隐因子，是因为每个维度不具备与现实场景对应的具体的可解释的含义，所以矩阵分解算法也是一类隐因子算法。这 $k$ 个维度代表的是某种行为特性，但是这个行为特性又是无法用具体的特征解释的，从这点也可以看出，矩阵分解算法的可解释性不强，我们比较难以解释矩阵分解算法为什么这么推荐。</p>
<h6 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h6><p>通过矩阵分解将用户 $u$ 和标的物 $v$ 嵌入如下的 $k$ 维隐式特征空间向量：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231137649.png" alt=""></p>
<p>那么用户 $u$ 对标的物 $v$ 的预测评分为 $\hat{r_{uv}}=p_{u} \ast q_{v}^{T}$ ，真实值与预测值之间的误差为 $\Delta r=r_{uv}-\hat{r_{uv}}$ 。如果预测得越准，那么 $| \Delta r |$ 越小，针对所有用户评分过的 $(u,v)$ 对，如果我们可以保证这些误差之和尽量小，那么有理由认为我们的预测是精准的。有了上面的分析，我们就可以将矩阵分解转化为一个机器学习问题，这也就是<strong>Funk-SVD(Basic SVD/LFM)</strong>的思想。在Funk-SVD的基础之上加上正则化项，也就成了<strong>RSVD</strong>。具体地说，我们可以将矩阵分解转化为如下等价的求最小值的最优化问题。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231146383.png" alt=""></p>
<p>其中 $\lambda$ 是超参数， $| p_{u} |^{2}+| q_{v} |^{2}$ 是正则化项。</p>
<h6 id="求解方法"><a href="#求解方法" class="headerlink" title="求解方法"></a>求解方法</h6><p>对于上一节讲到的最优化问题，在工程上一般有两种求解方法，<code>SGD(Stochastic Gradient Descent)</code>和<code>ALS(Alternating Least Squares)</code>。</p>
<ul>
<li>利用SGD来求解矩阵分解</li>
</ul>
<p>我们定义真实评分和预测评分的误差为： $e_{uv}=r_{uv}-p_{u} \ast q_{v}$ ，我们可以将上面的优化问题改写成如下函数：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231152053.png" alt=""></p>
<p>对 $p_u$ 和 $q_{v}$ 求偏导数，我们可以得到：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231153530.png" alt=""></p>
<p>有了偏导数，我们沿着导数(梯度)相反的方向更新：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231154583.png" alt=""></p>
<ul>
<li>利用ALS来求解矩阵分解</li>
</ul>
<p><code>ALS</code>方法是一个高效的求解矩阵分解的算法，目前<code>Spark Mllib</code>中的协同过滤算法就是基于<code>ALS</code>求解的矩阵分解算法，它可以很好地拓展到分布式计算场景，轻松应对大规模训练数据的情况。下面对ALS算法原理及特点做一个简单介绍。<code>ALS</code>算法的原理基本就是名字表达的意思，通过交替优化求得极值。</p>
<p>一般过程是先固定 $p_{u}$ ，那么上述优化问题就变成了一个关于 $q_{v}$ 的二次函数，可以作为最小二乘问题来解决，求出最优的 $q^{\ast}_{v}$ 后，固定 $q^{\ast}_{v}$ ，再解关于 $p_{u}$ 的最小二乘问题，交替进行直到收敛。</p>
<p><code>ALS</code>算法有如下两个优势：</p>
<ul>
<li><p><strong>可以并行处理</strong></p>
<p>当固定某一个参数时，另一个参数的迭代更新只依赖自己，不依赖于其他的标的物的特征向量，所以可以将不同的参数更新放到不同的服务器上执行。<code>Spark</code>的<code>ALS</code>算法就是采用这样的方式做到并行化的。</p>
</li>
<li><p><strong>对于隐式特征问题比较合适</strong></p>
<p>用户真正的评分是很稀少的，所以利用隐式行为是更好的选择（其实也是不得已的选择）。当利用了隐式行为，那么用户行为矩阵就不会那么稀疏了，即有非常多的 $(u,v)$ 对是非空的，计算量会更大，这时采用<code>ALS</code>算法是更合适的，因为固定 $p_{u}$ 或者 $q_{v}$ ，让整个计算问题更加简单，容易求目标函数的极值。</p>
</li>
</ul>
<h6 id="矩阵分解推荐算法的拓展与优化"><a href="#矩阵分解推荐算法的拓展与优化" class="headerlink" title="矩阵分解推荐算法的拓展与优化"></a>矩阵分解推荐算法的拓展与优化</h6><p>矩阵分解算法是一个非常容易理解并易于分布式实现的算法。不光如此，矩阵分解算法的框架还是一个非常容易拓展的框架，可以整合非常多的其他信息及特性到该框架之下，从而丰富模型的表达空间，提升预测的准确度。本节我们就来总结和梳理一下矩阵分解算法可以进行哪些拓展与优化。</p>
<p><strong>整合偏差(bias)项</strong></p>
<p>不同的人对标的物的评价可能是不一样的，有的人倾向于给更高的评分，而有的人倾向于给更低的评分。对于同一个标的物，也会受到外界其他信息的干扰，影响人们对它的评价（比如视频，可能由于主演的热点事件导致该视频突然变火），这两种情况是由于用户和标的物引起的偏差。我们可以在这里引入<code>Bias</code>项，将评分表中观察到的值分解为 $4$ 个部分：全局均值<code>(global average)</code>，标的物偏差<code>(item bias)</code>，用户偏差<code>(user bias)</code>和用户标的物交叉项<code>(user-item interaction)</code>。这时，我们可以用如下公式来预测用户 $u$ 对标的物 $v$ 的评分：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231206104.png" alt=""></p>
<p><strong>增加更多的用户信息输入</strong></p>
<p>由于用户一般只对很少的标的物评分，导致评分过少，可能无法给该用户做出较好的推荐，这时可以通过引入更多的信息来缓解评分过少的问题。具体来说，我们可以整合用户隐式反馈（收藏、点赞、分享等）和用户人口统计学信息（年龄、性别、地域、收入等）到矩阵分解模型中。</p>
<p><strong>整合时间因素</strong></p>
<p>到目前为止，我们的模型都是静态的。实际上，用户的偏好、用户对标的物的评分趋势、以及标的物的受欢迎程度都是随着时间变化的。拿电影来说，用户可能原来喜欢爱情类的电影，后面可能会转而喜欢科幻喜剧类电影，所以我们用包含时间的 $p_{u}(t)$ 来表示用户的偏好特性向量。用户开始对某个视频偏向于打高分，经过一段时间后，用户看的电影多了起来，用户的审美越来越挑剔，所以一般不会再对一个电影打很高的分数了，除非他觉得真的特别好，因此，我们可以用包含时间的 $b_{u}(t)$ 来表示用户的偏差随着时间而变化。对于标的物偏差也一样，一个电影可能开始不是很火，但是如果它的主演后面演了一部非常火的电影，也会将原来的电影热度带到一个新的高度。比如，前两年比较火的李现演的《亲爱的，热爱的》，导致李现人气高涨，他原来演的《南方有乔木》的百度搜索指数在《亲爱的，热爱的》播出期间高涨。因此，我们可以用包含时间的 $b_{v}(t)$ 来表示标的物偏差随着时间的变化而变化的趋势。标的物本身的特征 $q_{v}$ ，我们可以认为是稳定的，它代表的是标的物本身的固有属性或者品质，所以不会随着时间而变化。</p>
<p>基于上面的分析，我们最终的预测用户评分的公式整合时间因素后可以表达为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231212913.png" alt=""></p>
<p><strong>整合用户对评分的置信度</strong></p>
<p>一般来说，用户对不同标的物的评分不是完全一样可信的，可能会受到外界其他因素的影响，比如某个视频播出后，主播发生了热点事件，肯定会影响用户对该视频的评价，节假日，特殊事件也会影响用户的评价。对于隐式反馈，一般我们用 $0$ 和 $1$ 来表示用户是否喜欢该标的物，多少有点绝对，更好的方式是引入一个喜欢的概率/置信度，用户对该标的物操作次数越多、时间越长、付出越大，相应的置信度也越大。因此，我们可以在用户对标的物的评分中增加一个置信度的因子 $c_{uv}$ ，那么最终的优化公式就变为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231213243.png" alt=""></p>
<p><strong>隐式反馈</strong></p>
<p>用二元变量 $p_{uv}$ 表示用户 $u$ 对标的物的偏好， $p_{uv}=1$ 表示用户 $u$ 对标的物 $v$ 有兴趣， $p_{uv}=0$ 表示对标的物 $v$ 无兴趣。 $r_{uv}$ 是用户 $u$ 对标的物的隐式反馈，如观看视频的时长，点击次数等等。 $r_{uv}$ 和 $p_{uv}$ 的关系见下面公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231220341.png" alt=""></p>
<p>$r_{uv}$ 越大，有理由认为用户对标的物兴趣的置信度越高，比如一个文章读者看了好几篇，肯定比看一遍更能反映出读者对这篇文章的喜爱。具体可以用下面的公式来衡量用户 $u$ 对标的物 $v$ 的置信度，其中 $\alpha$ 是一个超参数，作者建议取 $\alpha=40$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231221419.png" alt=""></p>
<p>于是我们可以定义如下公式</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231225576.png" alt=""></p>
<p>将用户的操作 $r_{uv}$ 分解为置信度 $c_{uv}$ 和偏好 $p_{uv}$ 能够更好地反映隐式行为的特征，并且从实践上可以大幅提升预测的准确度。同时，通过该分解，利用代数上的一些技巧及该模型的巧妙设计，该算法的时间复杂度与用户操作行为总次数线性相关，不依赖于用户数和标的物数，因此非常容易并行化。</p>
<p>隐式反馈也有一些缺点，不像明确的用户评分，无法很好地表达负向反馈，用户购买一个物品可能是作为礼物送给别人的，他自己可能不喜欢这个物品，用户观看了某个视频，有可能是产品进入视频详情页时是自动起播的，这些行为是包含很多噪音的。</p>
<p><strong>整合用户和标的物metadata信息</strong></p>
<p>利用特征的嵌入向量之和来表示用户或者标的物向量，这就很好地将<code>metadata</code>信息整合到了用户和标的物向量中了，再利用用户向量 $p_{u}$  和标的物向量 $q_{i}$ 的内积加上<code>bias</code>项，通过一个<code>logistic</code>函数来获得用户 $u$ 对标的物 $i$ 的偏好概率/得分，从这里的介绍可以看到，该模型很好地将矩阵分解和<code>metadata</code>信息整合到了一个框架之下。</p>
<h3 id="基于向量的召回"><a href="#基于向量的召回" class="headerlink" title="基于向量的召回"></a>基于向量的召回</h3><h4 id="FM召回"><a href="#FM召回" class="headerlink" title="FM召回"></a>FM召回</h4><p>FM用于排序详细介绍见后文经典排序模型。由于需要将 FM 模型用在召回，故将二阶特征交互项拆分为用户和物品项。有：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202004566.png" alt=""></p>
<p>在比较用户与不同物品之间的匹配分时，只需要比较：（1）物品内部之间的特征交互得分；（2）用户和物品之间的特征交互得分。因此合并 FM 的一阶、二阶特征交互项，得到基于 FM 召回的匹配分计算公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202035154.png" alt=""></p>
<h4 id="item2vec召回系列"><a href="#item2vec召回系列" class="headerlink" title="item2vec召回系列"></a>item2vec召回系列</h4><h5 id="Word2vec基础"><a href="#Word2vec基础" class="headerlink" title="Word2vec基础"></a>Word2vec基础</h5><p>Word2vec(Mikolov et al. 2013)是一个用来学习dense word vector的算法：</p>
<ol>
<li>我们使用<strong>大量的文本语料库</strong></li>
<li>词汇表中的每个单词都由一个<strong>词向量dense word vector</strong>表示</li>
<li>遍历文本中的每个位置 t，都有一个<strong>中心词 c（center） 和上下文词 o（“outside”）</strong>，如图1中的banking</li>
<li>在整个语料库上使用数学方法<strong>最大化单词o在单词c周围出现了这一事实</strong>，从而得到单词表中每一个单词的dense vector</li>
<li>不断调整词向量dense word vector以达到最好的效果</li>
</ol>
<p>Word2vec包含两个模型，<strong>Skip-gram与CBOW</strong>。我们希望<strong>最大化单词o在单词c周围出现了这一事实</strong>，而我们需要用数学语言表示“单词o在单词c周围出现了”这一事件，如此才能进行词向量的不断调整。很自然地，我们需要<strong>使用概率工具描述事件的发生</strong>，我们想到用条件概率 $P(o\vert c)$ 表示“给定中心词c,它的上下文词o在它周围出现了”。</p>
<p>下图展示了以“into”为中心词，窗口大小为2的情况下它的上下文词。以及相对应的 $P(o\vert c)$</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202054590.png" alt=""></p>
<p>我们滑动窗口，再以banking为中心词</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202054939.png" alt=""></p>
<p>那么，如果我们在整个语料库上不断地滑动窗口，我们可以得到所有位置的 $P(o\vert c)$ ，我们希望在所有位置上<strong>最大化单词o在单词c周围出现了这一事实</strong>，由极大似然法可得：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202058666.png" alt=""></p>
<p>此式还可以写为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202059177.png" alt=""></p>
<p>加log，加负号，缩放大小可得：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202100516.png" alt=""></p>
<p>上式即为<strong>skip-gram的损失函数</strong>，最小化损失函数，就可以得到合适的词向量</p>
<p>得到上式后，产生了两个问题：</p>
<ol>
<li>$P(o\vert c)$ 怎么表示？</li>
<li>为何最小化损失函数能够得到良好表示的词向量dense word vector？</li>
</ol>
<p>回答1：我们使用<strong>中心词c和上下文词o的相似性</strong>来计算 $P(o\vert c)$ ，更具体地，相似性由<strong>词向量的点积</strong>表示： $u_{o}\cdot v_{c}$ 。使用词向量的点积表示 $P(o\vert c)$ 的原因：（1）计算简单（1）出现在一起的词向量意义相关，则希望它们相似。又因为 $P(o\vert c)$ 是一个概率，所以我们在整个语料库上使用<strong>softmax</strong>将点积的值映射到概率：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202104262.png" alt=""></p>
<p>注：注意到上图，中心词词向量为 $v_{c}$ ，而上下文词词向量为 $u_{o}$ 。也就是说每个词会对应两个词向量，<strong>在词w做中心词时</strong>，使用 $v_{w}$ 作为词向量，而在它做上下文词时，使用 $u_{w}$ 作为词向量。这样做的原因是为了求导等操作时计算上的简便。当整个模型训练完成后，我们既可以使用 $v_{w}$ 作为词 $w$ 的词向量，也可以使用 $u_{w}$ 作为词 $w$ 的词向量，亦或是将二者平均。在下一部分的模型结构中，我们将更清楚地看到两个词向量究竟在模型的哪个位置。</p>
<p>回答2：由上文所述， $P(o\vert c)=softmax(u_{o}^{T}\cdot v_{c})$ 。所以损失函数是关于 $u_{o}$ 和 $v_{c}$ 的函数，我们通过梯度下降法调整 $u_{o}$ 和 $v_{c}$ 的值，最小化损失函数，即得到了良好表示的词向量。</p>
<h5 id="Word2vec模型结构"><a href="#Word2vec模型结构" class="headerlink" title="Word2vec模型结构"></a>Word2vec模型结构</h5><p>这是一个输入为 $1 \times V$ 维的one-hot向量（V为整个词汇表的长度，这个向量只有一个1值，其余为0值表示一个词），单隐藏层（<strong>隐藏层的维度为N，这里是一个超参数，这个参数由我们定义，也就是词向量的维度</strong>），输出为 $1 \times V$ 维的softmax层的模型。</p>
<p>$W^{I}$为 $V \times N$ 的参数矩阵，$W^{O}$为 $N \times V$ 的参数矩阵。</p>
<p>模型的输入为 $1 \times V$形状的one-hot向量（V为整个词汇表的长度，这个向量只有一个1值，其余为0值表示一个词）。隐藏层的维度为N，这里是一个超参数，这个参数由我们定义，也就是词向量的维度。$W^{I}$为 $V \times N$ 的参数矩阵。</p>
<p>我们这里，考虑Skip-gram算法，输入为中心词c的one-hot表示</p>
<p>由输入层到隐藏层，根据矩阵乘法规则，可知，<strong>$W^{I}$的每一行即为词汇表中的每一个单词的词向量v</strong>，$1 \times V$ 的 inputs 乘上 $V \times N$ 的$W^{I}$，隐藏层即为 $1 \times N$ 维的 $v_{c}$ 。</p>
<p>而$W^{O}$中的每一列即为词汇表中的每一个单词的词向量u。根据乘法规则，$1 \times N$ 的隐藏层乘上 $N \times V$ 的$W^{O}$参数矩阵，得到的 $1 \times V$ 的输出层的每一个值即为$u_{w^T} \cdot v_c$ ，加上softmax变化即为$P(w|c)$ ，其中 $W^{o}$ 的每一列就是周围词向量 $u_{o}$ 。</p>
<p>有V个w,其中的P(o|c)即实际样本中的上下文词的概率，为我们最为关注的值。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202111969.png" alt=""></p>
<p>如上文所述，Skip-gram为给定中心词，预测周围的词，即求 $P(o\vert c)$ ，如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202113543.png" alt=""></p>
<p>而CBOW为给定周围的词，预测中心词，即求 $P(c\vert o)$ ,如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202113829.png" alt=""></p>
<h5 id="item2Vec模型"><a href="#item2Vec模型" class="headerlink" title="item2Vec模型"></a>item2Vec模型</h5><p>Item2Vec 的原理十分简单，它是基于 Skip-Gram 模型的物品向量训练方法。但又存在一些区别，如下：</p>
<ul>
<li>词向量的训练是基于句子序列（sequence），但是物品向量的训练是基于物品集合（set）。</li>
<li>因此，物品向量的训练丢弃了空间、时间信息。</li>
</ul>
<p>Item2Vec 论文假设对于一个集合的物品，它们之间是相似的，与用户购买它们的顺序、时间无关。当然，该假设在其他场景下不一定使用，但是原论文只讨论了该场景下它们实验的有效性。由于忽略了空间信息，原文将共享同一集合的每对物品视为正样本。目标函数如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202119928.png" alt=""></p>
<p>在 Skip-Gram 模型中，提到过每个单词 $w_{i}$ 有2个特征表示。在 Item2Vec 中同样如此，论文中是将物品的中心词向量 $u_{i}$ 作为物品的特征向量。作者还提到了其他两种方式来表示物品向量：</p>
<ul>
<li><strong>add</strong>：$u_{i}+v_{i}$</li>
<li><strong>concat</strong>：$[u_{i}^{T}v_{i}^{T}]^{T}$</li>
</ul>
<h5 id="Airbnb召回"><a href="#Airbnb召回" class="headerlink" title="Airbnb召回"></a>Airbnb召回</h5><p>Airbnb 描述了两种 Embedding 的构建方法，分别为：</p>
<ul>
<li>用于描述短期实时性的个性化特征 Embedding：<strong>listing Embeddings</strong></li>
<li>用于描述长期的个性化特征 Embedding：<strong>user-type &amp; listing type Embeddings</strong></li>
</ul>
<h4 id="YouTubeDNN召回"><a href="#YouTubeDNN召回" class="headerlink" title="YouTubeDNN召回"></a>YouTubeDNN召回</h4><p>召回模型的结构如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202146491.png" alt=""></p>
<p>它的输入主要是用户侧的特征，包括用户观看的历史video序列， 用户搜索的历史tokens， 然后就是用户的人文特征，比如地理位置， 性别，年龄这些。</p>
<ul>
<li><p>用户历史序列，历史搜索tokens这种序列性的特征: 一般长这样<code>[item_id5, item_id2, item_id3, ...]</code>， 这种id特征是高维稀疏，首先会通过一个embedding层，转成低维稠密的embedding特征，即历史序列里面的每个id都会对应一个embedding向量，这样历史序列就变成了多个embedding向量的形式， 这些向量一般会进行融合，常见的是average pooling，即每一维求平均得到一个最终向量来表示用户的历史兴趣或搜索兴趣。论文里面使用了用户最近的50次观看历史，用户最近50次搜索历史token，embedding维度是256维， 采用的average pooling。  当然，这里还可以把item的类别信息也隐射到embedding，与前面的concat起来。</p>
</li>
<li><p>用户人文特征， 这种特征处理方式就是离散型的依然是labelEncoder，然后embedding转成低维稠密， 而连续型特征，一般是先归一化操作，然后直接输入，当然有的也通过分桶，转成离散特征，这里不过多整理，特征工程做的事情了。  当然，这里还有一波操作值得注意，就是连续型特征除了用了$x$本身，还用了 $x^2$ ， $logx$ 这种， 可以加入更多非线性，增加模型表达能力。这些特征对新用户的推荐会比较有帮助，常见的用户的地理位置， 设备， 性别，年龄等。</p>
</li>
<li><p>这里一个比较特色的特征是example age。我们知道，视频有明显的生命周期，例如刚上传的视频比之后更受欢迎，也就是用户往往喜欢看最新的东西，而不管它是不是和用户相关，所以视频的流行度随着时间的分布是高度非稳态变化的（下面图中的绿色曲线）。但是我们模型训练的时候，是基于历史数据训练的（历史观看记录的平均），所以模型对播放某个视频预测值的期望会倾向于其在训练数据时间内的平均播放概率（平均热度），图中蓝色线。但如图中绿色线，实际上该视频在训练数据时间窗口内热度很可能不均匀， 用户本身就喜欢新上传的内容。所以，为了让模型学习到用户这种对新颖内容的bias，作者引入了”example age”这个特征来捕捉视频的生命周期。</p>
<p>“example age”定义为$t_{max}-t$， 其中 $t_{max}$ 是训练数据中所有样本的时间最大值，而 $t$ 为当前样本的时间。<strong>线上预测时， 直接把example age全部设为0或一个小的负值，这样就不依赖于各个视频的上传时间了</strong>。 </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202158449.png" alt=""></p>
<p><code>example age</code>这个特征到这里还没完， 原来加入这种时间bias的传统方法是使用<code>video age</code>， 即一个video上传到样本生成的这段时间跨度，对于某个视频的不同样本，其实这两种定义是等价的，因为他们的和是一个常数。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305202201783.png" alt=""></p>
</li>
</ul>
<h4 id="双塔召回"><a href="#双塔召回" class="headerlink" title="双塔召回"></a>双塔召回</h4><h5 id="经典双塔"><a href="#经典双塔" class="headerlink" title="经典双塔"></a>经典双塔</h5><h6 id="经典双塔模型"><a href="#经典双塔模型" class="headerlink" title="经典双塔模型"></a>经典双塔模型</h6><p>在推荐系统中，最为关键的问题是如何做好用户与item的匹配问题，因此对于推荐系统中DSSM模型的则是为 user 和 item 分别构建独立的子网络塔式结构，利用user和item的曝光或点击日期进行训练，最终得到user侧的embedding和item侧的embedding。因此在推荐系统中，常见的模型结构如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211446441.png" alt=""></p>
<p>从模型结构上来看，主要包括两个部分：user侧塔和item侧塔，对于每个塔分别是一个DNN结构。通过两侧的特征输入，通过DNN模块到user和item的embedding，然后计算两者之间的相似度(常用內积或者余弦值，下面会说这两种方式的联系和区别)，因此对于user和item两侧最终得到的embedding维度需要保持一致，即最后一层全连接层隐藏单元个数相同。</p>
<p>在召回模型中，将这种检索行为视为多类分类问题，类似于YouTubeDNN模型。将物料库中所有的item视为一个类别，因此损失函数需要计算每个类的概率值：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211448528.png" alt=""></p>
<p>其中$s(x,y)$表示两个向量的相似度，$P(y|x;\theta)$表示预测类别的概率，$M$表示物料库所有的item。但是在实际场景中，由于物料库中的item数量巨大，在计算上式时会十分的耗时，因此会采样一定的数量的负样本来近似计算，后面针对负样本的采样做一些简单介绍。</p>
<p>以上就是推荐系统中经典的双塔模型，之所以在实际应用中非常常见，是因为<strong>在海量的候选数据进行召回的场景下，速度很快，效果说不上极端好，但一般而言效果也够用了</strong>。之所以双塔模型在服务时速度很快，是因为模型结构简单(两侧没有特征交叉)，但这也带来了问题，双塔的结构无法考虑两侧特征之间的交互信息，<strong>在一定程度上牺牲掉模型的部分精准性</strong>。例如在精排模型中，来自user侧和item侧的特征会在第一层NLP层就可以做细粒度的特征交互，而对于双塔模型，user侧和item侧的特征只会在最后的內积计算时发生，这就导致很多有用的信息在经过DNN结构时就已经被其他特征所模糊了，因此双塔结构由于其结构问题先天就会存在这样的问题。下面针对这个问题来看看一下现有模型的解决思路。</p>
<h6 id="SENet双塔模型"><a href="#SENet双塔模型" class="headerlink" title="SENet双塔模型"></a>SENet双塔模型</h6><p>SENet由Momenta在2017年提出，当时是一种应用于图像处理的新型网络结构。后来张俊林大佬将SENet引入了精排模型<strong>FiBiNET</strong>中，其作用是为了将大量长尾的低频特征抛弃，弱化不靠谱低频特征embedding的负面影响，强化高频特征的重要作用。那SENet结构到底是怎么样的呢，为什么可以起到特征筛选的作用？</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211452527.png" alt=""></p>
<p>从上图可以看出SENET主要分为三个步骤Squeeze, Excitation, Re-weight：</p>
<ul>
<li><p>Squeeze阶段：我们对每个特征的Embedding向量进行数据压缩与信息汇总，即在Embedding维度计算均值：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211453721.png" alt=""></p>
<p>其中k表示Embedding的维度，Squeeze阶段是将每个特征的Squeeze转换成单一的数值。</p>
</li>
<li><p>Excitation阶段：这阶段是根据上一阶段得到的向量进行缩放，即将上阶段的得到的 $1 \times f$ 的向量$Z$先压缩成 $1 \times \frac{f}{r}$ 长度，然后在放回到  $1 \times f$ 的维度，其中$r$表示压缩的程度。这个过程的具体操作就是经过两层DNN。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211454010.png" alt=""></p>
<p>该过程可以理解为：对于当前所有输入的特征，通过相互发生关联，来动态地判断哪些特征重要，哪些特征不重要，而这体现在Excitation阶段的输出结果 $A$，其反应每个特征对应的重要性权重。</p>
</li>
<li><p>Re-weight阶段：是将Excitation阶段得到的每个特征对应的权重 $A$ 再乘回到特征对应的Embedding里，就完成了对特征重要性的加权操作。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211454382.png" alt=""></p>
</li>
</ul>
<p>以上简单的介绍了一下SENet结构，可以发现这种结构可以通过对特征embedding先压缩，再交互，再选择，进而实现特征选择的效果。</p>
<p>此外张俊林大佬还将SENet应用于双塔模型中<strong>（SENet双塔模型）</strong>，模型结构如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211518409.png" alt=""></p>
<p>从上图可以发现，具体地是将双塔中的user塔和Item侧塔的特征输入部分加上一个SENet模块，通过SENet网络，动态地学习这些特征的重要性，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的。</p>
<p>之所以SENet双塔模型是有效的呢？张俊林老师的解释是：双塔模型的问题在于User侧特征和Item侧特征交互太晚，在高层交互，会造成细节信息，也就是具体特征信息的损失，影响两侧特征交叉的效果。而SENet模块在最底层就进行了特征的过滤，使得很多无效低频特征即使被过滤掉，这样更多有用的信息被保留到了双塔的最高层，使得两侧的交叉效果很好；同时由于SENet模块选择出更加重要的信息，使得User侧和Item侧特征之间的交互表达方面增强了DNN双塔的能力。</p>
<p>因此SENet双塔模型主要是从特征选择的角度，提高了两侧特征交叉的有效性，减少了噪音对有效信息的干扰，进而提高了双塔模型的效果。此外，除了这样的方式，还可以通过增加通道的方式来增强两侧的信息交互。即对于user和item两侧不仅仅使用一个DNN结构，而是可以通过不同结构(如FM，DCN等)来建模user和item的自身特征交叉，例如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211523344.png" alt=""></p>
<p>这样对于user和item侧会得到多个embedding，类似于多兴趣的概念。通过得到的多个user和item的embedding，然后分别计算余弦值再相加（两侧的Embedding维度需要对齐），进而增加了双塔两侧的信息交互。而这种方法在腾讯进行过尝试，他们提出的“并联”双塔就是按照这样的思路。</p>
<h6 id="多目标的双塔模型"><a href="#多目标的双塔模型" class="headerlink" title="多目标的双塔模型"></a>多目标的双塔模型</h6><p>现如今多任务学习在实际的应用场景也十分的常见，主要是因为实际场景中业务复杂，往往有很多的衡量指标，例如点击，评论，收藏，关注，转发等。在多任务学习中，往往会针对不同的任务使用一个独有的tower，然后优化不同任务损失。那么针对双塔模型应该如何构建多任务学习框架呢？</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211528560.png" alt=""></p>
<p>如上图所示，在user侧和item侧分别通过多个通道(DNN结构)为每个任务得到一个user embedding和item embedding，然后针对不同的目标分别计算user 和 item 的相似度，并计算各个目标的损失，最后的优化目标可以是多个任务损失之和，或者使用多任务学习中的动态损失权重。</p>
<p>这种模型结构，可以针对多目标进行联合建模，通过多任务学习的结构，一方面可以利用不同任务之间的信息共享，为一些稀疏特征提供其他任务中的迁移信息，另一方面可以在召回时，直接使用一个模型得到多个目标预测，解决了多个模型维护困难的问题。也就是说，在线上通过这一个模型就可以同时得到多个指标，例如视频场景，一个模型就可以直接得到点赞，品论，转发等目标的预测值，进而通过这些值计算分数获得最终的Top-K召回结果。</p>
<h6 id="模型的应用"><a href="#模型的应用" class="headerlink" title="模型的应用"></a>模型的应用</h6><p>在实际的工业应用场景中，分为离线训练和在线服务两个环节。</p>
<ul>
<li>在离线训练阶段，同过训练数据，训练好模型参数。然后将候选库中所有的item集合离线计算得到对应的embedding，并存储进ANN检索系统，比如faiss。为什么将离线计算item集合，主要是因为item的会相对稳定，不会频繁的变动，而对于用户而言，如果将用户行为作为user侧的输入，那么user的embedding会随着用户行为的发生而不断变化，因此对于user侧的embedding需要实时的计算。</li>
<li>在线服务阶段，正是因为用户的行为变化需要被即使的反应在用户的embedding中，以更快的反应用户当前的兴趣，即可以实时地体现用户即时兴趣的变化。因此在线服务阶段需要实时的通过拼接用户特征，输入到user侧的DNN当中，进而得到user embedding，在通过user embedding去 faiss中进行ANN检索，召回最相似的K个item embedding。</li>
</ul>
<p>可以看到双塔模型结构十分的适合实际的应用场景，在快速服务的同时，还可以更快的反应用户即时兴趣的变化。</p>
<h5 id="Youtube双塔"><a href="#Youtube双塔" class="headerlink" title="Youtube双塔"></a>Youtube双塔</h5><p><strong>文章核心思想</strong></p>
<ul>
<li>在大规模的推荐系统中，利用双塔模型对user-item对的交互关系进行建模，学习 $\{user，context\}$ 向量与 $\{item\}$ 向量.</li>
<li>针对大规模流数据，提出in-batch softmax损失函数与流数据频率估计方法(Streaming Frequency Estimation)，可以更好的适应item的多种数据分布。</li>
</ul>
<p><strong>文章主要贡献</strong></p>
<ul>
<li>提出了改进的流数据频率估计方法：针对流数据来估计item出现的频率，利用实验分析估计结果的偏差与方差，模拟实验证明该方法在数据动态变化时的功效</li>
<li>提出了双塔模型架构：提供了一个针对大规模的检索推荐系统，包括了 in-batch softmax 损失函数与流数据频率估计方法，减少了负采样在每个batch中可能会出现的采样偏差问题。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211603934.png" alt=""></p>
<p>模型结构如上图所示，论文旨在对用户和物品建立两个不同的模型，将它们投影到相同维度的空间：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211605627.png" alt=""></p>
<p>模型的输出为用户与物品向量的内积：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211606515.png" alt=""></p>
<p>模型的目标是为了学习参数 $\theta$， 样本集被表示为如下格式 $\{query, item, reward \}$：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211606561.png" alt=""></p>
<ul>
<li>在推荐系统中，$r_i$ 可以扩展来捕获用户对不同候选物品的参与度。</li>
<li>例如，在新闻推荐中 $r_i$ 可以是用户在某篇文章上花费的时间。</li>
</ul>
<h6 id="模型流程"><a href="#模型流程" class="headerlink" title="模型流程"></a>模型流程</h6><ol>
<li><p>给定用户 $x$，基于 softmax 函数从物料库 $M$ 中选中候选物品 $y$ 的概率为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211612604.png" alt=""></p>
<ul>
<li><p>考虑到相关奖励 $r_i$ ，加权对数似然函数的定义如下：  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211612034.png" alt=""></p>
</li>
</ul>
</li>
<li><p>原表达式 $\mathcal{P}(y \mid x ; \theta)$ 中的分母需要遍历物料库中所有的物品，计算成本太高，故对分母中的物品要进行负采样。为了提高负采样的速度，一般是直接从训练样本所在 Batch 中进行负样本选择。于是有：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211613957.png" alt=""></p>
<ul>
<li>其中，$B$ 表示与样本 $\{x_i,y_j\}$ 同在一个 Batch 的物品集合。</li>
<li>举例来说，对于用户1，Batch 内其他用户的正样本是用户1的负样本。</li>
</ul>
</li>
<li><p>一般而言，负采样分为 <strong>Easy Negative Sample</strong> 和 <strong>Hard Negative Sample</strong> 。</p>
<ul>
<li><p>这里的 Easy Negative Sample 一般是直接从<strong>全局物料库</strong>中随机选取的负样本，由于每个用户感兴趣的物品有限，而物料库又往往很大，故即便从物料库中随机选取负样本，也大概率是用户不感兴趣的。</p>
</li>
<li><p>在真实场景中，热门物品占据了绝大多数的购买点击。而这些热门物品往往只占据物料库物品的少部分，绝大部分物品是冷门物品。</p>
<ul>
<li>在物料库中随机选择负样本，往往被选中的是冷门物品。这就会造成马太效应，热门物品更热，冷门物品更冷。</li>
<li>一种解决方式时，在对训练样本进行负采样时，提高热门物品被选为负样本的概率，工业界的经验做法是物品被选为负样本的概率正比于物品点击次数的 0.75 次幂。</li>
</ul>
</li>
<li><p>前面提到 Batch 内进行负采样，热门物品出现在一个 Batch 的概率正比于它的点击次数。问题是，热门物品被选为负样本的概率过高了（一般正比于点击次数的 0.75 次幂），导致热门物品被过度打压。</p>
</li>
<li><p>在本文中，为了避免对热门物品进行过度惩罚，进行了纠偏。公式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211616699.png" alt=""></p>
<ul>
<li>在内积 $s(x_i,y_j)$ 的基础上，减去了物品 $j$ 的采样概率的对数。</li>
</ul>
</li>
</ul>
</li>
<li><p>纠偏后，物品 $y$ 被选中的概率为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211617066.png" alt=""></p>
<ul>
<li>此时，batch loss function 的表示式如下：</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211617038.png" alt=""></p>
<ul>
<li>通过 SGD 和学习率，来优化模型参数 $\theta$ ：</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211617296.png" alt=""></p>
</li>
<li><p>Normalization and Temperature</p>
<ul>
<li><p>最后一层，得到用户和物品的特征 Embedding 表示后，再进行进行 $l2$ 归一化：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211618509.png" alt=""></p>
<ul>
<li>本质上，其实就是将用户和物品的向量内积转换为了余弦相似度。</li>
</ul>
</li>
<li><p>对于内积的结果，再除以温度参数 $\tau$：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211618888.png" alt=""></p>
<ul>
<li>论文提到，这样有利于提高预测准确度。</li>
<li>从实验结果来看，温度参数 $\tau$ 一般小于 $1$，所以感觉就是放大了内积结果。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="基于图的召回"><a href="#基于图的召回" class="headerlink" title="基于图的召回"></a>基于图的召回</h3><h4 id="EGES"><a href="#EGES" class="headerlink" title="EGES"></a>EGES</h4><p><strong>Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</strong></p>
<p>在电商领域，推荐已经是不可或缺的一部分，旨在为用户的喜好提供有趣的物品，并且成为淘宝和阿里巴巴收入的重要引擎。尽管学术界和产业界的各种推荐方法都取得了成功，如协同过滤、基于内容的方法和基于深度学习的方法，但由于用户和项目的数十亿规模，传统的方法已经不能满足于实际的需求，主要的问题体现在三个方面：</p>
<ul>
<li>可扩展性：现有的推荐方法无法扩展到在拥有十亿的用户和二十亿商品的淘宝中。</li>
<li>稀疏性：存在大量的物品与用户的交互行为稀疏。即用户的交互到多集中于以下部分商品，存在大量商品很少被用户交互。</li>
<li>冷启动：在淘宝中，每分钟会上传很多新的商品，由于这些商品没有用户行为的信息（点击、购买等），无法进行很好的预测。</li>
</ul>
<p><strong>思路</strong></p>
<p>根据上述所面临的三个问题，本文针对性的提出了三个模型予以解决：Base Graph Embedding（BGE）；Graph Embedding with Side Information（GES）；Enhanced Graph Embedding with Side Information（EGES）。</p>
<p>考虑可扩展性的问题，图嵌入的随机游走方式可以在物品图上捕获<strong>物品之间高阶相似性</strong>，即Base Graph Embedding（BGE）方法。其不同于CF方法，除了考虑物品的共现，还考虑到了行为的序列信息。</p>
<p>考虑到稀疏性和冷启物品问题，在图嵌入的基础上，考虑了节点的属性信息。希望具有相似属性的物品可以在空间上相似，即希望通过头部物品，提高属性信息的泛化能力，进而帮助尾部和冷启物品获取更加准确的embedding，即Graph Embedding with Side Information（GES）方法。</p>
<p>考虑到不同属性信息对于学习embedding的贡献不同，因此在聚合不同的属性信息时，动态的学习不同属性对于学习节点的embedding所参与的重要性权重，即Enhanced Graph Embedding with Side Information（EGES）。</p>
<h5 id="构建物品图"><a href="#构建物品图" class="headerlink" title="构建物品图"></a>构建物品图</h5><p>在介绍三个模型之前，我们首先需要构建好item-item图。由于基于CF的方法仅考虑物品之间的共现，忽略了行为的序列信息(即序列中相邻的物品之间的语义信息)，因此item-item图的构建方式如下图所示。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211633299.png" alt=""></p>
<p>首先根据用户的session行为序列构建网络结构，即序列中相邻两个item之间在存在边，并且是有向带权图。物品图边上的权重为所有用户行为序列中两个 item 共现的次数，最终构造出来简单的有向有权图。</p>
<p>值得注意的是，本文通过行为序列中物品的共现来表示其中的<strong>语义信息</strong>，并将这种语义信息理解为<strong>物品之间的相似性</strong>，并将共现频次作为相似性的一个度量值。其次基于用户的历史行为序列数据，一般不太可能取全量的历史序列数据，一方面行为数据量过大，一方面用户的兴趣会随时间发生演变，因此在处理行为序列时会设置了一个窗口来截断历史序列数据，切分出来的序列称为session。</p>
<p>由于实际中会存在一些现实因素，数据中会有一些噪音，需要特殊处理，主要分为三个方面：</p>
<ul>
<li>从行为方面考虑，用户在点击后停留的时间少于1秒，可以认为是误点，需要移除。</li>
<li>从用户方面考虑，淘宝场景中会有一些过度活跃用户。本文对活跃用户的定义是三月内购买商品数超过1000，或者点击数超过3500，就可以认为是一个无效用户，需要去除。</li>
<li>从商品方面考虑，存在一些商品频繁的修改，即ID对应的商品频繁更新，这使得这个ID可能变成一个完全不同的商品，这就需要移除与这个ID相关的这个商品。</li>
</ul>
<h5 id="图嵌入-BGE"><a href="#图嵌入-BGE" class="headerlink" title="图嵌入(BGE)"></a>图嵌入(BGE)</h5><p>对于图嵌入模型，第一步先进行随机游走得到物品序列；第二步通过skip-gram为图上节点生成embedding。那么对于随机游走的思想：如何利用随机游走在图中生成的序列？不同于DeepWalk中的随机游走，本文的采样策略使用的是带权游走策略，不同权重的游走到的概率不同，（其本质上就是node2vec），传统的node2vec方法可以直接支持有向带权图。因此在给定图的邻接矩阵M后(表示节点之间的边权重)，随机游走中每次转移的概率为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211638065.png" alt=""></p>
<p>其中$M_{ij}$为边$e_{ij}$上的权重，$N_{+}(v_i)$表示节点$v_i$所有邻居节点集合，并且随机游走的转移概率的对每个节点所有邻接边权重的归一化结果。在随机游走之后，每个item得到一个序列。</p>
<p>然后类似于word2vec，为每个item学习embedding，于是优化目标如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211639379.png" alt=""></p>
<p>其中，w 为窗口大小。考虑独立性假设的话，上面的式子可以进一步化简：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211640294.png" alt=""></p>
<p>这样看起来就很直观了，在已知物品 i 时，最大化序列中(上下文)其他物品 j 的条件概率。为了近似计算，采样了Negative sampling，上面的优化目标可以化简得到如下式子：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211640558.png" alt=""></p>
<p>其中$N(v_i)’$表示负样本集合，负采样个数越多，结果越好。</p>
<h5 id="基于side-information的图嵌入（GES）"><a href="#基于side-information的图嵌入（GES）" class="headerlink" title="基于side information的图嵌入（GES）"></a>基于side information的图嵌入（GES）</h5><p>尽管BGE将行为序列关系编码进物品的embedding中，从而从用户行为中捕捉高阶相似性。但是这里有个问题，对于新加入的商品，由于未和用户产生过交互，所以不会出现在item-item图上，进而模型无法学习到其embedding，即无法解决冷启动问题。</p>
<p>为了解决冷启问题，本文通过使用side information（ 类别，店铺, 价格等）加入模型的训练过程中，使得模型最终的泛化能力体现在商品的side information上。这样通过<strong>side information学习到的embedding来表示具体的商品</strong>，使得相似side information的物品可以得到在空间上相近的表示，进而来增强 BGE。</p>
<p>那么对于每个商品如何通过side information的embedidng来表示呢？对于随机游走之后得到的商品序列，其中每个商品由其id和属性(品牌，价格等)组成。用公式表示，对于序列中的每一个物品可以得到$W^0_V,…W_V^n$,（n+1）个向量表示，$W^0_V$表示物品v，剩下是side information的embedding。然后将所有的side information聚合成一个整体来表示物品，聚合方式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211642022.png" alt=""></p>
<p>其中，$H_v$是商品 v 的聚合后的 embedding 向量。</p>
<h5 id="增强型EGS（EGES）"><a href="#增强型EGS（EGES）" class="headerlink" title="增强型EGS（EGES）"></a>增强型EGS（EGES）</h5><p>尽管 GES 相比 BGE 在性能上有了提升，但是在聚合多个属性向量得到商品的embedding的过程中，不同 side information的聚合依然存在问题。在GES中采用 average-pooling 是在假设不同种类的 side information 对商品embedding的贡献是相等的，但实际中却并非如此。例如，购买 Iphone 的用户更可能倾向于 Macbook 或者 Ipad，相比于价格属性，品牌属性相对于苹果类商品具有更重要的影响。因此，根据实际现状，不同类型的 side information 对商品的表示是具有不同的贡献值的。</p>
<p>针对上述问题，作者提出了weight pooling方法来聚合不同类型的 side information。具体地，EGES 与 GES 的区别在聚合不同类型 side information计算不同的权重，根据权重聚合 side information 得到商品的embedding，如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211645673.png" alt=""></p>
<p>其中 $a_i$ 表示每个side information 用于计算权重的参数向量，最终通过下面的公式得到商品的embedding：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211646357.png" alt=""></p>
<p>这里对参数 $a_v^j$ 先做指数变换，目的是为了保证每个边界信息的贡献都能大于0，然后通过归一化为每个特征得到一个o-1之内的权重。最终物品的embedding通过权重进行加权聚合得到，进而优化损失函数：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211646534.png" alt=""></p>
<p> y是标签符号，等于1时表示正样本，等于0时表示负样本。$H_v$表示商品 v 的最终的隐层表示，$Z_u$表示训练数据中的上下文节点的embedding。</p>
<h4 id="PinSAGE"><a href="#PinSAGE" class="headerlink" title="PinSAGE"></a>PinSAGE</h4><p><strong>Graph Convolutional Neural Networks for Web-Scale Recommender Systems</strong></p>
<p>该论文是斯坦福大学和Pinterest公司与2018年联合发表与KDD上的一篇关于GCN成功应用于工业级推荐系统的工作。该论文提到的PinSage模型，是在GraphSAGE的理论基础进行了更改，以适用于实际的工业场景。下面将简单介绍一下GraphSAGE的原理，以及Pinsage的核心和细节。</p>
<h5 id="GraphSAGE原理"><a href="#GraphSAGE原理" class="headerlink" title="GraphSAGE原理"></a>GraphSAGE原理</h5><p>GraphSAGE提出的前提是因为基于直推式(transductive)学习的图卷积网络无法适应工业界的大多数业务场景。我们知道的是，基于直推式学习的图卷积网络是通过拉普拉斯矩阵直接为图上的每个节点学习embedding表示，每次学习是针对于当前图上所有的节点。然而在实际的工业场景中，图中的结构和节点都不可能是固定的，会随着时间的变化而发生改变。例如在Pinterest公司的场景下，每分钟都会上传新的照片素材，同时也会有新用户不断的注册，那么图上的节点会不断的变化。在这样的场景中，直推式学习的方法就需要不断的重新训练才能够为新加入的节点学习embedding，导致在实际场景中无法投入使用。</p>
<p>在这样的背景下，斯坦福大学提出了一种归纳(inductive)学习的GCN方法——GraphSAGE，即<strong>通过聚合邻居信息的方式为给定的节点学习embedding</strong>。不同于直推式(transductive)学习，GraphSAGE是通过学习聚合节点邻居生成节点Embedding的函数的方式，为任意节点学习embedding，进而将GCN扩展成归纳学习任务。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211711238.png" alt=""></p>
<p>上面这个公式可以非常直观的让我们理解GraphSAGE的原理。</p>
<ul>
<li>$h_v^0$表示图上节点的初始化表示，等同于节点自身的特征。</li>
<li>$h_v^k$表示第k层卷积后的节点表示，其来源于两个部分：<ul>
<li>第一部分来源于节点v的邻居节点集合$N(v)$，利用邻居节点的第k-1层卷积后的特征$h_u^{k-1}$进行 （ $\sum_{u \in N(v)} \frac{h_u^{k-1}}{|N(v)|}$ ）后，在进行线性变换。这里<strong>借助图上的边将邻居节点的信息通过边关系聚合到节点表示中(简称卷积操作)</strong>。</li>
<li>第二部分来源于节点v的第k-1层卷积后的特征$h_v^{k-1}$，进行线性变换。总的来说图卷积的思想是<strong>在对自身做多次非线性变换时，同时利用边关系聚合邻居节点信息。</strong></li>
</ul>
</li>
<li>最后一次卷积结果作为节点的最终表示$Z_v$，以用于下游任务(节点分类，链路预测或节点召回)。</li>
</ul>
<p>可以发现相比传统的方法(MLP，CNN，DeepWalk 或 EGES)，GCN或GraphSAGE存在一些优势：</p>
<ol>
<li>相比于传统的深度学习方法(MLP,CNN)，GCN在对自身节点进行非线性变换时，同时考虑了图中的邻接关系。从CNN的角度理解，GCN通过堆叠多层结构在图结构数据上拥有更大的<strong>感受野</strong>，利用更加广域内的信息。</li>
<li>相比于图嵌入学习方法(DeepWalk，EGES)，GCN在学习节点表示的过程中，在利用节点自身的属性信息之外，更好的利用图结构上的边信息。相比于借助随机采样的方式来使用边信息，GCN的方式能从全局的角度利用的邻居信息。此外，类似于GraphSAGE这种归纳(inductive)学习的GCN方法，通过学习聚合节点邻居生成节点Embedding的函数的方式，更适用于图结构和节点会不断变化的工业场景。</li>
</ol>
<p>在采样得到目标节点的邻居集之后，那么如何聚合邻居节点的信息来更新目标节点的嵌入表示呢？下面就来看看GraphSAGE中提及的四个聚合函数。</p>
<h5 id="GraphSAGE的采样和聚合"><a href="#GraphSAGE的采样和聚合" class="headerlink" title="GraphSAGE的采样和聚合"></a>GraphSAGE的采样和聚合</h5><p>通过上面的公式可以知道，得到节点的表示主要依赖于两部分，其中一部分其邻居节点。因此对于GraphSAGE的关键主要分为两步：Sample采样和Aggregate聚合。其中Sample的作用是从庞大的邻居节点中选出用于聚合的邻居节点集合$N(v)$以达到降低迭代计算复杂度，而聚合操作就是如何利用邻居节点的表示来更新节点v的表示，已达到聚合作用。具体的过程如下伪代码所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211716358.png" alt=""></p>
<p>GraphSAGE的minibatch算法的思路是针对Batch内的所有节点，通过采样和聚合节点，为每一个节点学习一个embedding。</p>
<p><strong>邻居采样</strong></p>
<p>GraphSAGE的具体采样过程是，首先根据中心节点集合$B^k$，对集合中每个中心节点通过随机采样的方式对其邻居节点采样固定数量S个(如果邻居节点数量大于S，采用无放回抽样；如果小于S，则采用有放回抽样)，形成的集合表示为$B^{k-1}$；以此类推每次都是为前一个得到的集合的每个节点随机采样S个邻居，最终得到第k层的所有需要参与计算的节点集合$B^{0}$。值得注意的有两点：<strong>为什么需要采样并且固定采样数量S？</strong> <strong>为什么第k层所采样的节点集合表示为$B^0$？</strong></p>
<p>进行邻居采样并固定采样数量S主要是因为：1. 采样邻居节点避免了在全图的搜索以及使用全部邻居节点所导致计算复杂度高的问题；2. 可以通过采样使得部分节点更同质化，即两个相似的节点具有相同表达形式。3. 采样固定数量是保持每个batch的计算占用空间是固定的，方便进行批量训练。</p>
<p>第k层所采样的节点集合表示为$B^0$主要是因为：采样和聚合过程是相反的，即采样时我们是从中心节点组层进行采样，而聚合的过程是从中心节点的第k阶邻居逐层聚合得到前一层的节点表示。因此可以认为聚合阶段是：将k阶邻居的信息聚合到k-1阶邻居上，k-1阶邻居的信息聚合到k-2阶邻居上，….，1阶邻居的信息聚合到中心节点上的过程。</p>
<p><strong>聚合函数</strong></p>
<p>如何对于采样到的节点集进行聚合，介绍的4种方式：Mean 聚合、Convolutional 聚合、LSTM聚合以及Pooling聚合。由于邻居节点是无序的，所以希望构造的聚合函数具有<strong>对称性(即输出的结果不因输入排序的不同而改变)</strong>，同时拥有<strong>较强的表达能力</strong>。</p>
<ul>
<li>Mean 聚合：首先会对邻居节点按照<strong>element-wise</strong>进行均值聚合，然后将当前节点k-1层得到特征$h_v^{k-1}$与邻居节点均值聚合后的特征 $MEAN(h_u^k | u\in N(v))$<strong>分别</strong>送入全连接网络后<strong>相加</strong>得到结果。</li>
<li><p>Convolutional 聚合：这是一种基于GCN聚合方式的变种，首先对邻居节点特征和自身节点特征求均值，得到的聚合特征送入到全连接网络中。与Mean不同的是，这里<strong>只经过一个全连接层</strong>。</p>
</li>
<li><p>LSTM聚合：由于LSTM可以捕捉到序列信息，因此相比于Mean聚合，这种聚合方式的<strong>表达能力更强</strong>；但由于LSTM对于输入是有序的，因此该方法不具备<strong>对称性</strong>。作者对于无序的节点进行随机排列以调整LSTM所需的有序性。</p>
</li>
<li>Pooling聚合：对于邻居节点和中心节点进行一次非线性转化，将结果进行一次基于<strong>element-wise</strong>的<strong>最大池化</strong>操作。该种方式具有<strong>较强的表达能力</strong>的同时还具有<strong>对称性</strong>。</li>
</ul>
<p>综上，可以发现GraphSAGE之所以可以用于大规模的工业场景，主要是因为模型主要是通过学习聚合函数，通过归纳式的学习方法为节点学习特征表示。</p>
<h5 id="PinSAGE-1"><a href="#PinSAGE-1" class="headerlink" title="PinSAGE"></a>PinSAGE</h5><p>PinSAGE 模型是Pinterest 在GraphSAGE 的基础上实现的可以应用于实际工业场景的召回算法。Pinterest 公司的主要业务是采用瀑布流的形式向用户展现图片，无需用户翻页，新的图片会自动加载。因此在Pinterest网站上，有大量的图片(被称为pins)，而用户可以将喜欢的图片分类，即将pins钉在画板 boards上。可以发现基于这样的场景，pin相当于普通推荐场景中item，用户<strong>钉</strong>的行为可以认为是用于的交互行为。于是PinSAGE 模型主要应用的思路是，基于GraphSAGE 的原理学习到聚合方法，并为每个图片(pin)学习一个向量表示，然后基于pin的向量表示做<strong>item2item的召回</strong>。</p>
<p>可以知道的是，PinSAGE 是在GraphSAGE的基础上进行改进以适应实际的工业场景，因此除了改进卷积操作中的邻居采样策略以及聚合函数的同时还有一些工程技巧上的改进，使得在大数据场景下能更快更好的进行模型训练。因此在了解GraphSAGE的原理后，我们详细的了解一下本文的主要改进以及与GraphSAGE的区别。</p>
<p><strong>重要性采样</strong></p>
<p>在实际场景当中，一个item可能被数以百万，千万的用户交互过，所以不可能聚合所有邻居节点是不可行的，只可能是采样部分邻居进行信息聚合。但是如果采用GraphSAGE中随机采样的方法，由于采样的邻居有限(这里是相对于所有节点而言)，会存在一定的偏差。因此PinSAGE 在采样中考虑了更加重要的邻居节点，即卷积时只注重部分重要的邻居节点信息，已达到高效计算的同时又可以消除偏置。</p>
<p>PinSAGE使用重要性采样方法，即需要为每个邻居节点计算一个重要性权重，根据权重选取top-t的邻居作为聚合时的邻居集合。其中计算重要性的过程是，以目标节点为起点，进行random-walk，采样结束之后计算所有节点访问数的L1-normalized作为重要性权重，同时这个权重也会在聚合过程中加以使用(<strong>加权聚合</strong>)。</p>
<p>这里对于<strong>计算权重之后如何得到top-t的邻居节点，</strong>原文并没有直接的叙述。这里可以有两种做法，第一种就是直接采用重要权重，这种方法言简意赅，比较直观。第二种做法就是对游走得到的所有邻居进行随机抽样，而计算出的权重可以用于聚合阶段。个人理解第二种做法的可行性出于两点原因，其一是这样方法可以避免存在一些item由于权重系数低永远不会被选中的问题；其二可能并不是将所有重要性的邻居进行聚合更合理，毕竟重要性权重是通过随机采样而得到的，具有一定的随机性。当然以上两种方法都是可行的方案，可以通过尝试看看具体哪种方法会更有效。</p>
<p><strong>聚合函数</strong></p>
<p>PinSAGE中提到的Convolve算法（单层图卷积操作）相当于GraphSAGE算法的聚合过程，在实际执行过程中通过对每一层执行一次图卷积操作以得到不同阶邻居的信息，具体过程如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211728762.png" alt=""></p>
<p>上述的单层图卷积过程如下三步：</p>
<ol>
<li>聚合邻居： 先将所有的邻居节点经过一次非线性转化(一层DNN)，再由聚合函数(Pooling聚合) $\gamma$（如元素平均，<strong>加权和</strong>等）将所有邻居信息聚合成目标节点的embedding。这里的加权聚合采用的是通过random-walk得到的重要性权重。</li>
<li>更新当前节点的embedding：将目标节点当前的向量 $z_u$ 与步骤1中聚合得到的邻居向量 $n_u$ 进行拼接，在通过一次非线性转化。</li>
<li>归一化操作：对目标节点向量 $z_u$ 归一化。</li>
</ol>
<p>Convolve算法的聚合方法与GraphSAGE的Pooling聚合函数相同，主要区别在于对更新得到的向量 $z_u$ 进行归一化操作，<strong>可以使训练更稳定，以及在近似查找最近邻的应用中更有效率。</strong></p>
<p><strong>基于mini-batch堆叠多层图卷积</strong></p>
<p>与GraphSAGE类似，采用的是基于mini-batch 的方式进行训练。之所以这么做的原因是因为什么呢？在实际的工业场景中，由于用户交互图非常庞大，无法对于所有的节点同时学习一个embedding，因此需要从原始图上寻找与 mini-batch 节点相关的子图。具体地是说，对于mini-batch内的所有节点，会通过采样的方式逐层的寻找相关邻居节点，再通过对每一层的节点做一次图卷积操作，以从k阶邻居节点聚合信息。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211730740.png" alt=""></p>
<p>如上图所示：对于batch内的所有节点(图上最顶层的6个节点)，依次根据权重采样，得到batch内所有节点的一阶邻居(图上第二层的所有节点)；然后对于所有一阶邻居再次进行采样，得到所有二阶邻居(图上的最后一层)。节点采样阶段完成之后，与采样的顺序相反进行聚合操作。首先对二阶邻居进行单次图卷积，将二阶节点信息聚合已更新一阶节点的向量表示(其中小方块表示的是一层非线性转化)；其次对一阶节点再次进行图卷积操作，将一阶节点的信息聚合已更新batch内所有节点的向量表示。仅此对于一个batch内的所有的样本通过卷积操作学习到一个embedding，而每一个batch的学习过程中仅<strong>利用与mini-batch内相关节点的子图结构。</strong></p>
<p><strong>训练过程</strong></p>
<p>PinSage在训练时采用的是 Margin Hinge Loss 损失函数，主要的思想是最大化正例embedding之间的相关性，同时还要保证负例之间相关性相比正例之间的相关性小于某个阈值(Margin)。具体的公式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211731896.png" alt=""></p>
<p>其中$Z_p$是学习得到的目标节点embedding，$Z_i$是与目标节点相关item的embedding，$Z_{n_k}$是与目标节点不相关item的embedding，$\Delta$为margin值，具体大小需要调参。那么对于相关节点i，以及不相关节点nk，具体都是如何定义的，这对于召回模型的训练意义重大，让我们看看具体是如何定义的。</p>
<p>对于正样本而言，文中的定义是如果用户在点击的 item q之后立即点击了 item i，即认为 &lt; q, i &gt;构成正样本对。直观的我们很好理解这句话，不过在参考DGL中相关代码实现时，发现这部分的内容和原文中有一定的出入。具体地，代码中将所有的训练样本构造成用户-项目二部图，然后对batch内的每个 item q，根据item-user-item的元路径进行随机游走，得到被同一个用户交互过的 item i，因此组成<q,i>正样本对。对于负样本部分，相对来说更为重要，因此内容相对比较多，将在下面的负样本生成部分详细介绍。</p>
<p>这里还有一个比较重要的细节需要注意，由于模型是用于 item to item的召回，因此优化目标是与正样本之间的表示尽可能的相近，与负样本之间的表示尽可能的远。而图卷积操作会使得具有邻接关系的节点表示具有同质性，因此结合这两点，就需要在构建图结构的时，要将<strong>训练样本之间可能存在的边在二部图上删除</strong>，避免因为边的存在使得因卷积操作而导致的信息泄露。</p>
<p><strong>工程技巧</strong></p>
<ol>
<li><p>负样本的生成</p>
<p>召回模型最主要的任务是从候选集合中选出用户可能感兴趣的item，直观的理解就是让模型将用户喜欢的和不喜欢的进行区分。然而由于候选集合的庞大数量，许多item之间十分相似，导致模型划分出来用户喜欢的item中会存在一些难以区分的item(即与用户非常喜欢item比较相似的那一部分)。因此对于召回模型不仅能区分用户喜欢和不喜欢的 item，同时还能区分与用户喜欢的 item 十分相似的那一部分item。那么如果做到呢？这主要是交给 easy negative examples 和 hard negative examples 两种负样本给模型学习。</p>
<ul>
<li>easy 负样本：这里对于mini-batch内的所有pair(训练样本对)会共享500负样本，这500个样本从batch之外的所有节点中随机采样得到。这么做可以减少在每个mini-batch中因计算所有节点的embedding所需的时间，文中指出这和为每个item采样一定数量负样本无差异。</li>
<li>hard 负样本：这里使用hard 负样本的原因是根据实际场景的问题出发，模型需要从20亿的物品item集合中识别出最相似的1000个，即模型需要从2百万 item 中识别出最相似的那一个 item。也就是说模型的区分能力不够细致，为了解决这个问题，加入了一些hard样本。对于hard 负样本，应该是与 q 相似 以及和 i 不相似的物品，具体地的生成方式是将图上的节点计算相对节点 q 的个性化PageRank分值，根据分值的排序随机从2000~5000的位置选取节点作为负样本。</li>
</ul>
</li>
<li><p>渐进式训练(Curriculum training)</p>
<p>由于hard 负样本的加入，模型的训练时间加长（由于与q过于相似，导致loss比较小，导致梯度更新的幅度比较小，训练起来比较慢），那么渐进式训练就是为了来解决这个问题。</p>
<p>如何渐进式：先在第一轮训练使用easy 负样本，帮助模型先快速收敛(先让模型有个最基本的分辨能力)到一定范围，然后在逐步分加入hard负样本(方式是在第n轮训练时给每个物品的负样本集合增加n-1个 hard 负样本)，以调整模型细粒度的区分能力(让模型能够区分相似的item)。</p>
</li>
<li><p>节点特征(side information)</p>
<p>这里与EGES的不同，这里的边信息不是端到端训练得到，而是通过事前的预处理得到的。对于每个节点(即 pin)，都会有一个图片和一点文本信息。因此对于每个节点使用图片的向量、文字的向量以及节点的度拼接得到。这里其实也解释了为什么在图卷积操作时，会先进行一个非线性转化，其实就是将不同空间的特征进行转化(融合)。</p>
</li>
<li><p>构建 mini-batch</p>
<p>不同于常规的构建方式，PinSAGE中构建mini-batch的方式是基于生产者消费者模式。什么意思的，就是将CPU和GPU分开工作，让CPU负责取特征，重建索引，邻接列表，负采样等工作，让GPU进行矩阵运算，即CPU负责生产每个batch所需的所有数据，GPU则根据CPU生产的数据进行消费(运算)。这样由于考虑GPU的利用率，无法将所有特征矩阵放在GPU，只能存在CPU中，然而每次查找会导致非常耗时，通过上面的方式使得图卷积操作过程中就没有GPU与CPU的通信需求。</p>
</li>
<li><p>多GPU训练超大batch</p>
<p>前向传播过程中，各个GPU等分minibatch，共享一套模型参数；反向传播时，将每个GPU中的参数梯度都聚合到一起，同步执行SGD。为了保证因海量数据而使用的超大batchsize的情况下模型快速收敛以及泛化精度，采用warmup过程，即在第一个epoch中将学习率线性提升到最高，后面的epoch中再逐步指数下降。</p>
</li>
<li><p>使用MapReduce高效推断</p>
<p>在模型训练结束之后，需要为所有节点计算一个embedding，如果按照训练过程中的前向传播过程来生成，会存在大量重复的计算。因为当计算一个节点的embedding的时候，其部分邻居节点已经计算过了，同时如果该节点作为其他节点邻居时，也会被再次计算。针对这个问题，本文采用MapReduce的方法进行推断。该过程主要分为两步，具体如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211735181.png" alt=""></p>
<ol>
<li>将item的embedding进行聚合，即利用item的图片、文字和度等信息的表示进行join(拼接)，在通过一层dense后得到item的低维向量。</li>
<li>然后根据item来匹配其一阶邻居(join)，然后根据item进行pooling(其实就是GroupBy pooling)，得到一次图卷积操作。通过堆叠多次直接得到全量的embedding。</li>
</ol>
<p>其实这块主要就是通过MapReduce的大数据处理能力，直接对全量节点进行一次运算得到其embedding，避免了分batch所导致的重复计算。</p>
</li>
</ol>
<h3 id="基于序列的召回"><a href="#基于序列的召回" class="headerlink" title="基于序列的召回"></a>基于序列的召回</h3><h4 id="MIND"><a href="#MIND" class="headerlink" title="MIND"></a>MIND</h4><p>MIND模型(Multi-Interest Network with Dynamic Routing)， 是阿里团队2019年在CIKM上发的一篇paper，该模型依然是用在召回阶段的一个模型，解决的痛点是之前在召回阶段的模型，比如双塔，YouTubeDNN召回模型等，在模拟用户兴趣的时候，总是基于用户的历史点击，最后通过pooling的方式得到一个兴趣向量，用该向量来表示用户的兴趣，但是该篇论文的作者认为，<strong>用一个向量来表示用户的广泛兴趣未免有点太过于单一</strong>，这是作者基于天猫的实际场景出发的发现，每个用户每天与数百种产品互动， 而互动的产品往往来自于很多个类别，这就说明用户的兴趣极其广泛，<strong>用一个向量是无法表示这样广泛的兴趣的</strong>，于是乎，就自然而然的引出一个问题，<strong>有没有可能用多个向量来表示用户的多种兴趣呢？</strong> </p>
<p>这篇paper的核心是胶囊网络，<strong>该网络采用了动态路由算法能非常自然的将历史商品聚成多个集合，每个集合的历史行为进一步推断对应特定兴趣的用户表示向量。这样，对于一个特定的用户，MND输出了多个表示向量，它们代表了用户的不同兴趣。当用户再有新的交互时，通过胶囊网络，还能实时的改变用户的兴趣表示向量，做到在召回阶段的实时个性化</strong>。那么，胶囊网络究竟是怎么做到的呢？ 胶囊网络又是什么原理呢？</p>
<h5 id="背景与动机"><a href="#背景与动机" class="headerlink" title="背景与动机"></a>背景与动机</h5><p>本章是基于天猫APP的背景来探索十亿级别的用户个性化推荐。天猫的推荐的流程主要分为召回阶段和排序阶段。召回阶段负责检索数千个与用户兴趣相关的候选物品，之后，排序阶段预测用户与这些候选物品交互的精确概率。这篇文章做的是召回阶段的工作，来对满足用户兴趣的物品的有效检索。</p>
<p>作者这次的出发点是基于场景出发，在天猫的推荐场景中，作者发现<strong>用户的兴趣存在多样性</strong>。平均上，10亿用户访问天猫，每个用户每天与数百种产品互动。交互后的物品往往属于不同的类别，说明用户兴趣的多样性。 一张图片会更加简洁直观：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212023626.png" alt=""></p>
<p>因此如果能在<strong>召回阶段建立用户多兴趣模型来模拟用户的这种广泛兴趣</strong>，那么作者认为是非常有必要的，因为召回阶段的任务就是根据用户兴趣检索候选商品嘛。</p>
<p>那么，如何能基于用户的历史交互来学习用户的兴趣表示呢？  以往的解决方案如下：</p>
<ul>
<li>协同过滤的召回方法(itemcf和usercf)是通过历史交互过的物品或隐藏因子直接表示用户兴趣，但会遇到<strong>稀疏或计算问题</strong></li>
<li>基于深度学习的方法用低维的embedding向量表示用户，比如YoutubeDNN召回模型，双塔模型等，都是把用户的基本信息，或者用户交互过的历史商品信息等，过一个全连接层，最后编码成一个向量，用这个向量来表示用户兴趣，但作者认为，<strong>这是多兴趣表示的瓶颈</strong>，因为需要压缩所有与用户多兴趣相关的信息到一个表示向量，所有用户多兴趣的信息进行了混合，导致这种多兴趣并无法体现，所以往往召回回来的商品并不是很准确，除非向量维度很大，但是大维度又会带来高计算。</li>
<li>DIN模型在Embedding的基础上加入了Attention机制，来选择的捕捉用户兴趣的多样性，但采用Attention机制，<strong>对于每个目标物品，都需要重新计算用户表示</strong>，这在召回阶段是行不通的（海量），所以DIN一般是用于排序。</li>
</ul>
<p>所以，作者想在召回阶段去建模用户的多兴趣，但以往的方法都不好使，为了解决这个问题，就提出了动态路由的多兴趣网络MIND。为了推断出用户的多兴趣表示，提出了一个多兴趣提取层，该层使用动态路由机制自动的能将用户的历史行为聚类，然后每个类簇中产生一个表示向量，这个向量能代表用户某种特定的兴趣，而多个类簇的多个向量合起来，就能表示用户广泛的兴趣了。</p>
<p>这就是MIND的提出动机以及初步思路了，这里面的核心是Multi-interest extractor layer，而这里面重点是动态路由与胶囊网络，所以接下来先补充这方面的相关知识。</p>
<h5 id="胶囊网络与动态路由机制"><a href="#胶囊网络与动态路由机制" class="headerlink" title="胶囊网络与动态路由机制"></a>胶囊网络与动态路由机制</h5><h6 id="胶囊网络初识"><a href="#胶囊网络初识" class="headerlink" title="胶囊网络初识"></a>胶囊网络初识</h6><p>Hinton大佬在2011年的时候，就首次提出了”胶囊”的概念， “胶囊”可以看成是一组聚合起来输出整个向量的小神经元组合，这个向量的每个维度(每个小神经元)，代表着某个实体的某个特征。</p>
<p>胶囊网络其实可以和神经网络对比着看可能更好理解，我们知道神经网络的每一层的神经元输出的是单个的标量值，接收的输入，也是多个标量值，所以这是一种value to value的形式，而胶囊网络每一层的胶囊输出的是一个向量值，接收的输入也是多个向量，所以它是vector to vector形式的。来个图对比下就清楚了：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212029715.png" alt=""></p>
<p>左边的图是普通神经元的计算示意，而右边是一个胶囊内部的计算示意图。 神经元这里不过多解释，这里主要是剖析右边的这个胶囊计算原理。从上图可以看出， 输入是两个向量$v_1,v_2$，首先经过了一个线性映射，得到了两个新向量$u_1,u_2$，然后呢，经过了一个向量的加权汇总，这里的$c_1$,$c_2$可以先理解成权重，具体计算后面会解释。 得到汇总后的向量$s$，接下来进行了Squash操作，整体的计算公式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212030357.png" alt=""></p>
<p>这里的Squash操作可以简单看下，主要包括两部分，右边的那部分其实就是向量归一化操作，把norm弄成1，而左边那部分算是一个非线性操作，如果$s$的norm很大，那么这个整体就接近1， 而如果这个norm很小，那么整体就会接近0， 和sigmoid很像有没有？</p>
<p>这样就完成了一个胶囊的计算，但有两点需要注意：</p>
<ol>
<li>这里的$W^i$参数是可学习的，和神经网络一样， 通过BP算法更新</li>
<li>这里的$c_i$参数不是BP算法学习出来的，而是采用动态路由机制现场算出来的，这个非常类似于pooling层，我们知道pooling层的参数也不是学习的，而是根据前面的输入现场取最大或者平均计算得到的。</li>
</ol>
<p>所以这里的问题，就是怎么通过动态路由机制得到$c_i$，下面是动态路由机制的过程。</p>
<h6 id="动态路由机制原理"><a href="#动态路由机制原理" class="headerlink" title="动态路由机制原理"></a>动态路由机制原理</h6><p>我们先来一个胶囊结构: </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212035441.png" alt=""></p>
<p>这个$c_i$是通过动态路由机制计算得到，那么动态路由机制究竟是啥子意思？  其实就是通过迭代的方式去计算，没有啥神秘的，迭代计算的流程如下图:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212035613.png" alt=""></p>
<p>首先我们先初始化$b_i$，与每一个输入胶囊$u_i$进行对应，这哥们有个名字叫做”routing logit”， 表示的是输出的这个胶囊与输入胶囊的相关性，和注意力机制里面的score值非常像。由于一开始不知道这个哪个胶囊与输出的胶囊有关系，所以默认相关性分数都一样，然后进入迭代。</p>
<p>在每一次迭代中，首先把分数转成权重，然后加权求和得到$s$，这个很类似于注意力机制的步骤，得到$s$之后，通过归一化操作，得到$a$，接下来要通过$a$和输入胶囊的相关性以及上一轮的$b_i$来更新$b_i$。</p>
<p>通过若干次迭代之后，得到最后的输出胶囊向量$a$会慢慢的走到与它更相关的那些$u$附近，而远离那些与它不相干的$u$。所以上面的这个迭代过程有点像<strong>排除异常输入胶囊的感觉</strong>。</p>
<p>而从另一个角度来考虑，这个过程其实像是聚类的过程，因为胶囊的输出向量$v$经过若干次迭代之后，会最终停留到与其非常相关的那些输入胶囊里面，而这些输入胶囊，其实就可以看成是某个类别了，因为既然都共同的和输出胶囊$v$比较相关，那么彼此之间的相关性也比较大，于是乎，经过这样一个动态路由机制之后，就不自觉的，把输入胶囊实现了聚类。把和与其他输入胶囊不同的那些胶囊给排除了出去。</p>
<p>所以，这个动态路由机制的计算设计的还是比较巧妙的， 下面是上述过程的展开计算过程， 这个和RNN的计算有点类似：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212037262.png" alt=""></p>
<p>这样就完成了一个胶囊内部的计算过程了。</p>
<p>Ok， 有了上面的这些铺垫，再来看MIND就会比较简单了。下面正式对MIND模型的网络架构剖析。</p>
<h5 id="MIND模型的网络结构与细节剖析"><a href="#MIND模型的网络结构与细节剖析" class="headerlink" title="MIND模型的网络结构与细节剖析"></a>MIND模型的网络结构与细节剖析</h5><h6 id="网络整体结构"><a href="#网络整体结构" class="headerlink" title="网络整体结构"></a>网络整体结构</h6><p>MIND网络的架构如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212044848.png" alt=""></p>
<p>初步先分析这个网络结构的运作： 首先接收的输入有三类特征，用户base属性，历史行为属性以及商品的属性，用户的历史行为序列属性过了一个多兴趣提取层得到了多个兴趣胶囊，接下来和用户base属性拼接过DNN，得到了交互之后的用户兴趣。然后在训练阶段，用户兴趣和当前商品向量过一个label-aware attention，然后求softmax损失。 在服务阶段，得到用户的向量之后，就可以直接进行近邻检索，找候选商品了。 这就是宏观过程，但是，多兴趣提取层以及这个label-aware attention是在做什么事情呢？  如果单独看这个图，感觉得到多个兴趣胶囊之后，直接把这些兴趣胶囊以及用户的base属性拼接过全连接，那最终不就成了一个用户向量，此时label-aware attention的意义不就没了？ 所以这个图初步感觉画的有问题，和论文里面描述的不符。所以下面先以论文为主，正式开始描述具体细节。</p>
<h6 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h6><p>召回任务的目标是对于每一个用户$u \in \mathcal{U}$从十亿规模的物品池$\mathcal{I}$检索出包含与用户兴趣相关的上千个物品集。</p>
<p><strong>模型的输入</strong></p>
<p>对于模型，每个样本的输入可以表示为一个三元组：$\left(\mathcal{I}_{u}, \mathcal{P}_{u}, \mathcal{F}_{i}\right)$，其中$\mathcal{I}_{u}$代表与用户$u$交互过的物品集，即用户的历史行为；$\mathcal{P}_{u}$表示用户的属性，例如性别、年龄等；$\mathcal{F}_{i}$定义为目标物品$i$的一些特征，例如物品id和种类id等。</p>
<p><strong>任务描述</strong></p>
<p>MIND的核心任务是学习一个从原生特征映射到<strong>用户表示</strong>的函数，用户表示定义为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212048618.png" alt=""></p>
<p>其中，$\mathbf{V}_{u}=\left(\overrightarrow{\boldsymbol{v}}_{u}^{1}, \ldots, \overrightarrow{\boldsymbol{v}}_{u}^{K}\right) \in \mathbb{R}^{d \times k}$是用户$u$的表示向量，$d$是embedding的维度，$K$表示向量的个数，即兴趣的数量。如果$K=1$，那么MIND模型就退化成YouTubeDNN的向量表示方式了。</p>
<p>目标物品$i$的embedding函数为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212049445.png" alt=""></p>
<p>其中，$\overrightarrow{\mathbf{e}}_{i} \in \mathbb{R}^{d \times 1}, \quad f_{i t e m}(\cdot)$表示一个embedding&amp;pooling层。</p>
<p><strong>最终结果</strong></p>
<p>根据评分函数检索（根据<strong>目标物品与用户表示向量的内积的最大值作为相似度依据</strong>，DIN的Attention部分也是以这种方式来衡量两者的相似度），得到top-N个候选项：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212049720.png" alt=""></p>
<h6 id="Embedding-amp-Pooling层"><a href="#Embedding-amp-Pooling层" class="headerlink" title="Embedding &amp; Pooling层"></a>Embedding &amp; Pooling层</h6><p>Embedding层的输入由三部分组成，用户属性$\mathcal{P}_{u}$、用户行为$\mathcal{I}_{u}$和目标物品标签$\mathcal{F}_{i}$。每一部分都由多个id特征组成，则是一个高维的稀疏数据，因此需要Embedding技术将其映射为低维密集向量。具体来说，</p>
<ul>
<li>对于$\mathcal{P}_{u}$的id特征（年龄、性别等）是将其Embedding的向量进行Concat，组成用户属性Embedding$\overrightarrow{\mathbf{p}}_{u}$；</li>
<li>目标物品$\mathcal{F}_{i}$通常包含其他分类特征id（品牌id、店铺id等） ，这些特征有利于物品的冷启动问题，需要将所有的分类特征的Embedding向量进行平均池化，得到一个目标物品向量$\overrightarrow{\mathbf{e}}_{i}$；</li>
<li>对于用户行为$\mathcal{I}_{u}$，由物品的Embedding向量组成用户行为Embedding列表$E_{u}=\overrightarrow{\mathbf{e}}_{j}, j \in \mathcal{I}_{u}$， 当然这里不仅只有物品embedding哈，也可能有类别，品牌等其他的embedding信息。</li>
</ul>
<h6 id="Multi-Interest-Extractor-Layer-核心"><a href="#Multi-Interest-Extractor-Layer-核心" class="headerlink" title="Multi-Interest Extractor Layer(核心)"></a>Multi-Interest Extractor Layer(核心)</h6><p>作者认为，单一的向量不足以表达用户的多兴趣。所以作者采用<strong>多个表示向量</strong>来分别表示用户不同的兴趣。通过这个方式，在召回阶段，用户的多兴趣可以分别考虑，对于兴趣的每一个方面，能够更精确的进行物品检索。</p>
<p>为了学习多兴趣表示，作者利用胶囊网络表示学习的动态路由将用户的历史行为分组到多个簇中。来自一个簇的物品应该密切相关，并共同代表用户兴趣的一个特定方面。</p>
<p>由于多兴趣提取器层的设计灵感来自于胶囊网络表示学习的动态路由，所以这里作者回顾了动态路由机制。当然，如果之前对胶囊网络或动态路由不了解，这里读起来就会有点艰难，但由于我上面进行了铺垫，这里就直接拿过原文并解释即可。</p>
<p><strong>动态路由</strong></p>
<p>动态路由是胶囊网络中的迭代学习算法，用于学习低水平胶囊和高水平胶囊之间的路由对数（logit）$b_{ij}$，来得到高水平胶囊的表示。</p>
<p>我们假设胶囊网络有两层，即低水平胶囊$\vec{c}_{i}^{l} \in \mathbb{R}^{N_{l} \times 1}, i \in\{1, \ldots, m\}$和高水平胶囊$\vec{c}_{j}^{h} \in \mathbb{R}^{N_{h} \times 1}, j \in\{1, \ldots, n\}$，其中$m,n$表示胶囊的个数， $N_l,N_h$表示胶囊的维度。 路由对数$b_{ij}$计算公式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212050370.png" alt=""></p>
<p>其中$\mathbf{S}_{i j} \in \mathbb{R}^{N_{h} \times N_{l}}$表示待学习的双线性映射矩阵【在胶囊网络的原文中称为转换矩阵】</p>
<p>通过计算路由对数，将高阶胶囊$j$的候选向量计算为所有低阶胶囊的加权和：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212050471.png" alt=""></p>
<p>其中$w_{ij}$定义为连接低阶胶囊$i$和高阶胶囊$j$的权重【称为耦合系数】，而且其通过对路由对数执行softmax来计算：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212051995.png" alt=""></p>
<p>最后，应用一个非线性的“压缩”函数来获得一个高阶胶囊的向量【胶囊网络向量的模表示由胶囊所代表的实体存在的概率】</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212051641.png" alt=""></p>
<p>路由过程重复进行3次达到收敛。当路由结束，高阶胶囊值$\vec{c}_{j}^{h}$固定，作为下一层的输入。</p>
<p>Ok，下面我们开始解释，其实上面说的这些就是胶囊网络的计算过程，只不过和之前所用的符号不一样了。这里拿个图：<br><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212051116.png" alt=""><br>首先，论文里面也是个两层的胶囊网络，低水平层-&gt;高水平层。 低水平层有$m$个胶囊，每个胶囊向量维度是$N_l$，用$\vec{c}_{i}^l$表示的，高水平层有$n$个胶囊，每个胶囊$N_h$维，用$\vec{c}_{j}^h$表示。</p>
<p>单独拿出每个$\vec{c}_{j}^h$，其计算过程如上图所示。首先，先随机初始化路由对数$b_{ij}=0$，然后开始迭代，对于每次迭代：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212052434.png" alt=""></p>
<p>只不过这里的符合和上图中的不太一样，这里的$w_{ij}$对应的是每个输入胶囊的权重$c_{ij}$， 这里的$\vec{c}_{j}^h$对应上图中的$a$， 这里的$\vec{z}_{j}^h$对应的是输入胶囊的加权组合。这里的$\vec{c}_{i}^l$对应上图中的$v_i$，这里的$S_{ij}$对应的是上图的权重$W_{ij}$，只不过这个可以换成矩阵运算。 和上图中不同的是路由对数$b_{ij}$更新那里，没有了上一层的路由对数值，但感觉这样会有问题。</p>
<p>所以，这样解释完之后就会发现，其实上面的一顿操作就是说的传统的动态路由机制。</p>
<p><strong>B2I动态路由</strong></p>
<p>作者设计的多兴趣提取层就是就是受到了上述胶囊网络的启发。</p>
<p>如果把用户的行为序列看成是行为胶囊， 把用户的多兴趣看成兴趣胶囊，那么多兴趣提取层就是利用动态路由机制学习行为胶囊<code>-&gt;</code>兴趣胶囊的映射关系。但是原始路由算法无法直接应用于处理用户行为数据。因此，提出了<strong>行为(Behavior)到兴趣(Interest)（B2I）动态路由</strong>来自适应地将用户的行为聚合到兴趣表示向量中，它与原始路由算法有三个不同之处：</p>
<ol>
<li><strong>共享双向映射矩阵</strong>。在初始动态路由中，使用固定的或者说共享的双线性映射矩阵$S$而不是单独的双线性映射矩阵， 在原始的动态路由中，对于每个输出胶囊$\vec{c}_{j}^h$，都会有对应的$S_{ij}$，而这里是每个输出胶囊，都共用一个$S$矩阵。 原因有两个：<ol>
<li>一方面，用户行为是可变长度的，从几十个到几百个不等，因此使用共享的双线性映射矩阵是有利于泛化。</li>
<li>另一方面，希望兴趣胶囊在同一个向量空间中，但不同的双线性映射矩阵将兴趣胶囊映射到不同的向量空间中。因为映射矩阵的作用就是对用户的行为胶囊进行线性映射嘛， 由于用户的行为序列都是商品，所以希望经过映射之后，到统一的商品向量空间中去。路由对数计算如下：</li>
</ol>
</li>
</ol>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212052546.png" alt=""></p>
<p>​        其中，$\overrightarrow{\boldsymbol{e}}_{i} \in \mathbb{R}^{d}$是历史物品$i$的embedding，$\vec{u}_{j} \in \mathbb{R}^{d}$表示兴趣胶囊$j$的向量。$S \in \mathbb{R}^{d \times d}$是每一对行为胶囊(低价)到兴趣胶囊(高阶)之间        的共享映射矩阵。</p>
<ol>
<li><strong>随机初始化路由对数</strong>。由于利用共享双向映射矩阵$S$，如果再初始化路由对数为0将导致相同的初始的兴趣胶囊。随后的迭代将陷入到一个不同兴趣胶囊在所有的时间保持相同的情景。因为每个输出胶囊的运算都一样了嘛(除非迭代的次数不同，但这样也会导致兴趣胶囊都很类似)，为了减轻这种现象，作者通过高斯分布进行随机采样来初始化路由对数$b_{ij}$，让初始兴趣胶囊与其他每一个不同，其实就是希望在计算每个输出胶囊的时候，通过随机化的方式，希望这几个聚类中心离得远一点，这样才能表示出广泛的用户兴趣(我们已经了解这个机制就仿佛是聚类，而计算过程就是寻找聚类中心)。</li>
<li><strong>动态的兴趣数量</strong>，兴趣数量就是聚类中心的个数，由于不同用户的历史行为序列不同，那么相应的，其兴趣胶囊有可能也不一样多，所以这里使用了一种启发式方式自适应调整聚类中心的数量，即$K$值。</li>
</ol>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212053113.png" alt=""></p>
<p>这种调整兴趣胶囊数量的策略可以为兴趣较小的用户节省一些资源，包括计算和内存资源。这个公式不用多解释，与行为序列长度成正比。</p>
<p>最终的B2I动态路由算法如下：<br><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212053253.png" alt=""><br>应该很好理解了吧。</p>
<h6 id="Label-aware-Attention-Layer"><a href="#Label-aware-Attention-Layer" class="headerlink" title="Label-aware Attention Layer"></a>Label-aware Attention Layer</h6><p> 通过多兴趣提取器层，从用户的行为embedding中生成多个兴趣胶囊。不同的兴趣胶囊代表用户兴趣的不同方面，相应的兴趣胶囊用于评估用户对特定类别的偏好。所以，在训练的期间，最后需要设置一个Label-aware的注意力层，对于当前的商品，根据相关性选择最相关的兴趣胶囊。这里其实就是一个普通的注意力机制，和DIN里面的那个注意力层基本上是一模一样，计算公式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212054677.png" alt=""></p>
<p>首先这里的$\overrightarrow{\boldsymbol{e}}_{i}$表示当前的商品向量，$V_u$表示用户的多兴趣向量组合，里面有$K$个向量，表示用户的$K$的兴趣。用户的各个兴趣向量与目标商品做内积，然后softmax转成权重，然后反乘到多个兴趣向量进行加权求和。 但是这里需要注意的一个小点，就是这里做内积求完相似性之后，先做了一个指数操作，<strong>这个操作其实能放大或缩小相似程度</strong>，至于放大或者缩小的程度，由$p$控制。 比如某个兴趣向量与当前商品非常相似，那么再进行指数操作之后，如果$p$也很大，那么显然这个兴趣向量就占了主导作用。$p$是一个可调节的参数来调整注意力分布。当$p$接近0，每一个兴趣胶囊都得到相同的关注。当$p$大于1时，随着$p$的增加，具有较大值的点积将获得越来越多的权重。考虑极限情况，当$p$趋近于无穷大时，注意机制就变成了一种硬注意，选关注最大的值而忽略其他值。在实验中，发现使用硬注意导致更快的收敛。</p>
<h6 id="训练与服务"><a href="#训练与服务" class="headerlink" title="训练与服务"></a>训练与服务</h6><p>得到用户向量$\overrightarrow{\boldsymbol{v}}_{u}$和标签物品embedding$\vec{e}_{i}$后，计算用户$u$与标签物品$i$交互的概率：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212054847.png" alt=""></p>
<p>目标函数是：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212054640.png" alt=""></p>
<p>其中$\mathcal{D}$是训练数据包含用户物品交互的集合。因为物品的数量可伸缩到数十亿，所以不能直接算。因此。使用采样的softmax技术，并且选择Adam优化来训练MIND。</p>
<p>训练结束后，抛开label-aware注意力层，MIND网络得到一个用户表示映射函数$f_{user}$。在服务期间，用户的历史序列与自身属性喂入到$f_{user}$，每个用户得到多兴趣向量。然后这个表示向量通过一个近似邻近方法来检索top N物品。</p>
<p>这就是整个MIND模型的细节了。</p>
<h4 id="SDM"><a href="#SDM" class="headerlink" title="SDM"></a>SDM</h4><p>SDM模型(Sequential Deep Matching Model)，是阿里团队在2019年CIKM上的一篇paper。和MIND模型一样，是一种序列召回模型，研究的依然是如何通过用户的历史行为序列去学习到用户的丰富兴趣。 对于MIND，我们已经知道是基于胶囊网络的动态路由机制，设计了一个动态兴趣提取层，把用户的行为序列通过路由机制聚类，然后映射成了多个兴趣胶囊，以此来获取到用户的广泛兴趣。而SDM模型，是先把用户的历史序列根据交互的时间分成了短期和长期两类，然后从<strong>短期会话</strong>和<strong>长期行为</strong>中分别采取<strong>相应的措施（短期的RNN+多头注意力，长期的Att Net）</strong> 去学习到用户的短期兴趣和长期行为偏好，并<strong>巧妙的设计了一个门控网络有选择的将长短期兴趣进行融合</strong>，以此得到用户的最终兴趣向量。 这篇paper中的一些亮点，比如长期偏好的行为表示，多头注意力机制学习多兴趣，长短期兴趣的融合机制等，又给了一些看待问题的新角度，同时，给出了我们一种利用历史行为序列去捕捉用户动态偏好的新思路。 </p>
<p>这篇paper依然是从引言开始， 介绍SDM模型提出的动机以及目前方法存在的不足(why)， 接下来就是SDM的网络模型架构(what)， 这里面的关键是如何从短期会话和长期行为两个方面学习到用户的短期长期偏好(how)。</p>
<h5 id="背景与动机-1"><a href="#背景与动机-1" class="headerlink" title="背景与动机"></a>背景与动机</h5><p> 这里要介绍该模型提出的动机，即why要有这样的一个模型？</p>
<p>一个好的推荐系统应该是能精确的捕捉用户兴趣偏好以及能对他们当前需求进行快速响应的，往往工业上的推荐系统，为了能快速响应， 一般会把整个推荐流程分成召回和排序两个阶段，先通过召回，从海量商品中得到一个小的候选集，然后再给到排序模型做精确的筛选操作。 这也是目前推荐系统的一个范式了。在这个过程中，召回模块所检索到的候选对象的质量在整个系统中起着至关重要的作用。</p>
<p>淘宝目前的召回模型是一些基于协同过滤的模型， 这些模型是通过用户与商品的历史交互建模，从而得到用户的物品的表示向量，但这个过程是<strong>静态的</strong>，而用户的行为或者兴趣是时刻变化的， 对于协同过滤的模型来说，并不能很好的捕捉到用户整个行为序列的动态变化。</p>
<p>那我们知道了学习用户历史行为序列很重要， 那么假设序列很长呢？这时候直接用模型学习长序列之间的演进可能不是很好，因为很长的序列里面可能用户的兴趣发生过很大的转变，很多商品压根就没有啥关系，这样硬学，反而会导致越学越乱，就别提这个演进了。所以这里是以会话为单位，对长序列进行切分。作者这里的依据就是用户在同一个Session下，其需求往往是很明确的， 这时候，交互的商品也往往都非常类似。 但是Session与Session之间，可能需求改变，那么商品类型可能骤变。 所以以Session为单位来学习商品之间的序列信息，感觉要比整个长序列学习来的靠谱。 </p>
<p>作者首先是先把长序列分成了多个会话， 然后<strong>把最近的一次会话，和之前的会话分别视为了用户短期行为和长期行为分别进行了建模，并采用不同的措施学习用户的短期兴趣和长期兴趣，然后通过一个门控机制融合得到用户最终的表示向量</strong>。这就是SDM在做的事情，</p>
<p>长短期行为序列联合建模，其实是在给我们提供一种新的学习用户兴趣的新思路， 那么究竟是怎么做的呢？以及为啥这么做呢？</p>
<ul>
<li><p>对于短期用户行为， 首先作者使用了LSTM来学习序列关系， 而接下来是用一个Multi-head attention机制，学习用户的多兴趣。 </p>
<p>先分析分析作者为啥用多头注意力机制，作者这里依然是基于实际的场景出发，作者发现，<strong>用户的兴趣点在一个会话里面其实也是多重的</strong>。这个可能之前的很多模型也是没考虑到的，但在商品购买的场景中，这确实也是个事实， 顾客在买一个商品的时候，往往会进行多方比较， 考虑品牌，颜色，商店等各种因素。作者认为用普通的注意力机制是无法反映广泛的兴趣了，所以用多头注意力网络。 </p>
<p>多头注意力机制从某个角度去看，也有类似聚类的功效，首先它接收了用户的行为序列，然后从多个角度学习到每个商品与其他商品的相关性，然后根据与其他商品的相关性加权融合，这样，相似的item向量大概率就融合到了一块组成一个向量，所谓用户的多兴趣，可能是因为这些行为商品之间，可以从多个空间或者角度去get彼此之间的相关性，这里面有着用户多兴趣的表达信息。</p>
</li>
<li><p>用户的长期行为也会影响当前的决策，作者在这里举了一个NBA粉丝的例子，说如果一个是某个NBA球星的粉丝，那么他可能在之前会买很多有关这个球星的商品，如果现在这个时刻想买鞋的时候，大概率会考虑和球星相关的。所以作者说<strong>长期偏好和短期行为都非常关键</strong>。但是长期偏好或者行为往往是复杂广泛的，就像刚才这个例子里面，可能长期行为里面，买的与这个球星相关商品只占一小部分，而就只有这一小部分对当前决策有用。<br>这个也是之前的模型利用长期偏好方面存在的问题，那么如何选择出长期偏好里面对于当前决策有用的那部分呢？  作者这里设计了一个门控的方式融合短期和长期，这个想法还是很巧妙的，后面介绍这个东西的时候说下我的想法。 </p>
</li>
</ul>
<p>所以下面总结动机以及本篇论文的亮点：</p>
<ul>
<li>动机： 召回模型需要捕获用户的动态兴趣变化，这个过程中利用好用户的长期行为和短期偏好非常关键，而以往的模型有下面几点不足：<ul>
<li>协同过滤模型： 基于用户的交互进行静态建模，无法感知用户的兴趣变化过程，易召回同质性的商品</li>
<li>早期的一些序列推荐模型: 要么是对整个长序列直接建模，但这样太暴力，没法很好的学习商品之间的序列信息，有些是把长序列分成会话，但忽视了一个会话中用户的多重兴趣</li>
<li>有些方法在考虑用户的长期行为方面，只是简单的拼接或者加权求和，而实际上用户长期行为中只有很少一小部分对当前的预测有用，这样暴力融合反而会适得其反，起不到效果。另外还有一些多任务或者对抗方法， 在工业场景中不适用等。 </li>
<li>这些我只是通过我的理解简单总结，详细内容看原论文相关工作部分。</li>
</ul>
</li>
<li>亮点: <ul>
<li>SDM模型， 考虑了用户的短期行为和长期兴趣，以会话的形式进行分割，并对这两方面分别建模</li>
<li>短期会话由于对当前决策影响比较大，那么我们就学习的全面一点， 首先RNN学习序列关系，其次通过多头注意力机制捕捉多兴趣，然后通过一个Attention Net加权得到短期兴趣表示</li>
<li>长期会话通过Attention Net融合，然后过DNN，得到用户的长期表示</li>
<li>我们设计了一个门控机制，类似于LSTM的那种门控，能巧妙的融合这两种兴趣，得到用户最终的表示向量</li>
</ul>
</li>
</ul>
<p>这就是动机与背景总结啦。 那么接下来，SDM究竟是如何学习短期和长期表示，又是如何融合的？ 为什么要这么玩？</p>
<h5 id="SDM的网络结构与细节剖析"><a href="#SDM的网络结构与细节剖析" class="headerlink" title="SDM的网络结构与细节剖析"></a>SDM的网络结构与细节剖析</h5><h6 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h6><p>这里本来直接看模型结构，但感觉还是先过一下问题定义吧，毕竟这次涉及到了会话，还有几个小规则。</p>
<p>$\mathcal{U}$表示用户集合，$\mathcal{I}$表示item集合，模型考虑在时间$t$，是否用户$u$会对$i$产生交互。 对于$u$， 我们能够得到它的历史行为序列，那么先说一下如何进行会话的划分， 这里有三个规则：</p>
<ol>
<li>相同会话ID的商品(后台能获取)算是一个会话</li>
<li>相邻的商品，时间间隔小于10分钟(业务自己调整)算一个会话</li>
<li>同一个会话中的商品不能超过50个，多出来的放入下一个会话</li>
</ol>
<p>这样划分开会话之后， 对于用户$u$的短期行为定义是离目前最近的这次会话， 用$\mathcal{S}^{u}=\left[i_{1}^{u}, \ldots, i_{t}^{u}, \ldots, i_{m}^{u}\right]$表示，$m$是序列长度。 而长期的用户行为是过去一周内的会话，但不包括短期的这次会话， 这个用$\mathcal{L}^{u}$表示。网络推荐架构如下：<br><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212059483.png" alt=""><br>这个感觉并不用过多解释。看过召回的应该都能懂， 接收了用户的短期行为和长期行为，然后分别通过两个盲盒得到表示向量，再通过门控融合就得到了最终的用户表示。 </p>
<p>下面要开那三个盲盒操作，即短期行为学习，长期行为学习以及门控融合机制。但在这之前，得先说一个东西，就是输入层这里， 要带物品的side infomation，比如物品的item ID, 物品的品牌ID，商铺ID， 类别ID等等， 那你说，为啥要单独说呢？ 之前的模型不也有，但是这里在利用方式上有些不一样需要注意。</p>
<h6 id="Input-Embedding-with-side-Information"><a href="#Input-Embedding-with-side-Information" class="headerlink" title="Input Embedding with side Information"></a>Input Embedding with side Information</h6><p>在淘宝的推荐场景中，作者发现， 顾客与物品产生交互行为的时候，不仅考虑特定的商品本身，还考虑产品， 商铺，价格等，这个显然。所以，这里对于一个商品来说，不仅要用到Item ID，还用了更多的side info信息，包括<code>leat category, fist level category, brand,shop</code>。 </p>
<p>所以，假设用户的短期行为是$\mathcal{S}^{u}=\left [i_{1}^{u},\cdots,i_{t}^{u}, \cdots,i_{m}^{u}\right]$， 这里面的每个商品$i_t^u$其实有5个属性表示了，每个属性本质是ID，但转成embedding之后，就得到了5个embedding， 所以这里就涉及到了融合问题。 这里用 $\boldsymbol{e}_{i^{u}_{t}} \in \mathbb{R}^{d \times 1}$ 来表示每个$i_t^u$，但这里不是embedding的pooling操作，而是Concat</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212101984.png" alt=""></p>
<p>其中，$\boldsymbol{e}_{i}^{f}=\boldsymbol{W}^{f} \boldsymbol{x}_{i}^{f} \in \mathbb{R}^{d_{f} \times 1}$， 这个公式看着负责，其实就是每个side info的id过embedding layer得到各自的embedding。这里embedding的维度是$d_f$， 等拼接起来之后，就是$d$维了。</p>
<p>另外就是用户的base表示向量了，这个很简单， 就是用户的基础画像，得到embedding，直接也是Concat，这个常规操作不解释：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212101121.png" alt=""></p>
<p>$e_u^p$是特征$p$的embedding。</p>
<p>Ok，输入这里说完了之后，就直接开盲盒， 不按照论文里面的顺序来了。想看更多细节的就去看原论文吧，感觉那里面说的有些啰嗦。不如直接上图解释来的明显：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212101061.png" alt=""><br>这里正好三个框把盒子框住了，下面剖析出每个来就行啦。</p>
<h6 id="短期用户行为建模"><a href="#短期用户行为建模" class="headerlink" title="短期用户行为建模"></a>短期用户行为建模</h6><p>这里短期用户行为是下面的那个框，接收的输入，首先是用户最近的那次会话，里面各个商品加入了side info信息之后，有了最终的embedding表示$\left[\boldsymbol{e}_{i_{1}^{u}}, \ldots, \boldsymbol{e}_{i_{t}^{u}}\right]$。 </p>
<p>这个东西，首先要过LSTM，学习序列信息，这个感觉不用多说，直接上公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212103361.png" alt=""></p>
<p>这里采用的是多输入多输出， 即每个时间步都会有一个隐藏状态$h_t^u$输出出来，那么经过LSTM之后，原始的序列就有了序列相关信息，得到了$\left[\boldsymbol{h}_{1}^{u}, \ldots, \boldsymbol{h}_{t}^{u}\right]$, 把这个记为$\boldsymbol{X}^{u}$。这里的$\boldsymbol{h}_{t}^{u} \in \mathbb{R}^{d \times 1}$表示时间$t$的序列偏好表示。</p>
<p>接下来， 这个东西要过Multi-head self-attention层，这个东西的原理我这里就不多讲了，这个东西可以学习到$h_i^u$系列之间的相关性，这个操作从某种角度看，也很像聚类， 因为我们这里是先用多头矩阵把$h_i^u$系列映射到多个空间，然后从各个空间中互求相关性</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212104750.png" alt=""></p>
<p>得到权重后，对原始的向量加权融合。 让$Q_{i}^{u}=W_{i}^{Q} X^{u}$， $K_{i}^{u}=W_{i}^{K} \boldsymbol{X}^{u}$，$V_{i}^{u}=W_{i}^{V} X^{u}$， 背后计算是：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212104358.png" alt=""></p>
<p>这是一个头的计算， 接下来每个头都这么算，假设有$h$个头，这里会通过上面的映射矩阵$W$系列，先把原始的$h_i^u$向量映射到$d_{k}=\frac{1}{h} d$维度，然后计算$head_i^u$也是$d_k$维，这样$h$个head进行拼接，正好是$d$维， 接下来过一个全连接或者线性映射得到MultiHead的输出。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212104590.png" alt=""></p>
<p>这样就相当于更相似的$h_i^u$融合到了一块，而这个更相似又是从多个角度得到的，于是乎， 作者认为，这样就能学习到用户的多兴趣。</p>
<p>得到这个东西之后，接下来再过一个User Attention， 因为作者发现，对于相似历史行为的不同用户，其兴趣偏好也不太一样。<br>所以加入这个用户Attention层，想挖掘更细粒度的用户个性化信息。 当然，这个就是普通的embedding层了， 用户的base向量$e_u$作为query，与$\hat{X}^{u}$的每个向量做Attention，然后加权求和得最终向量：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212105264.png" alt=""></p>
<p>其中$s_{t}^{u} \in \mathbb{R}^{d \times 1}$，这样短期行为兴趣就修成了正果。</p>
<h6 id="用户长期行为建模"><a href="#用户长期行为建模" class="headerlink" title="用户长期行为建模"></a>用户长期行为建模</h6><p>从长期的视角来看，用户在不同的维度上可能积累了广泛的兴趣，用户可能经常访问一组类似的商店，并反复购买属于同一类别的商品。 所以长期行为$\mathcal{L}^{u}$来自于不同的特征尺度。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212105990.png" alt=""></p>
<p>这里面包含了各种side特征。这里就和短期行为那里不太一样了，长期行为这里，是从特征的维度进行聚合，也就是把用户的历史长序列分成了多个特征，比如用户历史点击过的商品，历史逛过的店铺，历史看过的商品的类别，品牌等，分成了多个特征子集，然后这每个特征子集里面有对应的id，比如商品有商品id, 店铺有店铺id等，对于每个子集，过user Attention layer，和用户的base向量求Attention， 相当于看看用户喜欢逛啥样的商店， 喜欢啥样的品牌，啥样的商品类别等等，得到每个子集最终的表示向量。每个子集的计算过程如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212105910.png" alt=""></p>
<p>每个子集都会得到一个加权的向量，把这个东西拼起来，然后过DNN。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212106394.png" alt=""></p>
<p>这里的$\boldsymbol{p}^{u} \in \mathbb{R}^{d \times 1}$， 这样就得到了用户的长期兴趣表示。</p>
<h6 id="短长期兴趣融合"><a href="#短长期兴趣融合" class="headerlink" title="短长期兴趣融合"></a>短长期兴趣融合</h6><p>长短期兴趣融合这里，作者发现之前模型往往喜欢直接拼接起来，或者加和，注意力加权等，但作者认为这样不能很好的将两类兴趣融合起来，因为长期序列里面，其实只有很少的一部分行为和当前有关。那么这样的话，直接无脑融合是有问题的。所以这里作者用了一种较为巧妙的方式，即门控机制：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305212106342.png" alt=""></p>
<p>这个和LSTM的这种门控机制很像，首先门控接收的输入有用户画像$e_u$，用户短期兴趣$s_t^u$， 用户长期兴趣$p^u$，经过sigmoid函数得到了$G_{t}^{u} \in \mathbb{R}^{d \times 1}$，用来决定在$t$时刻短期和长期兴趣的贡献程度。然后根据这个贡献程度对短期和长期偏好加权进行融合。</p>
<p>为啥这东西就有用了呢？  实验中证明了这个东西有用，但这里给出我的理解哈，我们知道最终得到的短期或者长期兴趣都是$d$维的向量， 每一个维度可能代表着不同的兴趣偏好，比如第一维度代表品牌，第二个维度代表类别，第三个维度代表价格，第四个维度代表商店等等，当然假设哈，真实的向量不可解释。</p>
<p>那么如果我们是直接相加或者是加权相加，其实都意味着长短期兴趣这每个维度都有很高的保留， 但其实上，万一长期兴趣和短期兴趣维度冲突了呢？ 比如短期兴趣里面可能用户喜欢这个品牌，长期用户里面用户喜欢那个品牌，那么听谁的？ 你可能说短期兴趣这个占更大权重呗，那么普通加权可是所有向量都加的相同的权重，品牌这个维度听短期兴趣的，其他维度比如价格，商店也都听短期兴趣的？本身存在不合理性。那么反而直接相加或者加权效果会不好。</p>
<p>而门控机制的巧妙就在于，我会给每个维度都学习到一个权重，而这个权重非0即1(近似哈)， 那么接下来融合的时候，我通过这个门控机制，取长期和短期兴趣向量每个维度上的其中一个。比如在品牌方面听谁的，类别方面听谁的，价格方面听谁的，只会听短期和长期兴趣的其中一个的。这样就不会有冲突发生，而至于具体听谁的，交给网络自己学习。这样就使得用户长期兴趣和短期兴趣融合的时候，每个维度上的信息保留变得<strong>有选择</strong>。使得兴趣的融合方式更加的灵活。</p>
<p> <strong>这其实又给我们提供了一种两个向量融合的一种新思路，并不一定非得加权或者拼接或者相加了，还可以通过门控机制让网络自己学</strong></p>
<h3 id="基于树模型的召回"><a href="#基于树模型的召回" class="headerlink" title="基于树模型的召回"></a>基于树模型的召回</h3><h4 id="TDM"><a href="#TDM" class="headerlink" title="TDM"></a>TDM</h4><p>召回早前经历的第一代协同过滤技术，让模型可以在数量级巨大的item集中找到用户潜在想要看到的商品。这种方式有很明显的缺点，一个是对于用户而言，只能通过他历史行为去构建候选集，并且会基于算力的局限做截断。所以推荐结果的多样性和新颖性比较局限，导致推荐的有可能都是用户看过的或者买过的商品。之后在Facebook开源了FASSI库之后，基于内积模型的向量检索方案得到了广泛应用，也就是第二代召回技术。这种技术通过将用户和物品用向量表示，然后用内积的大小度量兴趣，借助向量索引实现大规模的全量检索。这里虽然改善了第一代的无法全局检索的缺点，然而这种模式下存在索引构建和模型优化目标不一致的问题，索引优化是基于向量的近似误差，而召回问题的目标是最大化topK召回率。且这类方法也不方便在用户和物品之间做特征组合。</p>
<p>所以阿里开发了一种可以承载各种深度模型来检索用户潜在兴趣的推荐算法解决方案。这个TDM模型是基于树结构，利用树结构对全量商品进行检索，将复杂度由O(N)下降到O(logN)。</p>
<h5 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h5><p><strong>树结构</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211740339.png" alt=""></p>
<p>如上图，树中的每一个叶子节点对应一个商品item，非叶子结点表示的是item的集合<strong>（这里的树不限于二叉树）</strong>。这种层次化结构体现了粒度从粗到细的item架构。</p>
<p><strong>整体结构</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211741089.png" alt=""></p>
<h5 id="算法详解"><a href="#算法详解" class="headerlink" title="算法详解"></a>算法详解</h5><ol>
<li><p>基于树的高效检索</p>
<p>算法通常采用beam-search的方法，根据用户对每层节点挑选出topK，将挑选出来的这几个topK节点的子节点作为下一层的候选集，最终会落到叶子节点上。<br>这么做的理论依据是当前层的最有优topK节点的父亲必然属于上次的父辈节点的最优topK：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211743079.png" alt=""></p>
<p>其中$p^{(j)}(n|u)$表示用户u对j层节点n感兴趣的概率，$\alpha^{j}$表示归一化因子。</p>
</li>
<li><p>对兴趣进行建模</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211744832.png" alt=""></p>
<p>如上图，用户对叶子层item6感兴趣，可以认为它的兴趣是1，同层别的候选节点的兴趣为0，顺着着绿色线路上去的节点都标记为1，路线上的同层别的候选节点都标记为0。这样的操作就可以根据1和0构建用于每一层的正负样本。</p>
<p>样本构建完成后，可以在模型结构左侧采用任意的深度学习模型来承担用户兴趣判别器的角色，输入就是当前层构造的正负样本，输出则是（用户，节点）对的兴趣度，这个将被用作检索过程中选取topK的评判指标。<strong>在整体结构图中，我们可以看到节点特征方面，使用的是node embedding</strong>，说明在进入模型前已经向量化了。</p>
</li>
<li><p>训练过程</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305211745271.png" alt=""></p>
<p>整体联合训练的方式如下：</p>
<ol>
<li>构造随机二叉树</li>
<li>基于树模型生成样本</li>
<li>训练DNN模型直到收敛</li>
<li>基于DNN模型得到样本的Embedding，重新构造聚类二叉树</li>
<li>循环上述2～4过程</li>
</ol>
<p>具体的，在初始化树结构的时候，首先借助商品的类别信息进行排序，将相同类别的商品放到一起，然后递归的将同类别中的商品等量的分到两个子类中，直到集合中只包含一项，利用这种自顶向下的方式来初始化一棵树。基于该树采样生成深度模型训练所需的样本，然后进一步训练模型，训练结束之后可以得到每个树节点对应的Embedding向量，利用节点的Embedding向量，采用K-Means聚类方法来重新构建一颗树，最后基于这颗新生成的树，重新训练深层网络。</p>
</li>
</ol>
<h2 id="经典排序模型"><a href="#经典排序模型" class="headerlink" title="经典排序模型"></a>经典排序模型</h2><h3 id="GBDT-LR"><a href="#GBDT-LR" class="headerlink" title="GBDT+LR"></a>GBDT+LR</h3><p>前面介绍的协同过滤和矩阵分解存在的劣势就是仅利用了用户与物品相互行为信息进行推荐，忽视了用户自身特征，物品自身特征以及上下文信息等，导致生成的结果往往会比较片面。而这次介绍的这个模型是2014年由Facebook提出的GBDT+LR模型，该模型利用GBDT自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当做LR模型的输入，来产生最后的预测结果，该模型能够综合利用用户、物品和上下文等多种不同的特征，生成较为全面的推荐结果，在CTR点击率预估场景下使用较为广泛。</p>
<h4 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h4><p>逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，使得逻辑回归成为了一个优秀的分类算法，学习逻辑回归模型，首先应该记住一句话：<strong>逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</strong>  </p>
<p>相比于协同过滤和矩阵分解利用用户的物品“相似度”进行推荐， 逻辑回归模型将问题看成了一个分类问题， 通过预测正样本的概率对物品进行排序。这里的正样本可以是用户“点击”了某个商品或者“观看”了某个视频， 均是推荐系统希望用户产生的“正反馈”行为， 因此<strong>逻辑回归模型将推荐问题转化成了一个点击率预估问题</strong>。而点击率预测就是一个典型的二分类， 正好适合逻辑回归进行处理， 那么逻辑回归是如何做推荐的呢？ 过程如下：</p>
<ol>
<li>将用户年龄、性别、物品属性、物品描述、当前时间、当前地点等特征转成数值型向量</li>
<li>确定逻辑回归的优化目标，比如把点击率预测转换成二分类问题， 这样就可以得到分类问题常用的损失作为目标， 训练模型</li>
<li>在预测的时候， 将特征向量输入模型产生预测， 得到用户“点击”物品的概率</li>
<li>利用点击概率对候选物品排序， 得到推荐列表</li>
</ol>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221652189.png" alt=""></p>
<p>这里的关键就是每个特征的权重参数$w$， 我们一般是使用梯度下降的方式， 首先会先随机初始化参数$w$， 然后将特征向量（也就是我们上面数值化出来的特征）输入到模型， 就会通过计算得到模型的预测概率， 然后通过对目标函数求导得到每个$w$的梯度， 然后进行更新$w$ ，这里的目标函数长下面这样：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221654279.png" alt=""></p>
<p>求导之后的方式长这样：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221655094.png" alt=""></p>
<p><strong>优点：</strong></p>
<ol>
<li>LR模型形式简单，可解释性好，从特征的权重可以看到不同的特征对最后结果的影响。</li>
<li>训练时便于并行化，在预测时只需要对特征进行线性加权，所以<strong>性能比较好</strong>，往往适合处理<strong>海量id类特征</strong>，用id类特征有一个很重要的好处，就是<strong>防止信息损失</strong>（相对于范化的 CTR 特征），对于头部资源会有更细致的描述</li>
<li>资源占用小,尤其是内存。在实际的工程应用中只需要存储权重比较大的特征及特征对应的权重。</li>
<li>方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)</li>
</ol>
<p><strong>当然， 逻辑回归模型也有一定的局限性</strong></p>
<ol>
<li>表达能力不强， 无法进行特征交叉， 特征筛选等一系列“高级“操作（这些工作都得人工来干， 这样就需要一定的经验， 否则会走一些弯路）， 因此可能造成信息的损失</li>
<li>准确率并不是很高。因为这毕竟是一个线性模型加了个sigmoid， 形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布</li>
<li>处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据， 如果想处理非线性， 首先对连续特征的处理需要先进行<strong>离散化</strong>（离散化的目的是为了引入非线性），如上文所说，人工分桶的方式会引入多种问题。</li>
<li>LR 需要进行<strong>人工特征组合</strong>，这就需要开发者有非常丰富的领域经验，才能不走弯路。这样的模型迁移起来比较困难，换一个领域又需要重新进行大量的特征工程。</li>
</ol>
<p>所以如何<strong>自动发现有效的特征、特征组合，弥补人工经验不足，缩短LR特征实验周期</strong>，是亟需解决的问题， 而GBDT模型， 正好可以<strong>自动发现特征并进行有效组合</strong></p>
<h4 id="GBDT-1"><a href="#GBDT-1" class="headerlink" title="GBDT"></a>GBDT</h4><p>GBDT全称梯度提升决策树，在传统机器学习算法里面是对真实分布拟合的最好的几种算法之一，在前几年深度学习还没有大行其道之前，gbdt在各种竞赛是大放异彩。原因大概有几个，一是效果确实挺不错。二是即可以用于分类也可以用于回归。三是可以筛选特征， 所以这个模型依然是一个非常重要的模型。 </p>
<h4 id="GBDT-LR模型"><a href="#GBDT-LR模型" class="headerlink" title="GBDT+LR模型"></a>GBDT+LR模型</h4><p>2014年， Facebook提出了一种利用GBDT自动进行特征筛选和组合， 进而生成新的离散特征向量， 再把该特征向量当做LR模型的输入， 来产生最后的预测结果， 这就是著名的GBDT+LR模型了。GBDT+LR 使用最广泛的场景是CTR点击率预估，即预测当给用户推送的广告会不会被用户点击，模型的总体结构长下面这样：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221737318.png" alt=""></p>
<p><strong>训练时</strong>，GBDT 建树的过程相当于自动进行的特征组合和离散化，然后从根结点到叶子节点的这条路径就可以看成是不同特征进行的特征组合，用叶子节点可以唯一的表示这条路径，并作为一个离散特征传入 LR 进行<strong>二次训练</strong>。</p>
<p>比如上图中， 有两棵树，x为一条输入样本，遍历两棵树后，x样本分别落到两颗树的叶子节点上，每个叶子节点对应LR一维特征，那么通过遍历树，就得到了该样本对应的所有LR特征。构造的新特征向量是取值0/1的。 比如左树有三个叶子节点，右树有两个叶子节点，最终的特征即为五维的向量。对于输入x，假设他落在左树第二个节点，编码[0,1,0]，落在右树第二个节点则编码[0,1]，所以整体的编码为[0,1,0,0,1]，这类编码作为特征，输入到线性分类模型（LR or FM）中进行分类。</p>
<p><strong>预测时</strong>，会先走 GBDT 的每棵树，得到某个叶子节点对应的一个离散特征(即一组特征组合)，然后把该特征以 one-hot 形式传入 LR 进行线性加权预测。</p>
<p>这个方案应该比较简单了， 下面有几个关键的点我们需要了解：</p>
<ol>
<li><strong>通过GBDT进行特征组合之后得到的离散向量是和训练数据的原特征一块作为逻辑回归的输入， 而不仅仅全是这种离散特征</strong></li>
<li>建树的时候用ensemble建树的原因就是一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT每棵树都在学习前面棵树尚存的不足，迭代多少次就会生成多少棵树。</li>
<li>RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。</li>
<li>在CRT预估中， GBDT一般会建立两类树(非ID特征建一类， ID类特征建一类)， AD，ID类特征在CTR预估中是非常重要的特征，直接将AD，ID作为feature进行建树不可行，故考虑为每个AD，ID建GBDT树。<ol>
<li>非ID类树：不以细粒度的ID建树，此类树作为base，即便曝光少的广告、广告主，仍可以通过此类树得到有区分性的特征、特征组合</li>
<li>ID类树：以细粒度 的ID建一类树，用于发现曝光充分的ID对应有区分性的特征、特征组合</li>
</ol>
</li>
</ol>
<h3 id="特征交叉"><a href="#特征交叉" class="headerlink" title="特征交叉"></a>特征交叉</h3><h4 id="FM模型"><a href="#FM模型" class="headerlink" title="FM模型"></a>FM模型</h4><p><code>FM(Factorization Machine)</code>，即因式分解机。</p>
<h5 id="为什么需要FM模型"><a href="#为什么需要FM模型" class="headerlink" title="为什么需要FM模型"></a>为什么需要FM模型</h5><ol>
<li>特征组合是许多机器学习建模过程中遇到的问题，如果对特征直接建模，很有可能会忽略掉特征与特征之间的关联信息，因此，可以通过构建新的交叉特征这一特征组合方式提高模型的效果。</li>
<li>高维的稀疏矩阵是实际工程中常见的问题，并直接会导致计算量过大，特征权值更新缓慢。试想一个 $10000\times100$ 的表，每一列都有 $8$ 种元素，经过<code>One-Hot</code>独热编码之后，会产生一个 $10000\times800$ 的表。因此表中每行元素只有 $100$ 个值为 $1$ ， $700$ 个值为 $0$ 。</li>
</ol>
<p>而<code>FM</code>的优势就在于对这两方面问题的处理。首先是特征组合，通过对两两特征组合，引入交叉项特征，提高模型得分；其次是高维灾难，通过引入隐向量（对参数矩阵进行矩阵分解），完成对特征的参数估计。</p>
<h5 id="FM模型的应用场景"><a href="#FM模型的应用场景" class="headerlink" title="FM模型的应用场景"></a>FM模型的应用场景</h5><p>我们已经知道了<code>FM</code>可以解决特征组合以及高维稀疏矩阵问题，而实际业务场景中，电商、豆瓣等推荐系统的场景是使用最广的领域，打个比方，小王只在豆瓣上浏览过 $20$ 部电影，而豆瓣上面有 $20000$ 部电影，如果构建一个基于小王的电影矩阵，毫无疑问，里面将有 $199980$ 个元素全为 $0$ 。而类似于这样的问题就可以通过<code>FM</code>来解决。</p>
<h5 id="FM模型的具体形式"><a href="#FM模型的具体形式" class="headerlink" title="FM模型的具体形式"></a>FM模型的具体形式</h5><p>首先我们回顾一下最常见的线性表达式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304191959346.png" alt=""></p>
<p>其中 $w_{0}$ 为初始权值，或者理解为偏置项， $w_{i}$ 为每个特征 $x_{i}$ 对应的权值。可以看到，这种线性表达式只描述了每个特征与输出的关系。</p>
<p><code>FM</code>的表达式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192002037.png" alt=""></p>
<p>可观察到，只是在线性表达式后面加入了新的交叉项特征及对应的权值。这里 $x_{i}$ 和 $x_{j}$ 分别表示两个不同的特征取值，对于 $n$ 维的特征来说，这样的组合应该一共有 $C_{n}^{2}$ 种，也就意味着我们需要同样数量的权重参数。</p>
<h5 id="FM模型的解决方法"><a href="#FM模型的解决方法" class="headerlink" title="FM模型的解决方法"></a>FM模型的解决方法</h5><p><code>FM</code>解决这个问题的方法非常简单，它不再是简单地为交叉之后的特征对设置参数，而是设置了一种计算特征参数的方法。<code>FM</code>模型引入了新的矩阵 $V$，矩阵 $V$ 是一个 $n \times k$ 的二维矩阵。这里的 $k$ 是我们设置的参数，一般不会很大，比如 $16$ 、 $32$ 之类。对于特征每一个维度 $i$ ，我们都可以找到一个 $v_{j}$ ，它表示一个 $1 \times k$ 的向量。于是我们可以用 $v_{i}$ 和 $v_{j}$ 来计算得出上式中的 $w_{ij}$ ，即 $w_{ij}=v_{i}v_{j}^{T}$ 。也就是说我们<strong>用向量的内积来计算得到了就交叉特征的系数</strong>，相比于原先 $O(n^{2})$ 量级的参数而言，我们将参数的量级降低到了 $O(n)$ 。</p>
<p>有了上面的方法，我们就能表示出交叉项，具体过程如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192107149.png" alt=""></p>
<h5 id="FM模型的训练"><a href="#FM模型的训练" class="headerlink" title="FM模型的训练"></a>FM模型的训练</h5><p>经过上述步骤，我们能够得到变形之后的原式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192121730.png" alt=""></p>
<p>首先需要明确的是我们想要优化的参数是 $w_{0}$ ， $w_{i}$ 和 $w_{ij}$ ，所以我们对损失函数求导即可得到：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192201890.png" alt=""></p>
<p>其中 $\sum\limits_{j=1}^{n}v_{j,f}x_{j}$ 和 $i$ 是独立的，所以它是可以提前算好的，这样一来对于所有参数项，我们都可以在 $O(1)$ 的时间内计算出它们的梯度。</p>
<h5 id="FM模型的高维扩展"><a href="#FM模型的高维扩展" class="headerlink" title="FM模型的高维扩展"></a>FM模型的高维扩展</h5><p>我们仿照刚才的公式，可以写出<code>FM</code>模型推广到 $d$ 维的方程：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192209525.png" alt=""></p>
<p>前面两项都很好理解，我们着重来看第三项。第三项当中包含了从 $2$ 维到 $d$ 维交叉特征的情况，我们以 $d=3$ 为例，那么这一项当中应该包含二维的交叉项以及三维的交叉项，应该是这样的：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192210293.png" alt=""></p>
<p>这个式子<strong>整体上和之前的形式是一样的</strong>，我们不难分析出它的复杂度是 $O(kn^{d})$ 。当 $d=2$ 的时候，我们通过一系列变形将它的复杂度优化到了 $O(kn)$ ，而当 $d&gt;2$ 的时候，没有很好的优化方法，而且三重特征的交叉往往没有意义，并且会过于稀疏，所以我们一般情况下只会使用 $d = 2$ 的情况。</p>
<h4 id="FFM模型"><a href="#FFM模型" class="headerlink" title="FFM模型"></a>FFM模型</h4><p><code>FFM(Field-aware Factorization Machine)</code>其实就是<code>FM</code>的进阶版，<code>FFM</code>将隐向量 $v$ 又进一步细化（引入了<code>field</code>的概念，即将特征所在的不同的<code>field</code>这个信息也考虑进去），其公式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201650996.png" alt=""></p>
<p>其中， $f_{j}$ 是第 $j$ 个特征所属的<code>field</code>。如果隐向量的长度为 $k$ ，那么<code>FFM</code>的二次参数有 $nfk$ 个，远多于<code>FM</code>模型的 $nk$ 个。此外，由于隐向量与<code>field</code>相关，<code>FFM</code>二次项并不能够化简，其预测复杂度是 $O(kn^{2})$ 。</p>
<h5 id="FFM模型的特征组合方式"><a href="#FFM模型的特征组合方式" class="headerlink" title="FFM模型的特征组合方式"></a>FFM模型的特征组合方式</h5><p>举一个例子进行辅助说明：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>User</th>
<th>Movie</th>
<th>Genre</th>
<th>Price</th>
</tr>
</thead>
<tbody>
<tr>
<td>YuChin</td>
<td>3Idiots</td>
<td>Comedy, Drama</td>
<td>$9.99</td>
</tr>
</tbody>
</table>
</div>
<p>对上面出现的信息进行分类标注，<strong>红字代表所在的field,蓝字代表特征,绿字代表特征的值。</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201703206.png" alt=""></p>
<p><strong>FM的特征组合方式为：</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201704698.png" alt=""></p>
<p><strong>FFM的特征组合方式为：</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201707323.png" alt=""></p>
<p>不难看出，<code>FM</code>中有 $w_{1}$ ， $w_{2}$ ， $w_{3}$ ， $w_{4}$ ，$w_{5}$ 五个隐向量，但到了<code>FFM</code>中有 $4\times5=20$ 个，选 $x_{i}$ 与 $x_{j}$ 对应的域 $f_{j}$ 对应的隐向量 $v_{i,f_{j}}$ 进行交叉。</p>
<h5 id="FFM模型的应用场景"><a href="#FFM模型的应用场景" class="headerlink" title="FFM模型的应用场景"></a>FFM模型的应用场景</h5><p>和<code>FM</code>算法一样，<code>FFM</code>主要应用在推荐算法中的<code>CTR</code>点击率预估（排序）问题，推荐系统一般可以分成两个模块，召回和排序。比如对于电影推荐，召回模块会针对用户生成一个推荐电影列表，而排序模块则负责对这个电影列表根据用户的兴趣做排序。当把<code>FFM</code>算法应用到推荐系统中时，具体地是应用在排序模块。</p>
<h4 id="PNN"><a href="#PNN" class="headerlink" title="PNN"></a>PNN</h4><p>在特征交叉的相关模型中FM, FFM都证明了特征交叉的重要性，FNN将神经网络的高阶隐式交叉加到了FM的二阶特征交叉上，一定程度上说明了DNN做特征交叉的有效性。但是对于DNN这种“add”操作的特征交叉并不能充分挖掘类别特征的交叉效果。PNN虽然也用了DNN来对特征进行交叉组合，但是并不是直接将低阶特征放入DNN中，而是设计了Product层先对低阶特征进行充分的交叉组合之后再送入到DNN中去。</p>
<p>PNN模型其实是对IPNN和OPNN的总称，两者分别对应的是不同的Product实现方法，前者采用的是inner product，后者采用的是outer product。在PNN的算法方面，比较重要的部分就是Product Layer的简化实现方法，需要在数学和代码上都能够比较深入的理解。</p>
<h5 id="模型结构及原理"><a href="#模型结构及原理" class="headerlink" title="模型结构及原理"></a>模型结构及原理</h5><p>PNN模型的整体架构如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221912175.png" alt=""></p>
<p>一共分为五层，其中除了Product Layer别的layer都是比较常规的处理方法，均可以从前面的章节进一步了解。模型中最重要的部分就是通过Product层对embedding特征进行交叉组合，也就是上图中红框所显示的部分。</p>
<p>Product层主要有线性部分和非线性部分组成，分别用$l_z$和$l_p$来表示，</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221914930.png" alt=""></p>
<ol>
<li>线性模块，一阶特征(未经过显示特征交叉处理)，对应论文中的$l_z=(l_z^1,l_z^2, …, l_z^{D_1})$</li>
<li>非线性模块，高阶特征(经过显示特征交叉处理)，对应论文中的$l_p=(l_p^1,l_p^2, …, l_p^{D_1})$</li>
</ol>
<p><strong>线性部分</strong></p>
<p>先来解释一下$l_z$是如何计算得到的，在介绍计算$l_z$之前先介绍一下矩阵内积计算, 如下公式所示，用一句话来描述就是两个矩阵对应元素相称，然后将相乘之后的所有元素相加</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221916770.png" alt=""></p>
<p>$l_z^n$的计算就是矩阵内积，而$l_z$是有$D_1$个$l_z^n$组成，所以需要$D_1$个矩阵求得，但是在代码实现的时候不一定是定义$D_1$个矩阵，可以将这些矩阵Flatten。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221917878.png" alt=""></p>
<p>总之这一波操作就是将所有的embedding向量中的所有元素都乘以一个矩阵的对应元素，最后相加即可，这一部分比较简单(N表示的是特征的数量，M表示的是所有特征转化为embedding之后维度，也就是N*emb_dim)</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221919755.png" alt=""></p>
<p><strong>非线性部分</strong></p>
<p>上面介绍了线性部分$l_p$的计算，非线性部分的计算相比线性部分要复杂很多，先从整体上看$l_p$的计算</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221925847.png" alt=""></p>
<p>从上述公式中可以发现，$l_p^n$和$l_z^n$类似需要$D_1$个$W_p^n$矩阵计算内积得到，重点就是如何求这个$p$，这里作者提出了两种方式，一种是使用内积计算，另一种是使用外积计算。</p>
<ul>
<li>IPNN</li>
</ul>
<p>使用内积实现特征交叉就和FM是类似的(两两向量计算内积)，下面将向量内积操作表示如下表达式</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221951440.png" alt=""></p>
<p>将内积的表达式带入$l_p^n$的计算表达式中有：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221951074.png" alt=""></p>
<p>上面就提到了这里使用的内积是计算两两特征之间的内积，然而向量a和向量b的内积与向量b和向量a的内积是相同的，其实是没必要计算的，看一下下面FM的计算公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221952139.png" alt=""></p>
<p>也就是说计算的内积矩阵$p$是对称的，那么与其对应元素做矩阵内积的矩阵$W_p^n$也是对称的，对于可学习的权重来说如果是对称的是不是可以只使用其中的一半就行了呢，所以基于这个思考，对Inner Product的权重定义及内积计算进行优化，首先将权重矩阵分解$W_p^n=\theta^n \theta^{nT}$,此时$\theta^n \in R^N$（参数从原来的$N^2$变成了$N$）,将分解后的$W_p^n$带入$l_p^n$的计算公式有：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221952937.png" alt=""></p>
<p>所以优化后的$l_p$的计算公式为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221952743.png" alt=""></p>
<p>这里为了好理解不做过多的解释，其实这里对于矩阵分解省略了一些细节，感兴趣的可以去看原文，最后模型实现的时候就是基于上面的这个公式计算的（给出的代码也是基于优化之后的实现）。</p>
<ul>
<li>OPNN</li>
</ul>
<p>使用外积实现相比于使用内积实现，唯一的区别就是使用向量的外积来计算矩阵$p$,首先定义向量的外积计算</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221953919.png" alt=""></p>
<p>从外积公式可以发现两个向量的外积得到的是一个矩阵，与上面介绍的内积计算不太相同，内积得到的是一个数值。内积实现的Product层是将计算得到的内积矩阵，乘以一个与其大小一样的权重矩阵，然后求和，按照这个思路的话，通过外积得到的$p$计算$W_p^n \odot{p}$相当于之前的内积值乘以权重矩阵对应位置的值求和就变成了，外积矩阵乘以权重矩阵中对应位置的子矩阵然后将整个相乘得到的大矩阵对应元素相加，用公式表示如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221953829.png" alt=""></p>
<p>需要注意的是此时的$(W_p^n)_{i,j}$表示的是一个矩阵，而不是一个值，此时计算$l_p$的复杂度是$O(D_1\ast N^2 \ast M^2)$, 其中$N^2$表示的是特征的组合数量，$M^2$表示的是计算外积的复杂度。这样的复杂度肯定是无法接受的，所以为了优化复杂度，PNN的作者重新定义了$p$的计算方式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221954360.png" alt=""></p>
<p>需要注意，这里新定义的外积计算与传统的外积计算时不等价的，这里是为了优化计算效率重新定义的计算方式，从公式中可以看出，相当于先将原来的embedding向量在特征维度上先求和，变成一个向量之后再计算外积。加入原embedding向量表示为$E \in R^{N\times M}$，其中$N$表示特征的数量，M表示的是所有特征的总维度，即$N*emb_dim$, 在特征维度上进行求和就是将$E \in R^{N\times M}$矩阵压缩成了$E \in R^M$, 然后两个$M$维的向量计算外积得到最终所有特征的外积交叉结果$p\in R^{M\times M}$，最终的$l_p^n$可以表示为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305221954996.png" alt=""></p>
<p>最终的计算方式和$l_z$的计算方式看起来差不多，但是需要注意外积优化后的$W_p^n$的维度是$R^{M \times M}$的，$M$表示的是特征矩阵的维度，即$N*emb_dim$。</p>
<h4 id="DCN"><a href="#DCN" class="headerlink" title="DCN"></a>DCN</h4><p>Wide&amp;Deep模型的提出不仅综合了“记忆能力”和“泛化能力”， 而且开启了不同网络结构融合的新思路。 所以后面就有各式各样的模型改进Wide部分或者Deep部分， 而Deep&amp;Cross模型(DCN)就是其中比较典型的一个，这是2017年斯坦福大学和谷歌的研究人员在ADKDD会议上提出的， 该模型针对W&amp;D的wide部分进行了改进， 因为Wide部分有一个不足就是需要人工进行特征的组合筛选， 过程繁琐且需要经验， 而2阶的FM模型在线性的时间复杂度中自动进行特征交互，但是这些特征交互的表现能力并不够，并且随着阶数的上升，模型复杂度会大幅度提高。于是乎，作者用一个Cross Network替换掉了Wide部分，来自动进行特征之间的交叉，并且网络的时间和空间复杂度都是线性的。 通过与Deep部分相结合，构成了深度交叉网络（Deep &amp; Cross Network），简称DCN。</p>
<h5 id="模型结构及原理-1"><a href="#模型结构及原理-1" class="headerlink" title="模型结构及原理"></a>模型结构及原理</h5><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222002416.png" alt=""></p>
<h5 id="Embedding和Stacking-层"><a href="#Embedding和Stacking-层" class="headerlink" title="Embedding和Stacking 层"></a>Embedding和Stacking 层</h5><p>Embedding层我们已经非常的熟悉了吧， 这里的作用依然是把稀疏离散的类别型特征变成低维密集型。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222003410.png" alt=""></p>
<p>其中对于某一类稀疏分类特征（如id），$X_{embed, i}$是第个$i$分类值（id序号）的embedding向量。$W_{embed,i}$是embedding矩阵， $n_e\times n_v$维度， $n_e$是embedding维度， $n_v$是该类特征的唯一取值个数。$x_i$属于该特征的二元稀疏向量(one-hot)编码的。 【实质上就是在训练得到的Embedding参数矩阵中找到属于当前样本对应的Embedding向量】。其实绝大多数基于深度学习的推荐模型都需要Embedding操作，参数学习是通过神经网络进行训练。</p>
<p>最后，该层需要将所有的密集型特征与通过embedding转换后的特征进行联合（Stacking）：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222003825.png" alt=""></p>
<p>一共$k$个类别特征， dense是数值型特征， 两者在特征维度拼在一块。 上面的这两个操作如果是看了前面的模型的话，应该非常容易理解了。</p>
<h5 id="Cross-Network"><a href="#Cross-Network" class="headerlink" title="Cross Network"></a>Cross Network</h5><p>这个就是本模型最大的亮点了【Cross网络】， 这个思路感觉非常Nice。设计该网络的目的是增加特征之间的交互力度。交叉网络由多个交叉层组成， 假设第$l$层的输出向量$x_l$， 那么对于第$l+1$层的输出向量$x_{l+1}$表示为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222005263.png" alt=""></p>
<p>可以看到， 交叉层的二阶部分非常类似PNN提到的外积操作， 在此基础上增加了外积操作的权重向量$w_l$， 以及原输入向量$x_l$和偏置向量$b_l$。 交叉层的可视化如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222006335.png" alt=""></p>
<p>可以看到， 每一层增加了一个$n$维的权重向量$w_l$（n表示输入向量维度）， 并且在每一层均保留了输入向量， 因此输入和输出之间的变化不会特别明显。关于这一层， 原论文里面有个具体的证明推导Cross Network为啥有效， 不过比较复杂，这里我拿一个式子简单的解释下上面这个公式的伟大之处：</p>
<p><strong>我们根据上面这个公式， 尝试的写前面几层看看:</strong></p>
<p>$l=0:\mathbf{x}_{1} =\mathbf{x}_{0} \mathbf{x}_{0}^{T} \mathbf{w}_{0}+ \mathbf{b}_{0}+\mathbf{x}_{0}$</p>
<p>$l=1:\mathbf{x}_{2} =\mathbf{x}_{0} \mathbf{x}_{1}^{T} \mathbf{w}_{1}+ \mathbf{b}_{1}+\mathbf{x}_{1}=\mathbf{x}_{0} [\mathbf{x}_{0} \mathbf{x}_{0}^{T} \mathbf{w}_{0}+ \mathbf{b}_{0}+\mathbf{x}_{0}]^{T}\mathbf{w}_{1}+\mathbf{b}_{1}+\mathbf{x}_{1}$</p>
<p>$l=2:\mathbf{x}_{3} =\mathbf{x}_{0} \mathbf{x}_{2}^{T} \mathbf{w}_{2}+ \mathbf{b}_{2}+\mathbf{x}_{2}=\mathbf{x}_{0} [\mathbf{x}_{0} [\mathbf{x}_{0} \mathbf{x}_{0}^{T} \mathbf{w}_{0}+ \mathbf{b}_{0}+\mathbf{x}_{0}]^{T}\mathbf{w}_{1}+\mathbf{b}_{1}+\mathbf{x}_{1}]^{T}\mathbf{w}_{2}+\mathbf{b}_{2}+\mathbf{x}_{2}$</p>
<p>我们暂且写到第3层的计算， 我们会发现什么结论呢？  给大家总结一下：</p>
<ol>
<li><p>$\mathrm{x}_1$中包含了所有的$\mathrm{x}_0$的1,2阶特征的交互， $\mathrm{x}_2$包含了所有的$\mathrm{x}_1, \mathrm{x}_0$的1、2、3阶特征的交互，$\mathrm{x}_3$中包含了所有的$\mathrm{x}_2$, $\mathrm{x}_1$与$\mathrm{x}_0$的交互，$\mathrm{x}_0$的1、2、3、4阶特征交互。 因此， 交叉网络层的叉乘阶数是有限的。 <strong>第$l$层特征对应的最高的叉乘阶数$l+1$</strong></p>
</li>
<li><p>Cross网络的参数是共享的， 每一层的这个权重特征之间共享， 这个可以使得模型泛化到看不见的特征交互作用， 并且对噪声更具有鲁棒性。 例如两个稀疏的特征$x_i,x_j$， 它们在数据中几乎不发生交互， 那么学习$x_i,x_j$的权重对于预测没有任何的意义。</p>
</li>
<li><p>计算交叉网络的参数数量。 假设交叉层的数量是$L_c$， 特征$x$的维度是$n$， 那么总共的参数是：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222009493.png" alt=""></p>
<p>这个就是每一层会有$w$和$b$。且$w$维度和$x$的维度是一致的。</p>
</li>
<li><p>交叉网络的时间和空间复杂度是线性的。这是因为， 每一层都只有$w$和$b$， 没有激活函数的存在，相对于深度学习网络， 交叉网络的复杂性可以忽略不计。</p>
</li>
<li><p>Cross网络是FM的泛化形式，在FM模型中，特征 $x_i$ 的权重 $v_i$，那么交叉项 $x_i,x_j$ 的权重为$\langle x_i,x_j\rangle$。在DCN中，$x_i$的权重为${W_K^{(i)}}_{k=1}^l$, 交叉项$x_i,x_j$的权重是参数${W_K^{(i)}}_{k=1}^l$和${W_K^{(j)}}_{k=1}^l$的乘积，这个看上面那个例子展开感受下。因此两个模型都各自学习了独立于其他特征的一些参数，并且交叉项的权重是相应参数的某种组合。FM只局限于2阶的特征交叉(一般)，而DCN可以构建更高阶的特征交互， 阶数由网络深度决定，并且交叉网络的参数只依据输入的维度线性增长。</p>
</li>
<li><p>还有一点我们也要了解，对于每一层的计算中， 都会跟着$\mathrm{x}_0$, 这个是咱们的原始输入， 之所以会乘以一个这个，是为了保证后面不管怎么交叉，都不能偏离我们的原始输入太远，别最后交叉交叉都跑偏了。</p>
</li>
<li><p>$\mathbf{x}_{l+1}=f\left(\mathbf{x}_{l}, \mathbf{w}_{l}, \mathbf{b}_{l}\right)+\mathbf{x}_{l}$, 这个东西其实有点跳远连接的意思，也就是和ResNet也有点相似，无形之中还能有效的缓解梯度消失现象。</p>
</li>
</ol>
<p>好了， 关于本模型的交叉网络的细节就介绍到这里了。这应该也是本模型的精华之处了，后面就简单了。</p>
<h5 id="Deep-Network"><a href="#Deep-Network" class="headerlink" title="Deep Network"></a>Deep Network</h5><p>这个就和上面的D&amp;W的全连接层原理一样。这里不再过多的赘述。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222013355.png" alt=""></p>
<p>具体的可以参考W&amp;D模型。</p>
<h5 id="组合输出层"><a href="#组合输出层" class="headerlink" title="组合输出层"></a>组合输出层</h5><p>这个层负责将两个网络的输出进行拼接， 并且通过简单的Logistics回归完成最后的预测：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222014011.png" alt=""></p>
<p>其中$\mathbf{x}_{L_{1}}^{T}$和$\mathbf{h}_{L_{2}}^{T}$分别表示交叉网络和深度网络的输出。<br>最后二分类的损失函数依然是交叉熵损失：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222014360.png" alt=""></p>
<p>Cross&amp;Deep模型的原理就是这些了，其核心部分就是Cross Network， 这个可以进行特征的自动交叉， 避免了更多基于业务理解的人工特征组合。 该模型相比于W&amp;D，Cross部分表达能力更强， 使得模型具备了更强的非线性学习能力。</p>
<h4 id="AutoInt"><a href="#AutoInt" class="headerlink" title="AutoInt"></a>AutoInt</h4><h5 id="动机和原理"><a href="#动机和原理" class="headerlink" title="动机和原理"></a>动机和原理</h5><p>这篇文章的前言部分依然是说目前模型的不足，以引出模型的动机所在， 简单的来讲，就是两句话：</p>
<ol>
<li>浅层的模型会受到交叉阶数的限制，没法完成高阶交叉</li>
<li>深层模型的DNN在学习高阶隐性交叉的效果并不是很好， 且不具有可解释性</li>
</ol>
<p>于是乎：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222018983.png" alt=""></p>
<p>那么是如何做到的呢？ 引入了transformer， 做成了一个特征交互层， 原理如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222018848.png" alt=""></p>
<h5 id="AutoInt模型的前向过程梳理"><a href="#AutoInt模型的前向过程梳理" class="headerlink" title="AutoInt模型的前向过程梳理"></a>AutoInt模型的前向过程梳理</h5><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222019899.png" alt=""></p>
<h6 id="Input-Layer"><a href="#Input-Layer" class="headerlink" title="Input Layer"></a>Input Layer</h6><p>输入层这里， 用到的特征主要是离散型特征和连续性特征， 这里不管是哪一类特征，都会过embedding层转成低维稠密的向量，是的， <strong>连续性特征，这里并没有经过分桶离散化，而是直接走embedding</strong>。这个是怎么做到的呢？就是就是类似于预训练时候的思路，先通过item_id把连续型特征与类别特征关联起来，最简单的，就是把item_id拿过来，过完embedding层取出对应的embedding之后，再乘上连续值即可， 所以这个连续值事先一定要是归一化的。 当然，这个玩法，我也是第一次见。 学习到了， 所以模型整体的输入如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222021880.png" alt=""></p>
<p>这里的$M$表示特征的个数, $X_1, X_2$这是离散型特征， one-hot的形式， 而$X_M$在这里是连续性特征。过embedding层的细节应该是我上面说的那样。</p>
<h6 id="Embedding-Layer"><a href="#Embedding-Layer" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h6><p>embedding层的作用是把高维稀疏的特征转成低维稠密， 离散型的特征一般是取出对应的embedding向量即可， 具体计算是这样：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222024864.png" alt=""></p>
<p>对于第$i$个离散特征，直接第$i$个嵌入矩阵$V_i$乘one-hot向量就取出了对应位置的embedding。 当然，如果输入的时候不是个one-hot， 而是个multi-hot的形式，那么对应的embedding输出是各个embedding求平均得到的。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222023574.png" alt=""></p>
<p>比如， 推荐里面用户的历史行为item。过去点击了多个item，最终的输出就是这多个item的embedding求平均。<br>而对于连续特征， 我上面说的那样， 也是过一个embedding矩阵取相应的embedding， 不过，最后要乘一个连续值</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222025480.png" alt=""></p>
<p>这样，不管是连续特征，离散特征还是变长的离散特征，经过embedding之后，都能得到等长的embedding向量。 我们把这个向量拼接到一块，就得到了交互层的输入。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222025399.png" alt=""></p>
<h6 id="Interacting-Layer"><a href="#Interacting-Layer" class="headerlink" title="Interacting Layer"></a>Interacting Layer</h6><p>这个是本篇论文的核心了，其实这里说的就是transformer块的前向传播过程，所以这里我就直接用比较白话的语言简述过程了，不按照论文中的顺序展开了。</p>
<p>通过embedding层， 我们会得到M个向量$e_1, …e_M$，假设向量的维度是$d$维， 那么这个就是一个$d\times M$的矩阵， 我们定一个符号$X$。 接下来我们基于这个矩阵$X$，做三次变换，也就是分别乘以三个矩阵$W_k^{(h)}, W_q^{(h)},W_v^{(h)}$， 这三个矩阵的维度是$d’\times d$的话， 那么我们就会得到三个结果：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222028818.png" alt=""></p>
<p>这三个矩阵都是$d’\times M$的。这其实就完成了一个Head的操作。所谓的自注意力， 就是$X$通过三次变换得到的结果之间，通过交互得到相关性，并通过相关性进行加权汇总，全是$X$自发的。 那么是怎么做到的呢？首先， 先进行这样的操作：</p>
<script type="math/tex; mode=display">Score(Q^h,K^h)=Q^h \times {K^h}^T</script><p>这个结果得到的是一个$d’\times d’$的矩阵， 那么这个操作到底是做了一个什么事情呢？</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222029092.png" alt=""></p>
<p>假设这里的$c_1..c_6$是我们的6个特征， 而每一行代表每个特征的embedding向量，这样两个矩阵相乘，相当于得到了当前特征与其它特征两两之间的內积值， 而內积可以表示两个向量之间的相似程度。所以得到的结果每一行，就代表当前这个特征与其它特征的相似性程度。</p>
<p>接下来，我们对$Score(Q^h,K^h)$， 在最后一个维度上进行softmax，就根据相似性得到了权重信息，这其实就是把相似性分数归一化到了0-1之间<script type="math/tex">Attention(Q^h,K^h)=Softmax(Score(Q^h,K^h))</script><br>接下来， 我们再进行这样的一步操作<script type="math/tex">E^{(h)}=Attention(Q^h,K^h) \times V</script><br>这样就得到了$d’\times M$的矩阵$E$， 这步操作，其实就是一个加权汇总的过程， 对于每个特征， 先求与其它特征的相似度，然后得到一个权重，再回乘到各自的特征向量再求和。 只不过这里的特征是经过了一次线性变化的过程，降维到了$d’$。</p>
<p>上面是我从矩阵的角度又过了一遍， 这个是直接针对所有的特征向量一部到位。 论文里面的从单个特征的角度去描述的，只说了一个矩阵向量过多头注意力的操作。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222030271.png" alt=""></p>
<p>这里会更好懂一些， 就是相当于上面矩阵的每一行操作拆开了， 首先，整个拼接起来的embedding矩阵还是过三个参数矩阵得到$Q,K,V$， 然后是每一行单独操作的方式，对于某个特征向量$e_k$，与其它的特征两两內积得到权重，然后在softmax，回乘到对应向量，然后进行求和就得到了融合其它特征信息的新向量。 具体过程如图：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222030314.png" alt=""></p>
<p>上面的过程是用了一个头，理解的话就类似于从一个角度去看特征之间的相关关系，用论文里面的话讲，这是从一个子空间去看， 如果是想从多个角度看，这里可以用多个头，即换不同的矩阵$W_q,W_k,W_v$得到不同的$Q,K,V$然后得到不同的$e_m$， 每个$e_m$是$d’\times 1$的。</p>
<p>然后，多个头的结果concat起来</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222031199.png" alt=""></p>
<p>这是一个$d’\times H$的向量， 假设有$H$个头。 </p>
<p>接下来， 过一个残差网络层，这是为了保留原始的特征信息</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222031359.png" alt=""></p>
<p>这里的$e_m$是$d\times 1$的向量， $W_{Res}$是$d’H\times d$的矩阵， 最后得到的$e_m^{Res}$是$d’H\times 1$的向量， 这是其中的一个特征，如果是$M$个特征堆叠的话，最终就是$d’HM\times 1$的矩阵， 这个就是Interacting Layer的结果输出。</p>
<h6 id="Output-Layer"><a href="#Output-Layer" class="headerlink" title="Output Layer"></a>Output Layer</h6><p>输出层就非常简单了，加一层全连接映射出输出值即可：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222032295.png" alt=""></p>
<p>这里的$W$是$d’HM\times 1$的， 这样最终得到的是一个概率值了， 接下来交叉熵损失更新模型参数即可。</p>
<h5 id="AutoInt的分析"><a href="#AutoInt的分析" class="headerlink" title="AutoInt的分析"></a>AutoInt的分析</h5><p>这里论文里面分析了为啥AutoInt能建模任意的高阶交互以及时间复杂度和空间复杂度的分析。我们一一来看。</p>
<p>关于建模任意的高阶交互， 我们这里拿一个transformer块看下， 对于一个transformer块， 我们发现特征之间完成了一个2阶的交互过程，得到的输出里面我们还保留着1阶的原始特征。 </p>
<p>那么再经过一个transformer块呢？ 这里面就会有2阶和1阶的交互了， 也就是会得到3阶的交互信息。而此时的输出，会保留着第一个transformer的输出信息特征。再过一个transformer块的话，就会用4阶的信息交互信息， 其实就相当于， 第$n$个transformer里面会建模出$n+1$阶交互来， 这个与CrossNet其实有异曲同工之妙的，无法是中间交互时的方式不一样。 前者是bit-wise级别的交互，而后者是vector-wise的交互。 </p>
<p>所以， AutoInt是可以建模任意高阶特征的交互的，并且这种交互还是显性。</p>
<p>关于时间复杂度和空间复杂度，空间复杂度是$O(Ldd’H)$级别的， 这个也很好理解，看参数量即可， 3个W矩阵， H个head，再假设L个transformer块的话，参数量就达到这了。 时间复杂度的话是$O(MHd’(M+d))$的，论文说如果d和d’很小的话，其实这个模型不算复杂。</p>
<h4 id="FiBiNet"><a href="#FiBiNet" class="headerlink" title="FiBiNet"></a>FiBiNet</h4><h5 id="模型原理及细节"><a href="#模型原理及细节" class="headerlink" title="模型原理及细节"></a>模型原理及细节</h5><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222035784.png" alt=""></p>
<h6 id="Embedding-Layer-1"><a href="#Embedding-Layer-1" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h6><p>这个不多讲， 整理这个是为了后面统一符号。</p>
<p>假设我们有$f$个离散特征，经过embedding层之后，会得到$E=\left[e_{1}, e_{2}, \cdots, e_{i}, \cdots, e_{f}\right]$， 其中$e_{i} \in R^{k}$，表示第$i$个离散特征对应的embedding向量，$k$维。</p>
<h6 id="SENET-Layer"><a href="#SENET-Layer" class="headerlink" title="SENET Layer"></a>SENET Layer</h6><p>这是第一个重点，首先这个网络接收的输入是上面的$E=\left[e_{1}, e_{2}, \cdots, e_{i}, \cdots, e_{f}\right]$， 网络的输出也是个同样大小的张量<code>(None, f, k)</code>矩阵。 结构如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222038251.png" alt=""></p>
<p>SENet由自动驾驶公司Momenta在2017年提出，在当时，是一种应用于图像处理的新型网络结构。它基于CNN结构，<strong>通过对特征通道间的相关性进行建模，对重要特征进行强化来提升模型准确率，本质上就是针对CNN中间层卷积核特征的Attention操作</strong>。SENet仍然是效果最好的图像处理网络结构之一。</p>
<p>把SENet放在Embedding层之上，通过SENet网络，动态地学习这些特征的重要性。<strong>对于每个特征学会一个特征权重，然后再把学习到的权重乘到对应特征的Embedding里，这样就可以动态学习特征权重，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的</strong>。在推荐系统里面， 结构长这个样子：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222040735.png" alt=""></p>
<p>下面看下这个网络里面的具体计算过程， SENET主要分为三个步骤Squeeze, Excitation, Re-weight。</p>
<ul>
<li><p><strong>在Squeeze阶段</strong>，我们对每个特征的Embedding向量进行数据压缩与信息汇总，如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222041255.png" alt=""></p>
<p>假设某个特征$v_i$是$k$维大小的$Embedding$，那么我们对$Embedding$里包含的$k$维数字求均值，得到能够代表这个特征汇总信息的数值 $z_i$，也就是说，把第$i$个特征的$Embedding$里的信息压缩到一个数值。原始版本的SENet，在这一步是对CNN的二维卷积核进行$Max$操作的，这里等于对某个特征Embedding元素求均值。我们试过，在推荐领域均值效果比$Max$效果好，这也很好理解，因为<strong>图像领域对卷积核元素求$Max$，等于找到最强的那个特征，而推荐领域的特征$Embedding$，每一位的数字都是有意义的，所以求均值能更好地保留和融合信息</strong>。通过Squeeze阶段，对于每个特征$v_i$ ，都压缩成了单个数值$z_i$，假设特征Embedding层有$f$个特征，就形成Squeeze向量$Z$，向量大小$f$。</p>
</li>
<li><p><strong>Excitation阶段</strong>，这个阶段引入了中间层比较窄的两层MLP网络，作用在Squeeze阶段的输出向量$Z$上，如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222042251.png" alt=""></p>
<p>$\sigma$非线性激活函数，一般$relu$。本质上，这是在做特征的交叉，也就是说，每个特征以一个$Bit$来表征，通过MLP来进行交互，通过交互，得出这么个结果：对于当前所有输入的特征，通过相互发生关联，来动态地判断哪些特征重要，哪些特征不重要。</p>
<p>其中，第一个MLP的作用是做特征交叉，第二个MLP的作用是为了保持输出的大小维度。因为假设Embedding层有$f$个特征，那么我们需要保证输出$f$个权重值，而第二个MLP就是起到将大小映射到$f$个数值大小的作用。</p>
<p>这样，经过两层MLP映射，就会产生$f$个权重数值，第$i$个数值对应第$i$个特征Embedding的权重$a_i$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222043024.png" alt=""></p>
<p>下面再分析下维度， SENet的输入是$E$，这个是<code>(None, f, k)</code>的维度， 通过Squeeze阶段，得到了<code>(None, f)</code>的矩阵，这个也就相当于Layer L1的输入(当然这里没有下面的偏置哈)，接下来过MLP1， 这里的$W_{1} \in R^{f \times \frac{f}{r}}, W_{2} \in R^{\frac{f}{r} \times f}$, 这里的$r$叫做reduction<br>ratio， $\frac{f}{r}$这个就是中间层神经元的个数， $r$表示了压缩的程度。</p>
</li>
</ul>
<ul>
<li><strong>Re-Weight</strong>，我们把Excitation阶段得到的每个特征对应的权重$a_i$，再乘回到特征对应的Embedding里，就完成了对特征重要性的加权操作。<script type="math/tex; mode=display">V=F_{\text {ReWeight }}(A, E)=\left[a_{1} \cdot e_{1}, \cdots, a_{f} \cdot e_{f}\right]=\left[v_{1}, \cdots, v_{f}\right]</script>$a_{i} \in R, e_{i} \in R^{k}$, and $v_{i} \in R^{k}$。$a_i$数值大，说明SENet判断这个特征在当前输入组合里比较重要， $a_i$数值小，说明SENet判断这个特征在当前输入组合里没啥用。如果非线性函数用Relu，会发现大量特征的权重会被Relu搞成0，也就是说，其实很多特征是没啥用的。</li>
</ul>
<p>这样，就可以将SENet引入推荐系统，用来对特征重要性进行动态判断。注意，<strong>所谓动态，指的是比如对于某个特征，在某个输入组合里可能是没用的，但是换一个输入组合，很可能是重要特征。它重要不重要，不是静态的，而是要根据当前输入，动态变化的</strong>。</p>
<p>这里正确的理解，算是一种特征重要性选择的思路， SENET和AFM的Attention网络是起着同样功效的一个网络。只不过那个是在特征交互之后进行特征交互重要性的选择，而这里是从embedding这里先压缩，再交互，再选择，去掉不太重要的特征。 <strong>考虑特征重要性上的两种考虑思路，难以说孰好孰坏，具体看应用场景</strong>。 不过如果分析下这个东西为啥会有效果， 就像张俊林老师提到的那样， 在Excitation阶段， 各个特征过了一个MLP进行了特征组合， 这样就真有可能过滤掉对于当前的交互不太重要的特征。 至于是不是， 那神经网络这东西就玄学了，让网络自己去学吧。</p>
<h6 id="Bilinear-Interaction-Layer"><a href="#Bilinear-Interaction-Layer" class="headerlink" title="Bilinear-Interaction Layer"></a>Bilinear-Interaction Layer</h6><p>特征重要性选择完事， 接下来就是研究特征交互， 这里作者直接就列出了目前的两种常用交互以及双线性交互:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222048779.png" alt=""></p>
<p>这个图其实非常了然了。以往模型用的交互， 内积的方式(FM,FFM)这种或者哈达玛积的方式(NFM,AFM)这种。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222048554.png" alt=""></p>
<p>所谓的双线性，其实就是组合了内积和哈达玛积的操作，看上面的右图。就是在$v_i$和$v_j$之间先加一个$W$矩阵， 这个$W$矩阵的维度是$(f,f)$, $v_i, v_j$是$(1,f)$的向量。 先让$v_i$与$W$内积，得到$(1,f)$的向量，这时候先仔细体会下这个<strong>新向量的每个元素，相当于是原来向量$v_i$在每个维度上的线性组合了</strong>。这时候再与$v_j$进行哈达玛积得到结果。</p>
<p>这里我不由自主的考虑了下双线性的功效，也就是为啥作者会说双线性是细粒度，下面是我自己的看法哈。</p>
<ul>
<li>如果我们单独先看内积操作，特征交互如果是两个向量直接内积，这时候， 结果大的，说明两个向量相似或者特征相似， 但向量内积，其实是相当于向量的各个维度先对应位置元素相乘再相加求和。 这个过程中认为的是向量的各个维度信息的重要性是一致的。类似于$v_1+v_2+..v_k$， 但真的一致吗？ —- <strong>内积操作没有考虑向量各个维度的重要性</strong></li>
<li>如果我们单独看哈达玛积操作， 特征交互如果是两个向量哈达玛积，这时候，是各个维度对应位置元素相乘得到一个向量， 而这个向量往往后面会进行线性或者非线性交叉的操作， 最后可能也会得到具体某个数值，但是这里经过了线性或者非线性交叉操作之后， 有没有感觉把向量各个维度信息的重要性考虑了进来？   就类似于$w_1v_{i1j1}+w_2k_{v2j2},…w_kv_{vkjk}$。 如果模型认为重要性相同，那么哈达玛积还有希望退化成内积，所以哈达玛积感觉考虑的比内积就多了一些。 —- <strong>哈达玛积操作自身也没有考虑各个维度重要性，但通过后面的线性或者非线性操作，有一定的维度重要性在里面</strong></li>
<li>再看看这个双线性， 是先内积再哈达玛积。这个内积操作不是直接$v_i$和$v_j$内积，而是中间引入了个$W$矩阵，参数可学习。 那么$v_i$和$W$做内积之后，虽然得到了同样大小的向量，但是这个向量是$v_i$各个维度元素的线性组合，相当于$v_i$变成了$[w_{11}v_{i1}+…w_{1k}v_{ik}, w_{21}v_{i1}+..w_{2k}v_{ik}, …., w_{k1}v_{i1}+…w_{kk}v_{ik}]$， 这时候再与$v_j$哈达玛积的功效，就变成了$[(w_{11}v_{i1}+…w_{1k}v_{ik})v_{j1}, (w_{21}v_{i1}+..w_{2k}v_{ik})v_{j2}, …., (w_{k1}v_{i1}+…w_{kk}v_{ik})v_{j_k}]$， 这时候，就可以看到，如果这里的$W$是个对角矩阵，那么这里就退化成了哈达玛积。  所以双线性感觉考虑的又比哈达玛积多了一些。如果后面再走一个非线性操作的话，就会发现这里同时考虑了两个交互向量各个维度上的重要性。—-<strong>双线性操作同时可以考虑交互的向量各自的各个维度上的重要性信息， 这应该是作者所说的细粒度，各个维度上的重要性</strong></li>
</ul>
<p><strong>当然思路是思路，双线性并不一定见得一定比哈达玛积有效， SENET也不一定就会比原始embedding要好，一定要辩证看问题</strong></p>
<p>这里还有个厉害的地方在于这里的W有三种选择方式，也就是三种类型的双线性交互方式。</p>
<ol>
<li>Field-All Type</li>
</ol>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222049946.png" alt=""></p>
<p>也就是所有的特征embedding共用一个$W$矩阵，这也是Field-All的名字来源。$W \in R^{k \times k}, \text { and } v_{i}, v_{j} \in R^{k}$。这种方式最简单</p>
<ol>
<li>Field-Each Type</li>
</ol>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222050330.png" alt=""></p>
<p>每个特征embedding共用一个$W$矩阵， 那么如果有$f$个特征的话，这里的$W_i$需要$f$个。所以这里的参数个数$f-1\times k\times k$， 这里的$f-1$是因为两两组合之后，比如<code>[0,1,2]</code>， 两两组合<code>[0,1], [0,2], [1,2]</code>。 这里用到的域是0和1。</p>
<ol>
<li>Field-Interaction Type</li>
</ol>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222050837.png" alt=""></p>
<p>每组特征交互的时候，用一个$W$矩阵， 那么这里如果有$f$个特征的话，需要$W_{ij}$是$\frac{f(f-1)}{2}$个。参数个数$\frac{f(f-1)}{2}\times k\times k$个。</p>
<p>不知道看到这里，这种操作有没有种似曾相识的感觉， 有没有想起FM和FFM， 反正我是不自觉的想起了哈哈，不知道为啥。总感觉FM的风格和上面的Field-All很像， 而FFM和下面的Field-Interaction很像。</p>
<p>我们的原始embedding和SKNET-like embedding都需要过这个层，那么得到的就是一个双线性两两组合的矩阵， 维度是$(\frac{f(f-1)}{2}, k)$的矩阵。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222051074.png" alt=""></p>
<h6 id="Combination-Layer"><a href="#Combination-Layer" class="headerlink" title="Combination Layer"></a>Combination Layer</h6><p>这个层的作用就是把目前得到的特征拼起来</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222051287.png" alt=""></p>
<p>这里他直拼了上面得到的两个离散特征通过各种交互之后的形式，如果是还有连续特征的话，也可以在这里拼起来，然后过DNN，不过这里其实还省略了一步操作就是Flatten，先展平再拼接。</p>
<h6 id="DNN和输出层"><a href="#DNN和输出层" class="headerlink" title="DNN和输出层"></a>DNN和输出层</h6><p> DNN的话普通的全连接网络， 再捕捉一波高阶的隐性交互。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222052753.png" alt=""></p>
<p>而输出层</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222052222.png" alt=""></p>
<p>分类问题损失函数：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222053977.png" alt=""></p>
<h3 id="Wide-amp-Deep系列"><a href="#Wide-amp-Deep系列" class="headerlink" title="Wide&amp;Deep系列"></a>Wide&amp;Deep系列</h3><h4 id="Wide-amp-Deep"><a href="#Wide-amp-Deep" class="headerlink" title="Wide&amp;Deep"></a>Wide&amp;Deep</h4><h5 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h5><p>在CTR预估任务中利用手工构造的交叉组合特征来使线性模型具有“记忆性”，使模型记住共现频率较高的特征组合，往往也能达到一个不错的baseline，且可解释性强。但这种方式有着较为明显的缺点：</p>
<ol>
<li>特征工程需要耗费太多精力。</li>
<li>模型是强行记住这些组合特征的，对于未曾出现过的特征组合，权重系数为0，无法进行泛化。</li>
</ol>
<p>为了加强模型的泛化能力，研究者引入了DNN结构，将高维稀疏特征编码为低维稠密的Embedding vector，这种基于Embedding的方式能够有效提高模型的泛化能力。但是，基于Embedding的方式可能因为数据长尾分布，导致长尾的一些特征值无法被充分学习，其对应的Embedding vector是不准确的，这便会造成模型泛化过度。</p>
<p>Wide&amp;Deep模型就是围绕记忆性和泛化性进行讨论的，模型能够从历史数据中学习到高频共现的特征组合的能力，称为是模型的Memorization。能够利用特征之间的传递性去探索历史数据中从未出现过的特征组合，称为是模型的Generalization。Wide&amp;Deep兼顾Memorization与Generalization并在Google Play store的场景中成功落地。</p>
<h5 id="模型结构及原理-2"><a href="#模型结构及原理-2" class="headerlink" title="模型结构及原理"></a>模型结构及原理</h5><p>其实wide&amp;deep模型本身的结构是非常简单的，对于有点机器学习基础和深度学习基础的人来说都非常的容易看懂，但是如何根据自己的场景去选择那些特征放在Wide部分，哪些特征放在Deep部分就需要理解这篇论文提出者当时对于设计该模型不同结构时的意图了，所以这也是用好这个模型的一个前提。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222057071.png" alt=""></p>
<p><strong>如何理解Wide部分有利于增强模型的“记忆能力”，Deep部分有利于增强模型的“泛化能力”？</strong></p>
<ul>
<li><p>wide部分是一个广义的线性模型，输入的特征主要有两部分组成，一部分是原始的部分特征，另一部分是原始特征的交叉特征(cross-product transformation)，对于交互特征可以定义为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222059178.png" alt=""></p>
<p>$c_{ki}$是一个布尔变量，当第i个特征属于第k个特征组合时，$c_{ki}$的值为1，否则为0，$x_i$是第i个特征的值，大体意思就是两个特征都同时为1这个新的特征才能为1，否则就是0，说白了就是一个特征组合。</p>
<p>对于wide部分训练时候使用的优化器是带$L_1$正则的FTRL算法(Follow-the-regularized-leader)，而L1 FTLR是非常注重模型稀疏性质的，也就是说W&amp;D模型采用L1 FTRL是想让Wide部分变得更加的稀疏，即Wide部分的大部分参数都为0，这就大大压缩了模型权重及特征向量的维度。<strong>Wide部分模型训练完之后留下来的特征都是非常重要的，那么模型的“记忆能力”就可以理解为发现”直接的”，“暴力的”，“显然的”关联规则的能力。</strong>例如Google W&amp;D期望wide部分发现这样的规则：<strong>用户安装了应用A，此时曝光应用B，用户安装应用B的概率大。</strong></p>
</li>
<li><p>Deep部分是一个DNN模型，输入的特征主要分为两大类，一类是数值特征(可直接输入DNN)，一类是类别特征(需要经过Embedding之后才能输入到DNN中)，Deep部分的数学形式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222100079.png" alt=""></p>
<p><strong>我们知道DNN模型随着层数的增加，中间的特征就越抽象，也就提高了模型的泛化能力。</strong>对于Deep部分的DNN模型作者使用了深度学习常用的优化器AdaGrad，这也是为了使得模型可以得到更精确的解。</p>
</li>
</ul>
<p><strong>Wide部分与Deep部分的结合</strong></p>
<p>W&amp;D模型是将两部分输出的结果结合起来联合训练，将deep和wide部分的输出重新使用一个逻辑回归模型做最终的预测，输出概率值。联合训练的数学形式如下：需要注意的是，因为Wide侧的数据是高维稀疏的，所以作者使用了FTRL算法优化，而Deep侧使用的是 Adagrad。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222100694.png" alt=""></p>
<h4 id="NFM"><a href="#NFM" class="headerlink" title="NFM"></a>NFM</h4><h5 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h5><p>NFM(Neural Factorization Machines)是2017年由新加坡国立大学的何向南教授等人在SIGIR会议上提出的一个模型，传统的FM模型仅局限于线性表达和二阶交互， 无法胜任生活中各种具有复杂结构和规律性的真实数据， 针对FM的这点不足， 作者提出了一种将FM融合进DNN的策略，通过引进了一个特征交叉池化层的结构，使得FM与DNN进行了完美衔接，这样就组合了FM的建模低阶特征交互能力和DNN学习高阶特征交互和非线性的能力，形成了深度学习时代的神经FM模型(NFM)。</p>
<p>那么NFM具体是怎么做的呢？ 首先看一下NFM的公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222103700.png" alt=""></p>
<p>我们对比FM， 就会发现变化的是第三项，前两项还是原来的， 因为我们说FM的一个问题，就是只能到二阶交叉， 且是线性模型， 这是他本身的一个局限性， 而如果想突破这个局限性， 就需要从他的公式本身下点功夫， 于是乎，作者在这里改进的思路就是<strong>用一个表达能力更强的函数来替代原FM中二阶隐向量内积的部分</strong>。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222104051.png" alt=""></p>
<h5 id="模型结构与原理"><a href="#模型结构与原理" class="headerlink" title="模型结构与原理"></a>模型结构与原理</h5><h6 id="Input-和Embedding层"><a href="#Input-和Embedding层" class="headerlink" title="Input 和Embedding层"></a>Input 和Embedding层</h6><p>输入层的特征， 文章指定了稀疏离散特征居多， 这种特征我们也知道一般是先one-hot, 然后会通过embedding，处理成稠密低维的。 所以这两层还是和之前一样，假设$\mathbf{v}_{\mathbf{i}} \in \mathbb{R}^{k}$为第$i$个特征的embedding向量， 那么$\mathcal{V}_{x}=\left\{x_{1} \mathbf{v}_{1}, \ldots, x_{n} \mathbf{v}_{n}\right\}$表示的下一层的输入特征。这里带上了$x_i$是因为很多$x_i$转成了One-hot之后，出现很多为0的， 这里的$\{x_iv_i\}$是$x_i$不等于0的那些特征向量。  </p>
<h6 id="Bi-Interaction-Pooling-layer"><a href="#Bi-Interaction-Pooling-layer" class="headerlink" title="Bi-Interaction Pooling layer"></a>Bi-Interaction Pooling layer</h6><p>在Embedding层和神经网络之间加入了特征交叉池化层是本网络的核心创新了，正是因为这个结构，实现了FM与DNN的无缝连接， 组成了一个大的网络，且能够正常的反向传播。假设$\mathcal{V}_{x}$是所有特征embedding的集合， 那么在特征交叉池化层的操作：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222106689.png" alt=""></p>
<p>$\odot$表示两个向量的元素积操作，即两个向量对应维度相乘得到的元素积向量（可不是点乘呀），其中第$k$维的操作：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222106134.png" alt=""></p>
<p>这便定义了在embedding空间特征的二阶交互，这个不仔细看会和感觉FM的最后一项很像，但是不一样，一定要注意这个地方不是两个隐向量的内积，而是元素积，也就是这一个交叉完了之后k个维度不求和，最后会得到一个$k$维向量，而FM那里内积的话最后得到一个数， 在进行两两Embedding元素积之后，对交叉特征向量取和， 得到该层的输出向量， 很显然， 输出是一个$k$维的向量。</p>
<p>注意， 之前的FM到这里其实就完事了， 上面就是输出了，而这里很大的一点改进就是加入特征池化层之后， 把二阶交互的信息合并， 且上面接了一个DNN网络， 这样就能够增强FM的表达能力了， 因为FM只能到二阶， 而这里的DNN可以进行多阶且非线性，只要FM把二阶的学习好了， DNN这块学习来会更加容易， 作者在论文中也说明了这一点，且通过后面的实验证实了这个观点。</p>
<p>如果不加DNN， NFM就退化成了FM，所以改进的关键就在于加了一个这样的层，组合了一下二阶交叉的信息，然后又给了DNN进行高阶交叉的学习，成了一种“加强版”的FM。</p>
<p>Bi-Interaction层不需要额外的模型学习参数，更重要的是它在一个线性的时间内完成计算，和FM一致的，即时间复杂度为$O\left(k N_{x}\right)$，$N_x$为embedding向量的数量。参考FM，可以将上式转化为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222107108.png" alt=""></p>
<h6 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h6><p>这一层就是全连接的神经网络， DNN在进行特征的高层非线性交互上有着天然的学习优势，公式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222109689.png" alt=""></p>
<p>这里的$\sigma_i$是第$i$层的激活函数，可不要理解成sigmoid激活函数。</p>
<h6 id="预测层"><a href="#预测层" class="headerlink" title="预测层"></a>预测层</h6><p>这个就是最后一层的结果直接过一个隐藏层，但注意由于这里是回归问题，没有加sigmoid激活：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222110388.png" alt=""></p>
<p>所以， NFM模型的前向传播过程总结如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222111207.png" alt=""></p>
<p>这就是NFM模型的全貌， NFM相比较于其他模型的核心创新点是特征交叉池化层，基于它，实现了FM和DNN的无缝连接，使得DNN可以在底层就学习到包含更多信息的组合特征，这时候，就会减少DNN的很多负担，只需要很少的隐藏层就可以学习到高阶特征信息。NFM相比之前的DNN， 模型结构更浅，更简单，但是性能更好，训练和调参更容易。集合FM二阶交叉线性和DNN高阶交叉非线性的优势，非常适合处理稀疏数据的场景任务。在对NFM的真实训练过程中，也会用到像Dropout和BatchNormalization这样的技术来缓解过拟合和在过大的改变数据分布。</p>
<h4 id="AFM"><a href="#AFM" class="headerlink" title="AFM"></a>AFM</h4><h5 id="AFM提出的动机"><a href="#AFM提出的动机" class="headerlink" title="AFM提出的动机"></a>AFM提出的动机</h5><p>AFM的全称是Attentional Factorization Machines, 从模型的名称上来看是在FM的基础上加上了注意力机制，FM是通过特征隐向量的内积来对交叉特征进行建模，从公式中可以看出所有的交叉特征都具有相同的权重也就是1，没有考虑到不同的交叉特征的重要性程度：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222115177.png" alt=""></p>
<p>如何让不同的交叉特征具有不同的重要性就是AFM核心的贡献，在谈论AFM交叉特征注意力之前，对于FM交叉特征部分的改进还有FFM，其是考虑到了对于不同的其他特征，某个指定特征的隐向量应该是不同的（相比于FM对于所有的特征只有一个隐向量，FFM对于一个特征有多个不同的隐向量）。</p>
<h5 id="AFM模型原理"><a href="#AFM模型原理" class="headerlink" title="AFM模型原理"></a>AFM模型原理</h5><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222115513.png" alt=""></p>
<p>上图表示的就是AFM交叉特征部分的模型结构(非交叉部分与FM是一样的，图中并没有给出)。AFM最核心的两个点分别是Pair-wise Interaction Layer和Attention-based Pooling。前者将输入的非零特征的隐向量两两计算element-wise product(哈达玛积，两个向量对应元素相乘，得到的还是一个向量)，假如输入的特征中的非零向量的数量为m，那么经过Pair-wise Interaction Layer之后输出的就是$\frac{m(m-1)}{2}$个向量，再将前面得到的交叉特征向量组输入到Attention-based Pooling，该pooling层会先计算出每个特征组合的自适应权重(通过Attention Net进行计算)，通过加权求和的方式将向量组压缩成一个向量，由于最终需要输出的是一个数值，所以还需要将前一步得到的向量通过另外一个向量将其映射成一个值，得到最终的基于注意力加权的二阶交叉特征的输出。(对于这部分如果不是很清楚，可以先看下面对两个核心层的介绍)</p>
<h6 id="Pair-wise-Interaction-Layer"><a href="#Pair-wise-Interaction-Layer" class="headerlink" title="Pair-wise Interaction Layer"></a>Pair-wise Interaction Layer</h6><p>FM二阶交叉项：所有非零特征对应的隐向量两两点积再求和，输出的是一个数值</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222116327.png" alt=""></p>
<p>AFM二阶交叉项(无attention)：所有非零特征对应的隐向量两两对应元素乘积，然后再向量求和，输出的还是一个向量。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222116575.png" alt=""></p>
<p>上述写法是为了更好的与FM进行对比，下面将公式变形方便与原论文中保持一致。首先是特征的隐向量。从上图中可以看出，作者对数值特征也对应了一个隐向量，不同的数值乘以对应的隐向量就可以得到不同的隐向量，相对于onehot编码的特征乘以1还是其本身(并没有什么变化)，其实就是为了将公式进行统一。</p>
<p>按照论文的意思，特征的embedding可以表示为：$\varepsilon = {v_ix_i}$，经过Pair-wise Interaction Layer输出可得：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222116226.png" alt=""></p>
<p>$R_x$表示的是有效特征集合。此时的$f_{PI}(\varepsilon)$表示的是一个向量集合，所以需要先将这些向量集合聚合成一个向量，然后在转换成一个数值：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222117210.png" alt=""></p>
<p>上式中的求和部分就是将向量集合聚合成一个维度与隐向量维度相同的向量，通过向量$p$再将其转换成一个数值，b表示的是偏置。</p>
<p>从开始介绍Pair-wise Interaction Layer到现在解决的一个问题是，如何将使用哈达玛积得到的交叉特征转换成一个最终输出需要的数值，到目前为止交叉特征之间的注意力权重还没有出现。在没有详细介绍注意力之前先感性的认识一下如果现在已经有了每个交叉特征的注意力权重，那么交叉特征的输出可以表示为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222117026.png" alt=""></p>
<p>就是在交叉特征得到的新向量前面乘以一个注意力权重$\alpha_{ij}$, 那么这个注意力权重如何计算得到呢？</p>
<h6 id="Attention-based-Pooling"><a href="#Attention-based-Pooling" class="headerlink" title="Attention-based Pooling"></a>Attention-based Pooling</h6><p>对于神经网络注意力相关的基础知识大家可以去看一下邱锡鹏老师的《神经网络与深度学习》第8章注意力机制与外部记忆。这里简单的叙述一下使用MLP实现注意力机制的计算。假设现在有n个交叉特征(假如维度是k)，将nxk的数据输入到一个kx1的全连接网络中，输出的张量维度为nx1，使用softmax函数将nx1的向量的每个维度进行归一化，得到一个新的nx1的向量，这个向量所有维度加起来的和为1，每个维度上的值就可以表示原nxk数据每一行(即1xk的数据)的权重。用公式表示为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222117989.png" alt=""></p>
<p>使用softmax归一化可得：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222118339.png" alt=""></p>
<p>这样就得到了AFM二阶交叉部分的注意力权重，如果将AFM的一阶项写在一起，AFM模型用公式表示为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222118174.png" alt=""></p>
<h6 id="AFM模型训练"><a href="#AFM模型训练" class="headerlink" title="AFM模型训练"></a>AFM模型训练</h6><p>AFM从最终的模型公式可以看出与FM的模型公式是非常相似的，所以也可以和FM一样应用于不同的任务，例如分类、回归及排序（不同的任务的损失函数是不一样的），AFM也有对防止过拟合进行处理：</p>
<ol>
<li>在Pair-wise Interaction Layer层的输出结果上使用dropout防止过拟合，因为并不是所有的特征组合对预测结果都有用，所以随机的去除一些交叉特征，让剩下的特征去自适应的学习可以更好的防止过拟合。</li>
<li>对Attention-based Pooling层中的权重矩阵$W$使用L2正则，作者没有在这一层使用dropout的原因是发现同时在特征交叉层和注意力层加dropout会使得模型训练不稳定，并且性能还会下降。</li>
</ol>
<p>加上正则参数之后的回归任务的损失函数表示为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222119465.png" alt=""></p>
<h4 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h4><h5 id="动机-2"><a href="#动机-2" class="headerlink" title="动机"></a>动机</h5><p>对于CTR问题，被证明的最有效的提升任务表现的策略是特征组合(Feature Interaction), 在CTR问题的探究历史上来看就是如何更好地学习特征组合，进而更加精确地描述数据的特点。可以说这是基础推荐模型到深度学习推荐模型遵循的一个主要的思想。而组合特征大牛们研究过组合二阶特征，三阶甚至更高阶，但是面临一个问题就是随着阶数的提升，复杂度就成几何倍的升高。这样即使模型的表现更好了，但是推荐系统在实时性的要求也不能满足了。所以很多模型的出现都是为了解决另外一个更加深入的问题：如何更高效的学习特征组合？</p>
<p>为了解决上述问题，出现了FM和FFM来优化LR的特征组合较差这一个问题。并且在这个时候科学家们已经发现了DNN在特征组合方面的优势，所以又出现了FNN和PNN等使用深度网络的模型。但是DNN也存在局限性。</p>
<ul>
<li><strong>DNN局限</strong><br>当我们使用DNN网络解决推荐问题的时候存在网络参数过于庞大的问题，这是因为在进行特征处理的时候我们需要使用one-hot编码来处理离散特征，这会导致输入的维度猛增。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222138309.png" alt=""></p>
<p>这样庞大的参数量也是不实际的。为了解决DNN参数量过大的局限性，可以采用非常经典的Field思想，将OneHot特征转换为Dense Vector</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222138450.png" alt=""></p>
<p>此时通过增加全连接层就可以实现高阶的特征组合，如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222139876.png" alt=""></p>
<p>但是仍然缺少低阶的特征组合，于是增加FM来表示低阶的特征组合。</p>
<ul>
<li><strong>FNN和PNN</strong><br>结合FM和DNN其实有两种方式，可以并行结合也可以串行结合。这两种方式各有几种代表模型。在DeepFM之前有FNN，虽然在影响力上可能并不如DeepFM，但是了解FNN的思想对我们理解DeepFM的特点和优点是很有帮助的。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222139053.png" alt=""></p>
<p>FNN是使用预训练好的FM模块，得到隐向量，然后把隐向量作为DNN的输入，但是经过实验进一步发现，在Embedding layer和hidden layer1之间增加一个product层（如上图所示）可以提高模型的表现，所以提出了PNN，使用product layer替换FM预训练层。</p>
<ul>
<li><strong>Wide&amp;Deep</strong><br>FNN和PNN模型仍然有一个比较明显的尚未解决的缺点：对于低阶组合特征学习到的比较少，这一点主要是由于FM和DNN的串行方式导致的，也就是虽然FM学到了低阶特征组合，但是DNN的全连接结构导致低阶特征并不能在DNN的输出端较好的表现。看来我们已经找到问题了，将串行方式改进为并行方式能比较好的解决这个问题。于是Google提出了Wide&amp;Deep模型（将前几章），但是如果深入探究Wide&amp;Deep的构成方式，虽然将整个模型的结构调整为了并行结构，在实际的使用中Wide Module中的部分需要较为精巧的特征工程，换句话说人工处理对于模型的效果具有比较大的影响（这一点可以在Wide&amp;Deep模型部分得到验证）。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222140078.png" alt=""></p>
<p>如上图所示，该模型仍然存在问题：<strong>在output Units阶段直接将低阶和高阶特征进行组合，很容易让模型最终偏向学习到低阶或者高阶的特征，而不能做到很好的结合。</strong></p>
<p>综上所示，DeepFM模型横空出世。</p>
<h5 id="模型的结构与原理"><a href="#模型的结构与原理" class="headerlink" title="模型的结构与原理"></a>模型的结构与原理</h5><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222140551.png" alt=""></p>
<p>前面的Field和Embedding处理是和前面的方法是相同的，如上图中的绿色部分；DeepFM将Wide部分替换为了FM layer如上图中的蓝色部分</p>
<p>这幅图其实有很多的点需要注意，很多人都一眼略过了，这里我个人认为在DeepFM模型中有三点需要注意：</p>
<ul>
<li><strong>Deep模型部分</strong></li>
<li><strong>FM模型部分</strong></li>
<li><strong>Sparse Feature中黄色和灰色节点代表什么意思</strong></li>
</ul>
<h6 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h6><p>详细内容参考FM模型部分的内容，下图是FM的一个结构图，从图中大致可以看出FM Layer是由一阶特征和二阶特征Concatenate到一起在经过一个Sigmoid得到logits（结合FM的公式一起看），所以在实现的时候需要单独考虑linear部分和FM交叉特征部分。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222141214.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222141391.png" alt=""></p>
<h6 id="Deep"><a href="#Deep" class="headerlink" title="Deep"></a>Deep</h6><p>Deep架构图</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222142236.png" alt=""></p>
<p>Deep Module是为了学习高阶的特征组合，在上图中使用用全连接的方式将Dense Embedding输入到Hidden Layer，这里面Dense Embeddings就是为了解决DNN中的参数爆炸问题，这也是推荐模型中常用的处理方法。</p>
<p>Embedding层的输出是将所有id类特征对应的embedding向量concat到到一起输入到DNN中。其中$v_i$表示第i个field的embedding，m是field的数量。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222143624.png" alt=""></p>
<p>上一层的输出作为下一层的输入，我们得到：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222143038.png" alt=""></p>
<p>其中$\sigma$表示激活函数，$z, W, b $分别表示该层的输入、权重和偏置。</p>
<p>最后进入DNN部分输出使用sigmod激活函数进行激活：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222143674.png" alt=""></p>
<h4 id="xDeepFM"><a href="#xDeepFM" class="headerlink" title="xDeepFM"></a>xDeepFM</h4><h5 id="xDeepFM的架构剖析"><a href="#xDeepFM的架构剖析" class="headerlink" title="xDeepFM的架构剖析"></a>xDeepFM的架构剖析</h5><p>首先，我们先看下xDeepFM的架构</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222202895.png" alt=""></p>
<p>这个网络结构名副其实，依然是采用了W&amp;D架构，DNN负责Deep端，学习特征之间的隐性高阶交互， 而CIN网络负责wide端，学习特征之间的显性高阶交互，这样显隐性高阶交互就在这个模型里面体现的淋漓尽致了。</p>
<p>最终的计算公式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222204455.png" alt=""></p>
<p>这里的$\mathbf{a}$表示原始的特征，$\mathbf{a}_{dnn}^k$表示的是DNN的输出， $\mathbf{p}^+$表示的是CIN的输出。最终的损失依然是交叉熵损失，这里也是做一个点击率预测的问题：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222205209.png" alt=""></p>
<p>最终的目标函数加了正则化:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222206641.png" alt=""></p>
<h5 id="CIN网络的细节"><a href="#CIN网络的细节" class="headerlink" title="CIN网络的细节"></a>CIN网络的细节</h5><p>这里尝试剖析下本篇论文的主角CIN网络，全称Compressed Interaction Network。这个东西说白了其实也是一个网络，并不是什么高大上的东西，和Cross Network一样，也是一层一层，每一层都是基于一个固定的公式进行的计算，那个公式长这样:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222209233.png" alt=""></p>
<p> 这个公式第一眼看过来，肯定更是懵逼，这是写的个啥玩意？如果我再把CIN的三个核心图放上来:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222209772.png" alt=""></p>
<p> 上面其实就是CIN网络的精髓了，也是它具体的运算过程，只不过直接上图的话，会有些抽象，难以理解，也不符合我整理论文的习惯。下面，我们就一一进行剖析， 先从上面这个公式开始。但在这之前，需要先约定一些符号。要不然不知道代表啥意思。</p>
<ol>
<li>$\mathbf{X}^{0} \in \mathbb{R}^{m \times D}$: 这个就是我们的输入，也就是embedding层的输出，可以理解为各个embedding的堆叠而成的矩阵，假设有$m$个特征，embedding的维度是$D$维，那么这样就得到了这样的矩阵， $m$行$D$列。$\mathbf{X}_{i, *}^{0}=\mathbf{e}_{i}$， 这个表示的是第$i$个特征的embedding向量$e_i$。所以上标在这里表示的是网络的层数，输入可以看做第0层，而下标表示的第几行的embedding向量，这个清楚了。</li>
<li>$\mathbf{X}^{k} \in \mathbb{R}^{H_{k} \times D}$: 这个表示的是CIN网络第$k$层的输出，和上面这个一样，也是一个矩阵，每一行是一个embedding向量，每一列代表一个embedding维度。这里的$H_k$表示的是第$k$层特征的数量，也可以理解为神经元个数。那么显然，这个$\mathbf{X}^{k}$就是$H_k$个$D$为向量堆叠而成的矩阵，维度也显然了。$\mathbf{X}_{h, *}^{k}$代表的就是第$k$层第$h$个特征向量了。</li>
</ol>
<p>所以上面的那个公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222210887.png" alt=""></p>
<p>其实就是计算第$k$层第$h$个特征向量， 这里的$1 \leq h \leq H_{k}, \mathbf{W}^{k, h} \in \mathbb{R}^{H_{k-1} \times m}$是第$h$个特征向量的参数矩阵。 $\circ$表示的哈达玛积，也就是向量之间对应维度元素相乘(不相加了)。$\left\langle a_{1}, a_{2}, a_{3}\right\rangle \circ\left\langle b_{1}, b_{2}, b_{3}\right\rangle=\left\langle a_{1} b_{1}, a_{2} b_{2}, a_{3} b_{3}\right\rangle$。通过这个公式也能看到$\mathbf{X}^k$是通过$\mathbf{X}^{k-1}$和$\mathbf{X}^0$计算得来的，也就是说特征的显性交互阶数会虽然网络层数的加深而增加。</p>
<p>那么这个公式到底表示的啥意思呢？ 是具体怎么计算的呢？我们往前计算一层就知道了，这里令$k=1$，也就是尝试计算第一层里面的第$h$个向量， 那么上面公式就变成了:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222210151.png" alt=""></p>
<p>这里的$\mathbf{W}^{1, h} \in \mathbb{R}^{H_{0} \times m}$。这个能看懂吗？ 首先这个$\mathbf{W}$矩阵是$H_0$行$m$列， 而前面那两个累加正好也是$H_0$行$m$列的参数。$m$代表的是输入特征的个数， $H_0$代表的是第0层($k-1$层)的神经元的个数， 这个也是$m$。这个应该好理解，输入层就是第0层。所以这其实就是一个$m\times m$的矩阵。那么后面这个运算到底是怎么算的呢？   首先对于第$i$个特征向量， 要依次和其他的$m$个特征向量做哈达玛积操作，当然也乘以对应位置的权重，求和。对于每个$i$特征向量，都重复这样的操作，最终求和得到一个$D$维的向量，这个就是$\mathbf{X}_{h, *}^{1}$。好吧，这么说。我觉得应该也没有啥感觉，画一下就了然了，现在可以先不用管论文里面是怎么说的，先跟着这个思路走，只要理解了这个公式是怎么计算的，论文里面的那三个图就会非常清晰了。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222211775.png" alt=""></p>
<p>这就是上面那个公式的具体过程了，图实在是太难看了， 但应该能说明这个详细的过程了。这样只要给定一个$\mathbf{W}^{1,h}$之后，就能算出一个相应的$\mathbf{X}^1_{h,*}$来，这样第一层的$H_1$个神经元按照这样的步骤就能够都计算出来了。 后面的计算过程其实是同理，无非就是输入是前一层的输出以及$\mathbf{X}_0$罢了，而这时候，第一个矩阵特征数就不一定是$m$了，而是一个$H_{k-1}$行$D$列的矩阵了。这里的$\mathbf{W}^{k,h}$就是上面写的$H_{k-1}$行$m$列了。</p>
<p>这个过程明白了之后，再看论文后面的内容就相对容易了，首先</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222211538.png" alt=""></p>
<p>CIN里面能看到RNN的身影，也就是当前层的隐藏单元的计算要依赖于上一层以及当前的输入层，只不过这里的当前输入每个时间步都是$\mathbf{X}_0$。 同时这里也能看到，CIN的计算是vector-wise级别的，也就是向量之间的哈达玛积的操作，并没有涉及到具体向量里面的位交叉。</p>
<p>下面我们再从CNN的角度去看这个计算过程。其实还是和上面一样的计算过程，只不过是换了个角度看而已，所以上面那个只要能理解，下面CNN也容易理解了。首先，这里引入了一个tensor张量$\mathbf{Z}^{k+1}$表示的是$\mathbf{X}^k$和$\mathbf{X}^0$的外积，那么这个东西是啥呢？ 上面加权求和前的那个矩阵，是一个三维的张量。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222211833.png" alt=""></p>
<p>这个可以看成是一个三维的图片，$H_{k-1}$高，$m$宽，$D$个通道。而$\mathbf{W}^{k,h}$的大小是$H_{k-1}\times m$的， 这个就相当于一个过滤器，用这个过滤器对输入的图片如果<strong>逐通道进行卷积</strong>，就会最终得到一个$D$维的向量，而这个其实就是$\mathbf{X}^{k}_{h,*}$，也就是一张特征图(每个通道过滤器是共享的)。 第$k$层其实有$H_k$个这样的过滤器，所以最后得到的是一个$H_k\times D$的矩阵。这样，在第$k$个隐藏层，就把了$H_{k-1}\times m\times D$的三维张量通过逐通道卷积的方式，压缩成了一个$H_k\times D$的矩阵($H_k$张特征图)， 这就是第$k$层的输出$\mathbf{X}^k$。 而这也就是“compressed”的由来。这时候再看这两个图就非常舒服了：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222212318.png" alt=""></p>
<p>通过这样的一个CIN网络，就很容易的实现了特征的显性高阶交互，并且是vector-wise级别的，那么最终的输出层是啥呢？   通过上面的分析，首先我们了解了对于第$k$层输出的某个特征向量，其实是综合了输入里面各个embedding向量显性高阶交互的信息(第$k$层其实学习的输入embedding$k+1$阶交互信息)，这个看第一层那个输出就能看出来。第$k$层的每个特征向量其实都能学习到这样的信息，那么如果把这些向量在从$D$维度上进行加和，也就是$\mathbf{X}^k$，这是个$H_k\times D$的，我们沿着D这个维度加和，又会得到一个$H_k$的向量，公式如下:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222212723.png" alt=""></p>
<p> 每一层，都会得到一个这样的向量，那么把所有的向量拼接到一块，其实就是CIN网络的输出了。之所以，这里要把中间结果都与输出层相连，就是因为CIN与Cross不同的一点是，在第$k$层，CIN只包含$k+1$阶的组合特征，而Cross是能包含从1阶-$k+1$阶的组合特征的，所以为了让模型学习到从1阶到所有阶的组合特征，CIN这里需要把中间层的结果与输出层建立连接。</p>
<p>这也就是第三个图表示的含义:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222213526.png" alt=""></p>
<p>这样， 就得到了最终CIN的输出$\mathbf{p}^+$了: </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202305222213378.png" alt=""></p>
<p>后面那个维度的意思，就是说每一层是的向量维度是$H_i$维， 最后是所有时间步的维度之和。 </p>
<h3 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h3><p>pass</p>
<h3 id="多任务模型"><a href="#多任务模型" class="headerlink" title="多任务模型"></a>多任务模型</h3><p>pass</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%80%BB%E7%BB%93/" rel="tag"># 总结</a>
              <a href="/tags/%E9%9D%A2%E8%AF%95/" rel="tag"># 面试</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/11/22/%E7%AE%97%E6%B3%95Tricks/" rel="prev" title="算法Tricks">
                  <i class="fa fa-chevron-left"></i> 算法Tricks
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/06/14/Linux%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/" rel="next" title="Linux使用总结">
                  Linux使用总结 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">木霈玖</span>
</div>

    </div>
  </footer>

  
  <script src="//unpkg.com/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//unpkg.com/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
