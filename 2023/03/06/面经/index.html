<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//unpkg.com/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//unpkg.com/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="前言记录一下暑期实习面试经验。">
<meta property="og:type" content="article">
<meta property="og:title" content="面经">
<meta property="og:url" content="http://example.com/2023/03/06/%E9%9D%A2%E7%BB%8F/index.html">
<meta property="og:site_name" content="木霈玖的博客">
<meta property="og:description" content="前言记录一下暑期实习面试经验。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222347197.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222349037.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101955821.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102004442.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101950246.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102008358.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102010115.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102019772.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071555276.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071559636.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071107979.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304211122841.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304211122760.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071138731.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101613543.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101613198.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101650959.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101651061.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101656177.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101658315.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101704500.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101704670.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101716405.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101727004.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101743580.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101744292.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101744829.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101746573.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102112494.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102149708.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102152516.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102156269.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102208969.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102216271.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111538347.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111601603.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111619137.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071255957.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071300267.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20230106231925.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303212301932.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222128650.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222129537.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317819.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317983.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222332494.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071328562.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071332166.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071333924.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071335563.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071336003.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339814.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339457.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071340668.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071341013.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071342703.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071343619.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071356215.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101147762.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101148082.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101504342.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101504156.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101539655.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101545859.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101553212.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302227637.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302042116.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302053360.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302112845.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302133753.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302134012.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302137784.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302138098.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302139598.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302156146.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310947858.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310948367.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310952548.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311001584.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311002558.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311016054.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311035374.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311039224.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311047793.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311102674.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101646509.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101836412.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101836048.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101738369.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101841213.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202215784.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202218223.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210945142.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210948508.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202254827.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210936324.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210950233.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210958573.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202054636.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202055336.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202037061.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202037832.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202048519.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202104133.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202135669.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231131422.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231137649.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231146383.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231152053.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231153530.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231154583.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231206104.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231212913.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231213243.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231220341.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231221419.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231225576.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304191959346.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192002037.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192107149.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192121730.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192201890.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192209525.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192210293.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201650996.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201703206.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201704698.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201707323.png">
<meta property="article:published_time" content="2023-03-06T14:04:43.000Z">
<meta property="article:modified_time" content="2023-04-24T06:24:54.180Z">
<meta property="article:author" content="木霈玖">
<meta property="article:tag" content="总结">
<meta property="article:tag" content="面试">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222347197.png">


<link rel="canonical" href="http://example.com/2023/03/06/%E9%9D%A2%E7%BB%8F/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>面经 | 木霈玖的博客</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">木霈玖的博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E6%A8%A1%E5%9E%8B%E4%B8%8D%E6%94%B6%E6%95%9B%EF%BC%8C%E6%98%AF%E5%90%A6%E8%AF%B4%E6%98%8E%E8%BF%99%E4%B8%AA%E6%A8%A1%E5%9E%8B%E6%97%A0%E6%95%88%EF%BC%8C%E8%87%B4%E6%A8%A1%E5%9E%8B%E4%B8%8D%E6%94%B6%E6%95%9B%E7%9A%84%E5%8E%9F%E5%9B%A0%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="nav-number">2.</span> <span class="nav-text">训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8A%A0%E5%BF%AB%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%80%9F%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">加快模型训练速度的方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">4.</span> <span class="nav-text">提升模型泛化能力的方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="nav-number">5.</span> <span class="nav-text">偏差和方差</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">6.</span> <span class="nav-text">过拟合和欠拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5"><span class="nav-number">6.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">6.2.</span> <span class="nav-text">过拟合的解决方案</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">6.3.</span> <span class="nav-text">欠拟合的解决方案</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="nav-number">7.</span> <span class="nav-text">梯度消失和梯度爆炸</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5-1"><span class="nav-number">7.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">7.2.</span> <span class="nav-text">解决方案</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dropout"><span class="nav-number">8.</span> <span class="nav-text">Dropout</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4Dropout%E5%8F%AF%E4%BB%A5%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="nav-number">8.1.</span> <span class="nav-text">为什么说Dropout可以解决过拟合？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dropout%E7%BC%BA%E7%82%B9"><span class="nav-number">8.2.</span> <span class="nav-text">Dropout缺点</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">9.</span> <span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Smooth-L1-Loss"><span class="nav-number">9.1.</span> <span class="nav-text">Smooth L1 Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cross-Entropy-Loss"><span class="nav-number">9.2.</span> <span class="nav-text">Cross Entropy Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Binary-Cross-Entropy-Loss"><span class="nav-number">9.3.</span> <span class="nav-text">Binary Cross Entropy Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Focal-loss"><span class="nav-number">9.4.</span> <span class="nav-text">Focal loss</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">10.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sigmoid"><span class="nav-number">10.1.</span> <span class="nav-text">Sigmoid</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tanh"><span class="nav-number">10.2.</span> <span class="nav-text">tanh</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ReLU"><span class="nav-number">10.3.</span> <span class="nav-text">ReLU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Leaky-ReLU"><span class="nav-number">10.4.</span> <span class="nav-text">Leaky ReLU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ELU"><span class="nav-number">10.5.</span> <span class="nav-text">ELU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GELU"><span class="nav-number">10.6.</span> <span class="nav-text">GELU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ReLU%E6%AF%94Sigmoid%E6%95%88%E6%9E%9C%E5%A5%BD%E5%9C%A8%E5%93%AA%E9%87%8C%EF%BC%9F"><span class="nav-number">10.7.</span> <span class="nav-text">ReLU比Sigmoid效果好在哪里？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="nav-number">11.</span> <span class="nav-text">优化器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SGD"><span class="nav-number">11.1.</span> <span class="nav-text">SGD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SGDM"><span class="nav-number">11.2.</span> <span class="nav-text">SGDM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adagrad"><span class="nav-number">11.3.</span> <span class="nav-text">Adagrad</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RMSProp"><span class="nav-number">11.4.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adam"><span class="nav-number">11.5.</span> <span class="nav-text">Adam</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="nav-number">12.</span> <span class="nav-text">集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Bagging"><span class="nav-number">12.1.</span> <span class="nav-text">Bagging</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="nav-number">12.1.1.</span> <span class="nav-text">随机森林</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">12.1.1.1.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA%E7%9A%84%E7%BB%88%E6%AD%A2%E6%9D%A1%E4%BB%B6"><span class="nav-number">12.1.1.1.1.</span> <span class="nav-text">决策树构建的终止条件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%89%AA%E6%9E%9D"><span class="nav-number">12.1.1.1.2.</span> <span class="nav-text">决策树剪枝</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95"><span class="nav-number">12.1.1.1.3.</span> <span class="nav-text">决策树生成算法</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Boosting"><span class="nav-number">12.2.</span> <span class="nav-text">Boosting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaBoost"><span class="nav-number">12.2.1.</span> <span class="nav-text">AdaBoost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT"><span class="nav-number">12.2.2.</span> <span class="nav-text">GBDT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95"><span class="nav-number">12.2.2.1.</span> <span class="nav-text">提升树算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient-Boosting-Decision-Tree%EF%BC%88%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%89"><span class="nav-number">12.2.2.2.</span> <span class="nav-text">Gradient Boosting Decision Tree（梯度提升决策树）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#XGBoost"><span class="nav-number">12.2.3.</span> <span class="nav-text">XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="nav-number">12.2.3.1.</span> <span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F%E5%B1%95%E5%BC%80"><span class="nav-number">12.2.3.2.</span> <span class="nav-text">泰勒公式展开</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%91%E7%9A%84%E5%8F%82%E6%95%B0%E5%8C%96"><span class="nav-number">12.2.3.3.</span> <span class="nav-text">树的参数化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%88%86%E8%A3%82"><span class="nav-number">12.2.3.4.</span> <span class="nav-text">特征分裂</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LightGBM"><span class="nav-number">12.2.4.</span> <span class="nav-text">LightGBM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B4%E6%96%B9%E5%9B%BEHistogram%E7%AE%97%E6%B3%95"><span class="nav-number">12.2.4.1.</span> <span class="nav-text">直方图Histogram算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%A6%E6%B7%B1%E5%BA%A6%E9%99%90%E5%88%B6%E7%9A%84Leaf-wise%E7%9A%84%E5%8F%B6%E5%AD%90%E7%94%9F%E9%95%BF%E7%AD%96%E7%95%A5"><span class="nav-number">12.2.4.2.</span> <span class="nav-text">带深度限制的Leaf-wise的叶子生长策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E7%B1%BB%E5%88%AB%E7%89%B9%E5%BE%81"><span class="nav-number">12.2.4.3.</span> <span class="nav-text">支持类别特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%8D%95%E8%BE%B9%E9%87%87%E6%A0%B7-GOSS"><span class="nav-number">12.2.4.4.</span> <span class="nav-text">基于梯度的单边采样(GOSS)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%92%E6%96%A5%E7%89%B9%E5%BE%81%E7%BB%91%E5%AE%9A"><span class="nav-number">12.2.4.5.</span> <span class="nav-text">互斥特征绑定</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CatBoost"><span class="nav-number">12.2.5.</span> <span class="nav-text">CatBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%B1%BB%E5%88%AB%E7%89%B9%E5%BE%81%E7%9A%84Ordered-Target-Statistics%E6%95%B0%E5%80%BC%E7%BC%96%E7%A0%81%E6%96%B9%E6%B3%95"><span class="nav-number">12.2.5.1.</span> <span class="nav-text">基于类别特征的Ordered Target Statistics数值编码方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E8%B4%AA%E5%BF%83%E7%AD%96%E7%95%A5%E7%9A%84%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89%E6%96%B9%E6%B3%95"><span class="nav-number">12.2.5.2.</span> <span class="nav-text">基于贪心策略的特征交叉方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%81%BF%E5%85%8D%E9%A2%84%E6%B5%8B%E5%81%8F%E7%A7%BB%E7%9A%84Ordered-Boosting%E6%96%B9%E6%B3%95"><span class="nav-number">12.2.5.3.</span> <span class="nav-text">避免预测偏移的Ordered Boosting方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%AF%B9%E7%A7%B0%E4%BA%8C%E5%8F%89%E6%A0%91%E4%BD%9C%E4%B8%BA%E5%9F%BA%E6%A8%A1%E5%9E%8B"><span class="nav-number">12.2.5.4.</span> <span class="nav-text">使用对称二叉树作为基模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stacking"><span class="nav-number">12.3.</span> <span class="nav-text">Stacking</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">13.</span> <span class="nav-text">解决样本不均衡的方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LableSmoothing"><span class="nav-number">14.</span> <span class="nav-text">LableSmoothing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%AD%E5%B9%B3%E6%BB%91%E5%92%8C%E9%94%90%E5%8C%96%E6%93%8D%E4%BD%9C%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">15.</span> <span class="nav-text">图像处理中平滑和锐化操作是什么？</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5-2"><span class="nav-number">15.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">15.2.</span> <span class="nav-text">使用场景</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#batchsize"><span class="nav-number">16.</span> <span class="nav-text">batchsize</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SVM"><span class="nav-number">17.</span> <span class="nav-text">SVM</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CNN"><span class="nav-number">18.</span> <span class="nav-text">CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">18.1.</span> <span class="nav-text">卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%99%AE%E9%80%9A%E5%8D%B7%E7%A7%AF"><span class="nav-number">18.1.1.</span> <span class="nav-text">普通卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF-group-convolution"><span class="nav-number">18.1.2.</span> <span class="nav-text">分组卷积(group convolution)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">18.2.</span> <span class="nav-text">池化层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E6%9D%83%E5%80%BC%E5%85%B1%E4%BA%AB%E7%9A%84%E7%90%86%E8%A7%A3%EF%BC%9F"><span class="nav-number">18.3.</span> <span class="nav-text">神经网络中权值共享的理解？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E5%BE%AE%E8%B0%83-fine-tuning-%E7%9A%84%E7%90%86%E8%A7%A3%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BF%AE%E6%94%B9%E6%9C%80%E5%90%8E%E5%87%A0%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E5%80%BC%EF%BC%9F"><span class="nav-number">18.4.</span> <span class="nav-text">对微调(fine-tuning)的理解，为什么要修改最后几层神经网络权值？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LeNet-5"><span class="nav-number">18.5.</span> <span class="nav-text">LeNet-5</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AlexNet"><span class="nav-number">18.6.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VGG"><span class="nav-number">18.7.</span> <span class="nav-text">VGG</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GoogLeNet"><span class="nav-number">18.8.</span> <span class="nav-text">GoogLeNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ResNet"><span class="nav-number">18.9.</span> <span class="nav-text">ResNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%AE%E7%82%B9"><span class="nav-number">18.9.1.</span> <span class="nav-text">ResNet中的一些亮点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%87%E7%94%A8residual"><span class="nav-number">18.9.2.</span> <span class="nav-text">为什么采用residual?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#residual%E7%BB%93%E6%9E%84"><span class="nav-number">18.9.3.</span> <span class="nav-text">residual结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#residual%E7%9A%84%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F"><span class="nav-number">18.9.3.1.</span> <span class="nav-text">residual的计算方式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ResNet%E4%B8%AD%E4%B8%A4%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84residual"><span class="nav-number">18.9.3.2.</span> <span class="nav-text">ResNet中两种不同的residual</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BatchNormalization"><span class="nav-number">18.9.4.</span> <span class="nav-text">BatchNormalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%94%A8Dropout%EF%BC%9F"><span class="nav-number">18.9.5.</span> <span class="nav-text">ResNet为什么不用Dropout？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DenseNet"><span class="nav-number">18.10.</span> <span class="nav-text">DenseNet</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RNN"><span class="nav-number">19.</span> <span class="nav-text">RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-number">19.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81RNN%EF%BC%9F"><span class="nav-number">19.2.</span> <span class="nav-text">为什么需要RNN？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E4%B8%BB%E8%A6%81%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F"><span class="nav-number">19.3.</span> <span class="nav-text">RNN的主要应用领域</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="nav-number">19.4.</span> <span class="nav-text">RNN的计算过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E5%BB%BA%E6%A8%A1%E6%96%B9%E5%BC%8F"><span class="nav-number">19.5.</span> <span class="nav-text">RNN的建模方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E5%AF%B9%E5%A4%9A-vector-to-sequence"><span class="nav-number">19.5.1.</span> <span class="nav-text">一对多(vector-to-sequence)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%AF%B9%E4%B8%80-sequence-to-vector"><span class="nav-number">19.5.2.</span> <span class="nav-text">多对一(sequence-to-vector)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%AF%B9%E5%A4%9A-Encoder-Decoder"><span class="nav-number">19.5.3.</span> <span class="nav-text">多对多(Encoder-Decoder)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E4%B8%AD%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%87%BA%E7%8E%B0%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%EF%BC%9F%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="nav-number">19.6.</span> <span class="nav-text">RNN中为什么会出现梯度消失？如何解决？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">19.7.</span> <span class="nav-text">RNN的注意力机制</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LSTM"><span class="nav-number">20.</span> <span class="nav-text">LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9FRNN%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">20.1.</span> <span class="nav-text">传统RNN存在的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM-1"><span class="nav-number">20.2.</span> <span class="nav-text">LSTM</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer"><span class="nav-number">21.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">21.1.</span> <span class="nav-text">注意力机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-Encoder"><span class="nav-number">21.2.</span> <span class="nav-text">编码器(Encoder)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8-Decoder"><span class="nav-number">21.3.</span> <span class="nav-text">解码器(Decoder)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BE%93%E5%87%BA"><span class="nav-number">21.4.</span> <span class="nav-text">输出</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-Positional-Encoding"><span class="nav-number">21.5.</span> <span class="nav-text">位置编码(Positional Encoding)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanilla-Transformer%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%84%E7%89%B9%E7%82%B9"><span class="nav-number">21.5.1.</span> <span class="nav-text">Vanilla Transformer的位置编码的特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E7%BC%96%E7%A0%81%E6%96%B9%E5%BC%8F"><span class="nav-number">21.5.2.</span> <span class="nav-text">其他编码方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanilla-Transformer%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%84%E7%BC%BA%E7%82%B9%E4%BB%A5%E5%8F%8A%E6%94%B9%E8%BF%9B"><span class="nav-number">21.5.3.</span> <span class="nav-text">Vanilla Transformer位置编码的缺点以及改进</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Vision-Transformer"><span class="nav-number">22.</span> <span class="nav-text">Vision Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%84%E6%88%90"><span class="nav-number">22.1.</span> <span class="nav-text">模型组成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Embedding%E5%B1%82%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3"><span class="nav-number">22.2.</span> <span class="nav-text">Embedding层结构详解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer-Encoder%E8%AF%A6%E8%A7%A3"><span class="nav-number">22.3.</span> <span class="nav-text">Transformer Encoder详解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MLP-Head%E8%AF%A6%E8%A7%A3"><span class="nav-number">22.4.</span> <span class="nav-text">MLP Head详解</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PyTorch%E5%92%8CTensorFlow%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">23.</span> <span class="nav-text">PyTorch和TensorFlow的区别</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%9B%BE%E4%B8%8E%E9%9D%99%E6%80%81%E5%9B%BE"><span class="nav-number">23.1.</span> <span class="nav-text">动态图与静态图</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2%E5%92%8C%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7"><span class="nav-number">23.2.</span> <span class="nav-text">部署和可扩展性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B5%84%E6%BA%90%E4%BC%98%E5%8C%96%E5%92%8C%E5%88%A9%E7%94%A8%E7%8E%87"><span class="nav-number">23.3.</span> <span class="nav-text">资源优化和利用率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E6%9C%AF%E7%A0%94%E7%A9%B6%E5%92%8C%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81"><span class="nav-number">23.4.</span> <span class="nav-text">学术研究和开源代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="nav-number">24.</span> <span class="nav-text">推荐系统</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">24.1.</span> <span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95"><span class="nav-number">24.2.</span> <span class="nav-text">协同过滤算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%89%A9%E5%93%81%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95"><span class="nav-number">24.2.1.</span> <span class="nav-text">基于物品的协同过滤算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span class="nav-number">24.2.1.1.</span> <span class="nav-text">算法步骤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="nav-number">24.2.1.2.</span> <span class="nav-text">相似度计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C%E9%A2%84%E6%B5%8B"><span class="nav-number">24.2.1.3.</span> <span class="nav-text">结果预测</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95"><span class="nav-number">24.2.2.</span> <span class="nav-text">基于用户的协同过滤算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4-1"><span class="nav-number">24.2.2.1.</span> <span class="nav-text">算法步骤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97-1"><span class="nav-number">24.2.2.2.</span> <span class="nav-text">相似度计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C%E9%A2%84%E6%B5%8B-1"><span class="nav-number">24.2.2.3.</span> <span class="nav-text">结果预测</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95"><span class="nav-number">24.2.3.</span> <span class="nav-text">基于模型的协同过滤算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3"><span class="nav-number">24.3.</span> <span class="nav-text">矩阵分解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="nav-number">24.3.1.</span> <span class="nav-text">算法原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3%E6%96%B9%E6%B3%95"><span class="nav-number">24.3.2.</span> <span class="nav-text">求解方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%A9%E7%94%A8SGD%E6%9D%A5%E6%B1%82%E8%A7%A3%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3"><span class="nav-number">24.3.2.1.</span> <span class="nav-text">利用SGD来求解矩阵分解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%A9%E7%94%A8ALS%E6%9D%A5%E6%B1%82%E8%A7%A3%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3"><span class="nav-number">24.3.2.2.</span> <span class="nav-text">利用ALS来求解矩阵分解</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E7%9A%84%E6%8B%93%E5%B1%95%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="nav-number">24.3.3.</span> <span class="nav-text">矩阵分解推荐算法的拓展与优化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FM%E6%A8%A1%E5%9E%8B"><span class="nav-number">24.4.</span> <span class="nav-text">FM模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81FM%E6%A8%A1%E5%9E%8B"><span class="nav-number">24.4.1.</span> <span class="nav-text">为什么需要FM模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FM%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">24.4.2.</span> <span class="nav-text">FM模型的应用场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FM%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B7%E4%BD%93%E5%BD%A2%E5%BC%8F"><span class="nav-number">24.4.3.</span> <span class="nav-text">FM模型的具体形式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FM%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="nav-number">24.4.4.</span> <span class="nav-text">FM模型的解决方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FM%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="nav-number">24.4.5.</span> <span class="nav-text">FM模型的训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FM%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%AB%98%E7%BB%B4%E6%89%A9%E5%B1%95"><span class="nav-number">24.4.6.</span> <span class="nav-text">FM模型的高维扩展</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FFM%E6%A8%A1%E5%9E%8B"><span class="nav-number">24.5.</span> <span class="nav-text">FFM模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FFM%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E7%BB%84%E5%90%88%E6%96%B9%E5%BC%8F"><span class="nav-number">24.5.1.</span> <span class="nav-text">FFM模型的特征组合方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FFM%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">24.5.2.</span> <span class="nav-text">FFM模型的应用场景</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Wide-amp-Deep%E6%A8%A1%E5%9E%8B"><span class="nav-number">24.6.</span> <span class="nav-text">Wide&amp;Deep模型</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">木霈玖</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/06/%E9%9D%A2%E7%BB%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="木霈玖">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="木霈玖的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          面经
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-03-06 22:04:43" itemprop="dateCreated datePublished" datetime="2023-03-06T22:04:43+08:00">2023-03-06</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2023-04-24 14:24:54" itemprop="dateModified" datetime="2023-04-24T14:24:54+08:00">2023-04-24</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">总结</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>45k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>41 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>记录一下暑期实习面试经验。</p>
<a id="more"></a>
<h1 id="训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些"><a href="#训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些" class="headerlink" title="训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些?"></a>训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些?</h1><p>在训练过程中，如果模型不收敛并不能说明该模型时无效的。</p>
<p>导致模型不收敛的原因包括：</p>
<ol>
<li>没有对数据做归一化处理。</li>
<li>没有使用正则化。</li>
<li><code>Batch Size</code>设的太大。</li>
<li>学习率设置的太大容易产生震荡，太小会导致不收敛。</li>
<li>没有做数据预处理。</li>
<li>没有检查过预处理结果和最终的训练测试结果。</li>
<li>网络存在坏梯度，比如当<code>ReLU</code>对负值的梯度为 $0$ ，反向传播时，梯度为 $0$ 表示不传播。</li>
<li>网络设定不合理，网络太浅或者太深。</li>
<li>最后一层的激活函数错误。</li>
<li>参数初始化错误。</li>
<li>隐藏层神经元数量错误。</li>
<li>数据集标签的设置有错误。</li>
</ol>
<h1 id="加快模型训练速度的方法"><a href="#加快模型训练速度的方法" class="headerlink" title="加快模型训练速度的方法"></a>加快模型训练速度的方法</h1><ul>
<li>合理的超参数设计</li>
<li>权值共享</li>
<li>升级相关软件包</li>
<li>多卡训练、数据并行</li>
<li>混合精度训练</li>
</ul>
<h1 id="提升模型泛化能力的方法"><a href="#提升模型泛化能力的方法" class="headerlink" title="提升模型泛化能力的方法"></a>提升模型泛化能力的方法</h1><ul>
<li>从数据角度上来说。可以通过数据增强、扩充训练集等方法提高泛化能力。</li>
<li>在训练策略上，可以增加每个<code>Batch Size</code>的大小，进而让模型每次迭代时见到更多数据，防止过拟合。</li>
<li>调整数据分布，做训练数据集的类别均衡。</li>
<li>调整网络结构。如果数据集较小，可以降低模型复杂度防止过拟合。如果数据集较大，可以尝试更加复杂的模型。</li>
<li>减少过拟合的方法也可以提升模型的泛化能力。</li>
</ul>
<h1 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h1><p>偏差：模型预测值的期望与真实值之间的差异，反应的是模型的拟合能力。<br>方差：反应的是训练集的变化所导致的学习性能的变化，即刻画了<strong>数据扰动</strong>所造成的影响，模型过拟合时会出现较大的方差。</p>
<h1 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>过拟合就是随着模型的训练，模型在训练集上的表现越来越好，但是在验证集上的表现却越来越差，也就是说对训练集的拟合程度过高，导致模型的泛化能力降低。<br>欠拟合就是模型在训练集上也无法达到满意的精度。</p>
<h2 id="过拟合的解决方案"><a href="#过拟合的解决方案" class="headerlink" title="过拟合的解决方案"></a>过拟合的解决方案</h2><p>过拟合通常是由于训练数据过少、模型复杂度过大等问题导致的，因此相应的解决方案也是从这两个角度考虑。</p>
<ul>
<li>使用更多的数据进行训练，并且数据集尽量均匀，做一些数据增强。</li>
<li>降低模型复杂度。并不是说模型越复杂越好，如果模型过于复杂，而训练集又较少，那么参数就很容易拟合到一个过于适配训练集的参数空间中，自然就会导致过拟合的出现。</li>
<li><code>Early Stopping</code>，简单来说就是减少迭代次数。随着训练次数的增加，模型对训练数据的拟合程度也会随之增高，所以也可以通过减少训练时间的方法避免过拟合，提高模型泛化能力。</li>
<li><code>L1</code>、<code>L2</code>正则化方法，限制模型权重。</li>
<li>在数据中增加一些噪声，从而通过影响损失函数的优化方向避免过拟合。</li>
<li><code>Dropout</code>，目的也是降低模型的复杂度。</li>
<li><code>ResNet</code>。是的，<code>ResNet</code>也可以解决过拟合问题，因为<code>ResNet</code>的跳线结构可以让部分参数权重归零，进而达到类似于<code>Dropout</code>的效果。</li>
</ul>
<h2 id="欠拟合的解决方案"><a href="#欠拟合的解决方案" class="headerlink" title="欠拟合的解决方案"></a>欠拟合的解决方案</h2><p>欠拟合通常是由于模型表征能力不足、数据量过大导致的，刚好和过拟合相反。</p>
<ul>
<li>使用更复杂的模型。</li>
<li>增加迭代次数。</li>
<li>减少数据中的噪声。</li>
</ul>
<h1 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h1><h2 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h2><p>梯度消失就是指在网络反向传播过程中由于链式求导法则不断的累积，如果每一层的梯度都小于 $1$ ，由于累乘效应，出现了某些参数的梯度非常小的现象。在使用这些梯度更新梯度的时候参数值基本没有发生变化，因此就出现了网络训练停滞、模型无法继续优化的问题。</p>
<p>梯度爆炸与之刚好相反，在网络反向传播过程中由于链式求导法则的累乘效应，在每一层梯度都大于 $1$ 的时候，就可能会出现某些参数的梯度非常大。在使用这些梯度更新参数的时候就会导致参数变化过大，就会出现损失函数震荡的现象。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol>
<li>预训练和<code>fine-tuning</code>就是将一些在公开训练集上训练好的模型参数加载到自己对应的模型中，这样损失函数通常就能稳定的优化。</li>
<li>梯度裁剪：梯度裁剪是一个针对梯度爆炸的解决方案，也就是说将梯度限制在某个阈值范围内，如果梯度超过的这个阈值，那么就将其设置为这个阈值。</li>
<li>正则化：正则化也是一种限制梯度爆炸的解决方案，同时也有限制过拟合的作用。</li>
<li>使用<code>ReLU</code>、<code>Leaky ReLU</code>、<code>ELU</code>等激活函数：梯度消失通常是因为损失函数选择 <code>Sigmoid</code> 导致的，而<code>ReLU</code>激活函数在正数部分梯度是恒等于 $1$ 的，由于 $1$ 不会累积加权的特性，自然就可以避免梯度消失或梯度爆炸现象。但是<code>ReLU</code>同样有缺点，作为分段函数，<code>ReLU</code>在负数部分恒为 $0$ ，导致一些神经元无法被激活。而<code>Leaky ReLU</code>、<code>ELU</code>就可以避免这个问题。</li>
<li><code>BN(Batch Normalization)</code>：<code>BN</code>可以加速网络收敛提升训练的稳定性，它把每一层神经网络的任意神经元输入值的分布规范为正态分布，如果采用<code>Sigmoid</code>激活函数，那么就可以使得激活函数的输入落在梯度较大的区域，因此就能一定程度解决梯度消失的问题。</li>
<li>使用类似<code>ResNet</code>的跳线结构：由于离输出近的层学习效果好，而由于链式求导法则的影响可能会导致梯度消失或者梯度爆炸，因此可以模仿<code>ResNet</code>在网络的中间增加跳线结构，这样对应层求导梯度时候由于跳线的连接可以增加一个让梯度无损传播的通路，从而避免梯度消失或者梯度爆炸。</li>
<li>采用<code>LSTM</code>等结构：在<code>NLP</code>领域中，<code>LSTM</code>有时也会被用于对抗梯度现象，这是由于其具有复杂的门结构来控制梯度更新。</li>
</ol>
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p><code>2012</code>年，<code>Hinton</code>在其论文中提出<code>Dropout</code>。当一个复杂的前馈神经网络被训练在小的数据集时，容易造成过拟合。为了防止过拟合，可以通过阻止特征检测器的共同作用来提高神经网络的性能。<br><code>Droupout</code>是一种针对深度学习广泛应用的<strong>正则化技术</strong>。在每次迭代时随机关闭一些神经单元，随着迭代的进行，由于其他神经元可能在任何时候都被关闭，因此神经元对其他特定神经元的激活变得不那么敏感。</p>
<p>经过上面屏蔽掉某些神经元，使其激活值为 $0$ 以后，我们还需要对向量 $y_{1},\cdots,y_{n}$ <strong>进行缩放</strong>，也就是乘以 $1/(1-p)$ ，此方法为<code>invert Dropout</code>；如果你在训练的时候，经过置 $0$ 后，没有对 $y_{1}\cdots y_{n}$ 进行<code>rescale</code>，那么在测试的时候，就需要对权重进行缩放，即对每个神经元的权重都乘以一个 $p$ ，这样在“总体上”使得测试数据和训练数据是大致一样的，此方法为<code>vanilla Dropout</code>。比如一个神经元的输出是 $x$ ，那么在训练的时候它有 $p$ 的概率参与训练， $(1-p)$ 的概率丢弃，那么它输出的期望是 $p\cdot x+ (1-p)\cdot 0=p\cdot x$。因此测试的时候把这个神经元 $d$ 的权重乘以 $p$ 可以得到同样的期望。</p>
<h2 id="为什么说Dropout可以解决过拟合？"><a href="#为什么说Dropout可以解决过拟合？" class="headerlink" title="为什么说Dropout可以解决过拟合？"></a>为什么说Dropout可以解决过拟合？</h2><ol>
<li>取平均的作用。<code>Dropout</code>掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个<code>Dropout</code>过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</li>
<li>减少神经元之间复杂的共适应关系：因为<code>Dropout</code>导致两个神经元不一定每次都在一个<code>Dropout</code>网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。</li>
<li><code>Dropout</code>类似于性别在生物进化中的角色：物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。</li>
</ol>
<h2 id="Dropout缺点"><a href="#Dropout缺点" class="headerlink" title="Dropout缺点"></a>Dropout缺点</h2><p>明确定义的损失函数每一次迭代都会下降，而<code>Dropout</code>每一次都会随机删除节点，也就是说每一次训练的网络都是不同的，损失函数不再被明确地定义，在某种程度上很难计算，我们失去了调试工具。</p>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="Smooth-L1-Loss"><a href="#Smooth-L1-Loss" class="headerlink" title="Smooth L1 Loss"></a>Smooth L1 Loss</h2><ol>
<li>当预测框与<code>ground truth</code>差别过大时，梯度值不至于过大；</li>
<li>当预测框与<code>ground truth</code>差别很小时，梯度值足够小。</li>
</ol>
<p>为啥要做这两方面的限制呢？</p>
<ol>
<li>差距大时，梯度过于大，可能会导致梯度爆炸；</li>
<li>差距很小时，梯度足够小，能够接近最优点，避免大幅横跳。</li>
</ol>
<p><code>L2</code>、<code>L1</code>、<code>Smooth L1</code>损失函数分别定义为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222347197.png" alt=""></p>
<p>损失函数对 $x$ 的导数分别为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222349037.png" alt=""></p>
<p><code>L2</code>对 $x$ 的导数，与 $x$ 成<strong>正比关系</strong>。也就是当 $x$ 增大时，对 $x$ 的导数也线性增大。这就导致在训练初期，预测值与 <code>groud truth</code> 差异过于大时，损失函数对预测值的梯度十分大，<strong>训练初期不稳定</strong>。</p>
<p><code>L1</code>对 $x$ 的导数为常数，始终为 $1$ 或 $-1$ 。这就导致训练后期，预测值与<code>ground truth</code>差异很小时，损失对预测值的导数的绝对值仍然为 $1$ ，而 <code>learning rate</code>如果不变，损失函数将在稳定值附近波动，难以继续收敛以达到更高精度。</p>
<p><code>Smooth L1</code> 在 $x$ 较小时，也就是 $[-1,1]$ 区间，对 $x$ 的梯度也会变小；而在 $x$ 很大时，对 $x$ 的梯度的绝对值达到上限 $1$ ，也不会太大以至于破坏网络参数。 完美地避开了<code>L1</code> 和 <code>L2</code>损失的缺陷。</p>
<h2 id="Cross-Entropy-Loss"><a href="#Cross-Entropy-Loss" class="headerlink" title="Cross Entropy Loss"></a>Cross Entropy Loss</h2><p>多分类任务。设标签为 $y$ ，网络预测结果为 $\sigma(x)=\frac{1}{1+e^{-x}}$ ，<code>CE</code>损失函数为：</p>
<h2 id="Binary-Cross-Entropy-Loss"><a href="#Binary-Cross-Entropy-Loss" class="headerlink" title="Binary Cross Entropy Loss"></a>Binary Cross Entropy Loss</h2><p>多标签分类。设标签为 $y$ ，网络预测结果为 $\sigma(x)=\frac{1}{1+e^{-x}}$ ，<code>BCE</code>损失函数为： $L=-\sum_{i=1}^{N}[y_{i}\ast \log(\sigma(x_{i}))+(1-y_{i})\ast \log(1-\sigma(x_{i}))]$ 。</p>
<h2 id="Focal-loss"><a href="#Focal-loss" class="headerlink" title="Focal loss"></a>Focal loss</h2><p><strong>Focal Loss的引入主要是为了解决one-stage目标检测中正负样本数量极不平衡问题</strong>。</p>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><p><code>Sigmoid</code>是最基础的激活函数，可以将任意数值转换为概率（缩放到 $0 \thicksim 1$ 之间），在分类等场景中有广泛的应用。<code>Sigmoid</code>函数的形式是 $\sigma(z)=\frac{1}{1+e^{-z}}$ 。其对应的函数图像如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101955821.png" alt=""></p>
<h2 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h2><p>激活函数<code>tanh</code>和<code>Sigmoid</code>类似，都是<code>S</code>形曲线，输出范围是$[-1, 1]$。<code>tanh</code>函数的形式为 $g(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$ 。其对应的函数图像如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102004442.png" alt=""></p>
<h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p><code>ReLU(Rectified Linear Unit)</code>，是一种人工神经网络中常用的激活函数。通常意义下，其指代数学中的斜坡函数，即<br> $f(x)=\max(0,x)$ 。其对应的函数图像如下所示：<br><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101950246.png" alt=""></p>
<h2 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h2><p>为了解决<code>dead ReLU</code>问题（<code>ReLU</code>在训练的时很“脆弱”。在 $x&lt;0$ 时，梯度为 $0$ ，这个神经元及之后的神经元梯度永远为 $0$ ，不再对任何数据有所响应，导致相应参数永远不会被更新）。<code>Leaky ReLU</code>用一个类似 $0.01$ 的小值来初始化神经元，从而使得<code>ReLU</code>在负数区域更偏向于激活而不是坏死，这里的斜率都是确定的。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102008358.png" alt=""></p>
<h2 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h2><p><code>ELU</code>的提出也解决了<code>ReLU</code>的问题。与<code>ReLU</code>相比，<code>ELU</code>有负值，这会使激活的平均值接近零，让模型学习得更快。当 $x&lt;0$ 时，<code>ELU</code>的函数形式为 $f(x)=\alpha(e^{x}-1)$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102010115.png" alt=""></p>
<h2 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h2><p>激活函数<code>GELU</code>的灵感来源于<code>ReLU</code>和<code>Dropout</code>，在激活中引入了<strong>随机正则</strong>的思想。<code>GELU</code>通过输入自身的概率分布情况，决定抛弃还是保留当前的神经元。<code>GELU</code>函数的形式是 $GELU(x)=0.5x(1+\tanh(\sqrt\frac{2}{\pi}(x+0.044715x^{3})))$ 。其对应的函数图像如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304102019772.png" alt=""></p>
<p>可以理解为，对于输入的值，根据它的情况乘上 $1$ 或 $0$ 。更「数学」一点的描述是，对于每一个输入 $x$ ，其服从于标准正态分布 $\mathcal{N}(0, 1)$ ，它会乘上一个伯努利分布 $Bernoulli(\Phi(x))$，其中 $\Phi(x) = P(X \leq x)$ 。随着 $x$ 的降低，它被归零的概率会升高。对于<code>ReLU</code>来说，这个界限就是 $0$ ，输入少于零就会被归零。这一类激活函数，不仅保留了概率性，同时也保留了对输入的依赖性。<code>GELU</code>在最近的<code>Transformer</code>模型中（包括<code>BERT</code>，<code>RoBertA</code>和<code>GPT2</code>等）得到了广泛的应用。</p>
<h2 id="ReLU比Sigmoid效果好在哪里？"><a href="#ReLU比Sigmoid效果好在哪里？" class="headerlink" title="ReLU比Sigmoid效果好在哪里？"></a>ReLU比Sigmoid效果好在哪里？</h2><p><code>ReLU</code>的输出要么是 $0$ , 要么是输入本身。虽然方程简单，但实际上效果更好。</p>
<ol>
<li><code>ReLU</code>函数计算简单，可以减少很多计算量。反向传播求误差梯度时，涉及除法，计算量相对较大，采用<code>ReLU</code>激活函数，可以节省很多计算量。</li>
<li>避免梯度消失问题。对于深层网络，<code>Sigmoid</code>函数反向传播时，很容易就会出现梯度消失问题（在<code>Sigmoid</code>接近饱和区时，变换太缓慢，导数趋于 $0$ ，这种情况会造成信息丢失），从而无法完成深层网络的训练。例如在<code>RNN</code>当中，随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于 $0$ ，这就是“梯度消失“现象。此时采用<code>ReLU</code>激活函数就避免了“梯度消失“的发生。</li>
<li>可以缓解过拟合问题的发生，<code>ReLU</code>会使一部分神经元的输出为 $0$ ，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。</li>
</ol>
<h1 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h1><p>记号： $\theta_{t}$ 表示第 $t$ 轮的参数， $\eta$ 表示学习率， $g_{t}$ 表示第 $t$ 轮的梯度即 $\triangledown\hat{\mathcal{L}(\theta_{t})}$ ， $m_{t}$ 表示第 $t$ 轮的一阶动量， $v_{t}$ 表示第 $t$ 轮的二阶动量， $\hat{m}_{t}$ 为偏差纠正后的一阶矩估计， $\hat{v}_{t}$ 为偏差纠正后的二阶矩估计。</p>
<h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p><code>SGD(Stochastic Gradient Descent)</code>，随机梯度下降。每次选择一个<code>mini-batch</code>，而不是全部样本，使用梯度下降来更新模型参数。它解决了随机小批量样本的问题，但仍然有自适应学习率、容易卡在梯度较小点等问题。</p>
<p>$m_{t}=g_{t}, \ v_{t}=1$</p>
<p>$\theta_{t+1}=\theta_{t}-\eta \frac{m_{t}}{\sqrt{v_{t}}}=\theta_{t}-\eta g_{t} $</p>
<p><strong>缺点：</strong>下降速度慢，而且可能会在沟壑的两边持续振荡，停留在一个局部最优点</p>
<h2 id="SGDM"><a href="#SGDM" class="headerlink" title="SGDM"></a>SGDM</h2><p><code>SGDM(SGD with momentum)</code>，在<code>SGD</code>基础上增加一阶动量。  参数更新时以上一个时刻的一阶动量为主。</p>
<p>$m_{t} = \beta m_{t-1} + (1-\beta)  g_{t}, \ v_{t} = 1$</p>
<p>$\theta_{t+1} = \theta_{t} - \eta \frac{m_{t}}{\sqrt{v_{t}}} = \theta_{t} - \eta (\beta m_{t-1} + (1-\beta) g_{t}) $</p>
<h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>在<code>SGD</code>基础上增加二阶动量，可以对模型中的每个参数分配自适应学习率。</p>
<p>$m_{t}=g_{t}$</p>
<p>$v_{t}=\sum_{\tau=1}^{t}g_{\tau}^{2}$</p>
<p>$\theta_{t+1}=\theta_{t}-\eta \frac{g_{t}}{\sqrt{v_{t}+\epsilon}}=\theta_{t}-\eta \frac{g_{t}}{\sqrt{\sum_{\tau=1}^{t}g_{\tau}^{2}+\epsilon}}$</p>
<p><strong>优点：Adagrad在稀疏数据场景下表现最好</strong>，因为对于频繁出现的参数，学习率衰减快；对于稀疏的参数，学习率衰减的更慢</p>
<p><strong>缺点：</strong>在实际很多情况下，<strong>二阶动量呈单调递增，累积从训练开始的梯度，学习率会很快减至0，导致参数不再更新</strong>，训练过程提前结束</p>
<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p><code>SGD(Root Mean Square Prop)</code>在<code>SGD</code>基础上增加二阶动量，由于<code>Adagrad</code>的学习率衰减太过激进，改变二阶动量的计算策略：<strong>不累计全部梯度，只关注过去某一窗口内的梯度</strong>。<strong>指数移动平均值</strong>大约是过去一段时间的平均值，反映<strong>局部的</strong>参数信息，用这个方法来计算二阶累积动量。</p>
<p>$m_{t}=g_{t}$</p>
<p>$v_{t}=\beta v_{t-1}+(1-\beta)g_{t}^{2}$</p>
<p>$\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{v_{t}}}g_{t}=\theta_{t}-\frac{\eta}{\sqrt{\beta v_{t-1}+(1-\beta)g_{t}^{2}}}g_{t} $</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p><code>Adam(Adaptive Moment Estimation)</code>，自适应矩估计。是2014年提出的一种万金油式的优化器，使用起来非常方便，梯度下降速度快，但是容易在最优值附近震荡。竞赛中性能会略逊于<code>SGD</code>，毕竟最简单的才是最有效的。但是超强的易用性使得<code>Adam</code>被广泛使用。是<code>SGDM</code>和<code>RMSProp</code>的结合。</p>
<p>$m_{t}=\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}$</p>
<p>$v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}$</p>
<p>$\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}}$</p>
<p>$\hat{v}_{t}=\frac{v_{t}}{1-\beta_{w}^{t}}$</p>
<p>$\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}\hat{m}_t$</p>
<p><strong>解释：</strong></p>
<p>第一项 $m_{t}$ 为t时刻，梯度在动量形式下的一阶矩估计。</p>
<p>第二项 $v_{t}$ 为梯度在动量形式下的二阶矩估计。</p>
<p>第三项 $\hat{m}_{t}$ 为偏差纠正后的一阶矩估计。</p>
<p>第四项 $\hat{v}_{t}$ 为偏差纠正后的二阶矩估计。</p>
<p>最后一项是更新公式，可以参考<code>RMSProp</code>以及之前的算法。</p>
<p><strong>为什么需要偏差纠正？</strong></p>
<p>拿梯度在动量形式下的二阶矩估计 $v_{t}$ 为例，各个 $v_{t}$ 的公式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071555276.png" alt=""></p>
<p>而我们实际上需要的是梯度的二阶矩估计，也就是 $E(g_{i}^{2})$ 。因此使用动量求出来的二阶矩估计是有偏的，需要纠正。我们对动量二阶矩估计 $v_{t}$ 求期望 $E(v_{t})$ ，可以通过等比数列公式得到 $E(v_{t})$ 与 $E(g_{i}^{2})$ 的关系：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071559636.png" alt=""></p>
<p>因此，要得到 $E(g_{i}^{2})$ ，就需要除掉前面的系数 $(1-\beta_{2}^{t})$ 是一个常数</p>
<h1 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h1><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p><code>Bagging(Bootstrap aggregating)</code>，引导聚集算法，又称装袋算法，是机器学习领域的一种团体学习算法。<code>Bagging</code>算法可与其他分类、回归算法结合，提高其准确率、稳定性的同时，通过降低结果的方差，避免过拟合的发生。</p>
<p><strong>随机采样（bootstrap sample）</strong>从 $n$ 个数据点中<strong>有放回地重复随机</strong>抽取一个样本（即同一个样本可被多次抽取），共抽取 $n$ 次。创建一个与原数据大小相同得数据集，但有些数据点会缺失（大约 $1/3$ ），有些会重复。</p>
<p><code>Bagging</code>对于弱学习器没有限制，这和<code>Adaboost</code>一样。但是最常用的一般也是<strong>决策树</strong>和<strong>神经网络</strong>。</p>
<p><code>Bagging</code>的集合策略也比较简单，对于分类问题，通常使用简单投票法，得到最多票数的类别或者类别之一为最终的模型输出。对于回归问题，通常使用简单平均法，对 $T$ 个弱学习器得到的回归结果进行算术平均得到最终的模型输出。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071107979.png" alt=""></p>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p>随机森林以决策树为基本单元，通过集成大量的决策树，就构成了随机森林。其构造过程如下：</p>
<ol>
<li>$T$ 中共有 $N$ 个样本，有放回的随机选择 $N$ 个样本（因为有放回，所以虽然是 $N$ 但是不可能遍历所有样本）。这选择好了的 $N$ 个样本用来训练一个决策树，作为决策树根节点处的样本。</li>
<li>当每个样本有 $M$ 个属性时，在决策树的每个节点需要分裂时，随机从这 $M$ 个属性中选取出 $m$ 个属性，满足条件 $m &lt;&lt; M$ 。然后从这 $m$ 个属性中采用某种策略来选择某个属性作为该节点的分裂属性。</li>
<li>决策树形成过程中每个节点都要按照上述步骤来分裂，一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。</li>
<li>重复建立大量的决策树，这样就构成了随机森林了。</li>
</ol>
<h4 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h4><h5 id="决策树构建的终止条件"><a href="#决策树构建的终止条件" class="headerlink" title="决策树构建的终止条件"></a>决策树构建的终止条件</h5><ol>
<li><p>样本进来属于同一个类别，直接输出结果是 $C$ 类返回。</p>
</li>
<li><p>没法划分了，特征向量中所有属性都用完了；或者 $D$ 样本中的属性 $A$ 都相同，然后将此节点标记为叶子节点，将样本 $D$ 中数目最多的类当做类别返回。</p>
</li>
<li>当对数据进行划分成多个分支，如果存在分支中没有数据（分支为空），将划分前类别数目多的当做类别返回。</li>
</ol>
<h5 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h5><p>各种准则虽然对决策树的尺寸有较大影响,但<strong>对泛化性能的影响很有限</strong>，<strong>剪枝方法和程度对决策树泛化性能的影响更为显著；剪枝是决策树防止过拟合的手段</strong>。</p>
<p><strong>预剪枝：</strong>在决策树构造时就进行剪枝。在决策树构造过程中，对节点进行评估，如果对其划分并不能再验证集中提高准确性，那么该节点就不要继续往下划分。这时就会把当前节点作为叶节点。</p>
<p><strong>后剪枝：</strong>在生成决策树之后再剪枝。通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉该节点，带来的验证集中准确性差别不大或有明显提升，则可以对它进行剪枝，用叶子节点来代填该节点。</p>
<p><strong>预剪枝vs后剪枝</strong></p>
<p>时间开销：</p>
<ul>
<li>预剪枝：训练时间开销降低，测试时间开销降低</li>
<li>后剪枝：训练时间开销增加，测试时间开销降低</li>
</ul>
<p>过/欠拟合风险:</p>
<ul>
<li>预剪枝：过拟合风险降低，欠拟合风险增加</li>
<li>后剪枝：过拟合风险降低，欠拟合风险基本不变</li>
</ul>
<p>泛化性能：后剪枝通常优于预剪枝</p>
<h5 id="决策树生成算法"><a href="#决策树生成算法" class="headerlink" title="决策树生成算法"></a>决策树生成算法</h5><p><strong>ID3</strong></p>
<p>使用信息熵增益作为特征选择的标准。</p>
<p>数据集 $D$ 的经验熵定义为： $Ent(D)=-\sum_{k=1}^{K}\frac{|C_{k}|}{|D|}\log_{2}\frac{|C_{k}|}{|D|}$ ，其中 $|C_{k}|$ 为第 $k$ 类样本的数目， $|D|$ 为数据集 $D$ 中样本的数目。</p>
<p>计算特征 $A$ 对数据集 $D$ 的经验条件熵： $Ent(D|A)=\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}Ent(D_{i})=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_{i}|}\log_{2}\frac{|D_{ik}|}{|D_{i}|}$  。意思就是对划分后的新的 $n$ 个小数据集的经验熵加权，权重为每一个小数据集中的样本数目占未划分前的大数据集的比例。</p>
<p>计算信息熵增益： $gain(D,A)=Ent(D)-Ent(D|A)$ ，选择信息熵增益最大的特征。</p>
<p><strong>C4.5</strong></p>
<p>与<code>ID3</code>算法的最大不同在于使用信息熵增益率代替信息熵增益。信息熵增益率的定义如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304211122841.png" alt=""></p>
<p>其中 $IV(A)=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}\log_{2}\frac{|D_{i}|}{|D|}$ 称为数据集 $D$ 关于 $A$ 的取值熵。</p>
<p>这种方法对可能取值少的属性有所偏好，因此<code>C4.5</code>算法也不是直接使用增益率最大的来划分属性，而是使用了一种“启发式”的方法，先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。</p>
<p><strong>CART</strong></p>
<p><code>CART</code>决策树使用“基尼指数”来选择划分属性，选取那个使划分后基尼指数<strong>最小</strong>的属性。数据集 $D$ 的纯度可用基尼值来度量。 $Gini(D)$ 越小，则数据集的纯度越高。 $Gini(D)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p_{k}^{2}$ ，则属性 $A$ 的基尼指数表达式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304211122760.png" alt=""></p>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p><code>Bagging</code>在随机森林的构建过程中，各棵树之间是相互独立的，在构建第 $m$ 棵树的时候，不会考虑前面的 $m-1$ 棵树。<code>Boosting</code>在构建第 $m$ 棵子树的时候，会考虑到前 $m-1$ 棵子树的结果。</p>
<p><strong>提升学习(Boosting)</strong>是一种机器学习技术，通过从训练数据构建模型，然后创建第二个模型来尝试纠正第一个模型中的错误来完成的。添加模型直到完美预测训练集或添加最大数量的模型。提升学习的每一步产生弱预测模型（如决策树），并加权累加到总模型中；如果每一步的弱预测模型的生成都是依据损失函数的梯度方式，就称为梯度提升<code>(Gradient Boosting)</code>。</p>
<h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071138731.png" alt=""></p>
<p><code>Adaptive Boosting(AdaBoost)</code>是第一个为二进制分类开发的真正成功的提升算法。这是理解<code>Boosting</code>的最佳起点，现代提升方法建立在<code>AdaBoost</code>之上。</p>
<p><code>AdaBoost</code>流程：</p>
<ol>
<li><p>训练数据集中的每个实例都被加权。初始权重设置为： $Weight(X_{i})=\frac{1}{N_{}}$ </p>
</li>
<li><p>使用加权之后的样本作为训练数据，以弱分类器（决策树桩）进行训练。</p>
</li>
<li><p>为训练后的模型计算当前分类器的分类误差。传统的计算方式如下：</p>
<p>分类误差： $error_t = \sum_{i=1}^{N}(W_{t,i}\times terror_{i})$ 。其中 $terror_{i}=I(G_{t}(X_{i})\neq y_{i})$ 。</p>
<p>例如：如果我们有三个训练实例，权重分别为 $0.01$ 、 $0.5$ 和 $0.2$ 。预测值为 $-1$ 、 $-1$ 和 $-1$ ，实例中的真实输出变量为 $-1$ 、 $1$ 和 $-1$ ，则  $terror$ 为 $0$ 、 $1$ 和 $0$ 。误分类率将计算为：<strong>error = (0.01*0 + 0.5*1 + 0.2*0) or error = 0.5</strong>。</p>
</li>
<li><p>为经过训练的模型计算阶段权值，该值为模型做出的任何预测提供权重。训练模型的阶段值计算公式： $\alpha_{t}=\frac{1}{2}\times\ln\frac{1-error_{t}}{error_{t}}$</p>
</li>
<li><p>更新训练权重，为错误预测的实例提供更多的权重，为正确预测的实例提供更少权重。计算公式： $W_{t+1,i}=\frac{W_{t,i}}{Z_{t}}\exp(-\alpha_{t}G_{t}(X_{i})y_{i})$ ， $Z_{t}$ 是规范因子， $Z_{t}=\sum_{i=1}^{N}W_{t,i}\exp(-\alpha_{t}G_{t}(X_{i})y_{i})$</p>
</li>
<li><p>最终的分类器为： $G(X)=sign(\sum_{m=1}^{K}\alpha_{m}G_{m}(X))$</p>
</li>
</ol>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p><code>GBDT(Gradient Boosting Decision Tree)</code>在数据分析和预测中的效果很好。它是一种基于决策树的集成算法。其中<code>Gradient Boosting</code>是集成方法<code>Boosting</code>中的一种算法，通过梯度下降来对新的学习器进行迭代。将表现一般的数个模型（通常是深度固定的决策树）组合在一起来集成一个表现较好的模型。抽象地说，模型的训练过程是对一任意可导目标函数的优化过程。通过反复地选择一个指向负梯度方向的函数，该算法可被看做在函数空间里对目标函数进行优化。因此可以说<code>Gradient Boosting = Gradient Descent + Boosting</code>。</p>
<p>模型的结果是一组回归分类树组合<code>(CART Tree Ensemble)</code>： $T_{1},\cdots,T_{K}$ 。其中 $T_{j}$ 学习的是之前 $j-1$ 棵树预测结果的残差，这种思想就像准备考试前的复习，先做一遍习题册，然后把做错的题目挑出来，再做一次，然后把做错的题目挑出来再做一次，经过反复多轮训练，取得最好的成绩。而模型最后的输出，是一个样本在各个树中输出的结果的和： $\bar{y}=\sum_{k=1}^{K}f_{k}(x)$ 。</p>
<p>和<code>AdaBoost</code>一样，<code>Gradient Boosting</code>也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，<code>AdaBoost</code>是通过提升错分数据点的权重来定位模型的不足而<code>Gradient Boosting</code>是通过算梯度<code>(gradient)</code>来定位模型的不足。因此相比<code>AdaBoost</code>，<code>Gradient Boosting</code>可以使用更多种类的目标函数。</p>
<h4 id="提升树算法"><a href="#提升树算法" class="headerlink" title="提升树算法"></a>提升树算法</h4><p>提升树是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差=真实值-预测值。提升树即是整个迭代过程生成的回归树的累加。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101613543.jpg" alt=""></p>
<p>具体的算法步骤：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101613198.png" alt=""></p>
<h4 id="Gradient-Boosting-Decision-Tree（梯度提升决策树）"><a href="#Gradient-Boosting-Decision-Tree（梯度提升决策树）" class="headerlink" title="Gradient Boosting Decision Tree（梯度提升决策树）"></a>Gradient Boosting Decision Tree（梯度提升决策树）</h4><p>提升树利用加法模型和前向分步算法实现学习的优化过程。当损失函数时平方损失和指数损失函数时，每一步的优化很简单，如平方损失函数学习残差回归树。</p>
<p>但对于一般的损失函数，往往每一步优化没那么容易，如上图中的绝对值损失函数和<code>Huber</code>损失函数。针对这一问题，<code>Freidman</code>提出了梯度提升算法：利用最速下降的近似方法，即利用损失函数的负梯度在当前模型的值，作为回归问题中提升树算法的残差的近似值，拟合一个回归树。</p>
<h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p><strong>原始目标函数</strong></p>
<p>目标函数，可以分为两个部分，一部分是损失函数，一部分是正则（用于控制模型的复杂度）。</p>
<p>对于第 $t$ 颗树，第 $i$ 个样本的，模型的预测值是这样的：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101650959.png" alt=""></p>
<p>进一步，我们可以得到我们的原始目标函数，如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101651061.png" alt=""></p>
<p><strong>损失函数化简</strong></p>
<p><code>XGBoost</code>是前向迭代，我们的重点在于第 $t$ 个树，所以涉及到前 $t-1$ 个树变量或者说参数我们是可以<strong>看做常数</strong>的。所以我们的损失函数进一步可以化为如下，其中一个变化是我们对正则项进行了拆分，变成可前 $t-1$ 项和第 $t$ 项：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101656177.png" alt=""></p>
<h4 id="泰勒公式展开"><a href="#泰勒公式展开" class="headerlink" title="泰勒公式展开"></a>泰勒公式展开</h4><p>使用泰勒公式进行近似展开的核心目标是对目标函数进行化简，将常数项抽离出来。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101658315.png" alt=""></p>
<p>这里 $\Delta x$ 对应的是第 $t$ 棵树的模型 $f_{t}(x_{i})$ ， $x$ 对应的是 $\hat{y}_{i}^{(t-1)}$ ，相应的 $f(x)$ 对应到损失函数应该是 $l(y_{i},\hat{y}_{i}^{(t-1)})+f_{t}(x_{i})$ 。</p>
<p>所以原有公式进行泰勒公式二阶展开，结果为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101704500.png" alt=""></p>
<p>进而我们可以得到目标函数展开公式为如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101704670.png" alt=""></p>
<h4 id="树的参数化"><a href="#树的参数化" class="headerlink" title="树的参数化"></a>树的参数化</h4><p><strong>树模型参数化</strong></p>
<ul>
<li>每棵树每个叶子节点的值（或者说每个叶子节点的权重） $w$ ：这是一个向量，因为每个树有很多叶子节点</li>
<li>样本到叶子节节点的映射关系 $q$ ：告诉每个样本落在当前这个树的哪一个叶子节点上</li>
<li>叶子节点样本归属集合 $I$ ：告诉每个叶子节点包含哪些样本</li>
</ul>
<p><strong>树复杂度参数化</strong></p>
<p>树的复杂度定义如下，其中 $T$ 参数表示当前这棵树叶子节点的个数； $w_{j}^{2}$ 是叶子节点值的 $L_{2}$ 范数：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101716405.png" alt=""></p>
<p>进而我们可以对树进行了参数化，带入到目标函数我们可以得到如下式子：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101727004.png" alt=""></p>
<p>最后一步的转化思路是从在这个树中，每个样本落在哪个节点转为了每个节点上有哪些样本。</p>
<p>叶子节点 $j$ 所包含的样本的一阶导数累加之和为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101743580.png" alt=""></p>
<p>叶子节点 $j$ 所包含的样本的二阶导数累加之和为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101744292.png" alt=""></p>
<p>进而我们可以进一步化简为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101744829.png" alt=""></p>
<p>对目标函数对 $w_{j}$ 进行求导就能得出极值点：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303101746573.png" alt=""></p>
<h4 id="特征分裂"><a href="#特征分裂" class="headerlink" title="特征分裂"></a>特征分裂</h4><p>对于上述的目标函数，我们仍存在问题，即 $T$ 的取值，也就是如何做特征分裂。</p>
<p><strong>贪心算法</strong></p>
<p>本质上是做两次循环，第一个循环是针对每个特征的每个分割点做一次循环，计算收益，从而选择此特征的最佳分割点。分裂收益使用的是分裂之后的目标函数的变化差值。第二个循环是对样本所有特征的循环，从中挑选出收益最大的特征。</p>
<p>简单说就是首先找到基于每个特征找到收益最大的分割点，然后基于所有特征找到收益最大的特征。</p>
<p><strong>近似算法-分位数候选点</strong></p>
<p>对于每个特征，不去暴力搜索每个值，而是使用分位点</p>
<ul>
<li>根据样本数量选择三分位点或者四分位点等</li>
<li>或者根据二阶导数（也就是梯度）作为权重进行划分</li>
</ul>
<p>也就是说原来是某个特征的所有取值作为候选点，现在是某个特征的分位点作为候选点。</p>
<h3 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h3><p><code>LightGBM(Light Gradient Boosting Machine)</code>是一种梯度提升框架，它使用决策树作为基学习器。<code>LightGBM</code>为高效并行计算而生，它的<code>Light</code>体现在以下几个点上：</p>
<ul>
<li>更快的训练速度</li>
<li>更低的内存使用</li>
<li>支持单机多线程，多机并行计算，以及<code>GPU</code>训练</li>
<li>能够处理大规模数据</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102112494.png" alt=""></p>
<p>概括来说，<code>lightGBM</code>主要有以下特点：</p>
<ul>
<li>基于<code>Histogram</code>的决策树算法</li>
<li>带深度限制的<code>Leaf-wise</code>的叶子生长策略</li>
<li>直方图做差加速</li>
<li>直接支持类别特征<code>(Categorical Feature)</code></li>
<li><code>Cache</code>命中率优化</li>
<li>基于直方图的稀疏特征优化</li>
<li>多线程优化</li>
</ul>
<h4 id="直方图Histogram算法"><a href="#直方图Histogram算法" class="headerlink" title="直方图Histogram算法"></a>直方图Histogram算法</h4><p><strong>寻找最佳分类点</strong></p>
<p>将连续型特征值放入离散化的箱子<code>(bin)</code>中，然后用这些箱子构建特征直方图。然后模型基于特征直方图寻找最佳分裂点，构建直方图的时间复杂度是 $O(data \times feature)$ ，但寻找最佳分裂点的时间复杂度为 $O(bin \times feature)$ 。模型训练速度会因此而提高，而且因为不需要存储排序索引，内存压力也变小了。<code>LGBM</code>采用的就是直方图算法（现在<code>XGBoost</code>开源代码也支持直方图算法）。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102149708.png" alt=""></p>
<p><strong>直方图差加速</strong></p>
<p>直方图做差是指：“一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的 $k$ 个桶。利用这个方法，<code>LightGBM</code>可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。” 示意图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102152516.png" alt=""></p>
<h4 id="带深度限制的Leaf-wise的叶子生长策略"><a href="#带深度限制的Leaf-wise的叶子生长策略" class="headerlink" title="带深度限制的Leaf-wise的叶子生长策略"></a>带深度限制的Leaf-wise的叶子生长策略</h4><p><code>GBDT</code>与<code>XGBoost</code>模型在叶子生长策略上均采用按层<code>level-wise</code>分裂的方式，这种方式在分裂时会针对同一层的每一个节点，即每次迭代都要遍历整个数据集中的全部数据，这种方式虽然可以使每一层的叶子节点并行完成，并控制模型的复杂度，但也会产生许多不必要搜索或分裂，从而消耗更多的运行内存，增加计算成本。</p>
<p>而<code>LightGBM</code>算法对其进行了改进，使用了按叶子节点<code>leaf-wise</code>分裂的生长方式，即每次是对所有叶子中<strong>分裂增益最大的叶子节点进行分裂</strong>，其他叶子节点则不会分裂。这种分裂方式比按层分裂会带来更小的误差，并且加快算法的学习速度，但由于没有对其他叶子进行分裂，会使得分裂结果不够细化，并且在每层中只对一个叶子不断进行分裂将增大树的深度，造成模型过拟合。因此，<code>LightGBM</code>算法在按叶子节点生长过程中会限制树的深度来避免过拟合。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102156269.png" alt=""></p>
<h4 id="支持类别特征"><a href="#支持类别特征" class="headerlink" title="支持类别特征"></a>支持类别特征</h4><p>实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化<code>one-hotting</code>特征，降低了空间和时间的效率。而类别特征的使用是在实践中很常用的。基于这个考虑，<code>LightGBM</code>优化了对类别特征的支持，可以直接输入类别特征，不需要额外的 $0/1$ 展开。并在决策树算法上增加了类别特征的决策规则。决策树在学习节点分裂时，是一种<code>one-vs-rest</code>模式，每次只能根据一个类别做分类，如下图。这种模式效率比较低，而且不利于决策树学习。<code>LightGBM</code>对此进行了优化，采用<code>many-vs-many</code>模式分裂节点，如下图。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102208969.png" alt=""></p>
<h4 id="基于梯度的单边采样-GOSS"><a href="#基于梯度的单边采样-GOSS" class="headerlink" title="基于梯度的单边采样(GOSS)"></a>基于梯度的单边采样(GOSS)</h4><p>在<code>AdaBoost</code>中，样本权重指示了数据样本的重要性，而在<code>GBDT</code>上并没有样本权重这一说，可作者发现：在<code>GBDT</code>中，梯度对于每个样本是个很有用的信息，它可以用来帮助采样。为什么这么说呢？让我们打个比方，如果某样本得到的一个小的梯度值，那么说明该样本的训练误差也小，模型在该样本上表现得就很好，那这些小梯度的样本其实是不是不用参与训练了？就好像准备考试时刷题不刷简单题，这样可以吗？不可以！因为如果真的直接剔除它们，数据分布会改变，从而损害模型的准确率。为了处理这个问题，作者提出了<code>GOSS</code>，<code>GOSS</code>全称是<code>Gradient-based One-Side Sampling</code>单边梯度采样，它保留所有大梯度的样本，然后小梯度样本采用随机采样，在不改变原始数据分布的同时，减小了样本数量，提升了模型的训练速度。</p>
<h4 id="互斥特征绑定"><a href="#互斥特征绑定" class="headerlink" title="互斥特征绑定"></a>互斥特征绑定</h4><p>从特征角度来看，稀疏特征会包含很多 $0$ 元素；从样本角度来看，一个样本的多个稀疏特征经常同时为 $0$ 。<code>EFB(Exclusive Feature Bundling)</code>基于这种想法，对<strong>互斥特征</strong>进行了<strong>捆绑</strong>，整体过程有点类似于<code>One-Hot</code>逆过程。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303102216271.png" alt=""></p>
<p><strong>捆哪些特征</strong></p>
<p>尝试每种组合，是<code>NP</code>难问题，现有算力做不到。所以只能采用贪心算法找。具体过程如下：</p>
<ol>
<li>遍历特征，先把第一个特征拿出来作为一个组合</li>
<li>第二个特征往这个组合里放，冲突比例小就放进去合并成一个特征，冲突比例大就单拿出来作为另一个组合</li>
<li>第三个特征继续往已有的组合里放，能放就放，不能放就单成一个新组合</li>
<li>以此类推对所有特征做同样的操作。</li>
</ol>
<p><strong>如何捆绑特征</strong></p>
<p>因为不同特征下的值有不同的量纲，比如：特征<code>A</code>的值范围为 $[0, 10)$ ，特征<code>B</code>的值范围为 $[0, 20)$ ，将特征<code>A</code>和特征<code>B</code>的直方图加起来捆绑一起后，<code>bundle</code>的值范围变为 $[0, 20)$ ，但是我们无法从中辨别哪些是特征<code>A</code>，哪些是特征<code>B</code>。这样对模型是不利的，因为模型这样就没法根据<code>bundle</code>直方图的值范围去很好区分特征，对树生成会带来误差。对于该问题的解决办法就是加偏移量，如果我们对特征<code>B</code>加偏移量 $10$ ，特征<code>B</code>的值范围变为 $[10, 30)$ ，合并后的<code>bundle</code>值范围变为 $[0, 30)$ 。</p>
<h3 id="CatBoost"><a href="#CatBoost" class="headerlink" title="CatBoost"></a>CatBoost</h3><p><code>CatBoost</code>主要是在类别特征上的处理上做了很多的改进。从用户使用角度来看，相比<code>XGBoost</code>和<code>LightGBM</code>，<code>CatBoost</code>具有如下特点。</p>
<ul>
<li><strong>模型精度：</strong><code>XGBoost</code>和<code>LightGBM</code>相当，<code>CatBoost</code>往往略好一些，无需调参即可获取很好的结果。</li>
<li><strong>训练速度：</strong><code>LightGBM</code>远快于<code>XGBoost</code>，<code>CatBoost</code>快于<code>XGBoost</code>但比<code>LightGBM</code>慢。</li>
<li><strong>预测速度：</strong><code>LightGBM</code>与<code>XGBoost</code>相当，<code>CatBoost</code>远快于<code>LightGBM</code>与<code>XGBoost</code>，是它们的几十分之一。</li>
<li><strong>内存消耗：</strong><code>LightGBM</code>远小于<code>XGBoost</code>，<code>CatBoost</code>小于<code>XGBoost</code>，但大于<code>LightGBM</code>。</li>
<li><strong>类别特征：</strong><code>XGBoost</code>不支持类别特征，需要<code>One-Hot</code>编码预处理。<code>LightGBM</code>支持类别特征，需转换成整数编码。<code>CatBoost</code>提供更强大的对类别特征的支持，直接支持字符串类型的类别特征，无需预处理。</li>
<li><strong>缺失值特征：</strong><code>XGBoost</code>和<code>LightGBM</code>都可以自动处理特征缺失值，<code>CatBoost</code>不能自动处理缺失值（或者将缺失值视为最小值/最大值）。</li>
<li><strong>GPU支持：</strong><code>LightGBM</code>与<code>CatBoost</code>支持<code>GPU</code>训练，<code>XGBoost</code>也支持<code>GPU</code>训练。</li>
<li><strong>可视化：</strong><code>CatBoost</code>还自带一套可视化工具，可以在<code>Jupyter Notebook</code>或者<code>TensorBoard</code>中实时看到指标变化。</li>
</ul>
<h4 id="基于类别特征的Ordered-Target-Statistics数值编码方法"><a href="#基于类别特征的Ordered-Target-Statistics数值编码方法" class="headerlink" title="基于类别特征的Ordered Target Statistics数值编码方法"></a>基于类别特征的Ordered Target Statistics数值编码方法</h4><p>对于类别特征，如果类别数目不多，可以使用<code>One-Hot</code>编码。但如果类别数量成百上千，使用<code>One-Hot</code>编码会导致特征数量爆炸。<strong>CatBoost设计了一种基于预测目标统计值的方法可以将类别特征转化为数值特征。</strong>先将样本随机打乱，然后每个样本只使用它排序在它前面的样本来计算其类别特征的数值编码。这样就防止了<code>label</code>的泄露，并且能够较为合理地评估这个特征的真实有效性。具体公式表达为： $i \rightarrow \frac{Current\ Count+a\star P}{Max\ Count + a}$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111538347.png" alt=""></p>
<p>对上述例子来说，我们要计算第 $i$ 条数据的<code>label</code>，计算结果就为 $\frac{2+a \star P}{3+a}$ ，如果将第三行的<code>label</code>改为 $1$ ，那么结果就变成了 $\frac{3+a\star P}{3+a}$ 。</p>
<h4 id="基于贪心策略的特征交叉方法"><a href="#基于贪心策略的特征交叉方法" class="headerlink" title="基于贪心策略的特征交叉方法"></a>基于贪心策略的特征交叉方法</h4><p>使用<code>Ordered Target Statistics</code>方法将类别特征转化成为数值特征以后，会影响到特征交叉，因为数值特征无法有效地进行交叉。为了有效地利用特征交叉，<code>CatBoost</code>在将类别特征转换为数值编码的同时，会自动生成交叉特征。但如果让全部的类别特征之间都进行交叉，两两交叉，三三交叉，四四交叉，这个复杂度是指数级的，特征维度一定会爆炸。<code>CatBoost</code>使用一种贪心的策略来进行特征交叉。生成<code>tree</code>的第一次分裂，<code>CatBoost</code>不使用任何交叉特征。在后面的分裂中，<code>CatBoost</code>会使用生成<code>tree</code>所用到的全部原始特征和交叉特征跟数据集中的全部类别特征进行交叉。</p>
<h4 id="避免预测偏移的Ordered-Boosting方法"><a href="#避免预测偏移的Ordered-Boosting方法" class="headerlink" title="避免预测偏移的Ordered Boosting方法"></a>避免预测偏移的Ordered Boosting方法</h4><p>使用<code>XGBoost</code>或者<code>LightGBM</code>做模型时，我们可能经常会发现模型在训练集上拟合的很好，<code>train_auc</code>甚至达到了 $1.0$ ，但是在验证集上却差了很多，<code>val_auc</code>可能只有 $0.7$ 。这当然有可能是因为<code>tree</code>的数量太多了，或者是每棵<code>tree</code>的<code>leaves</code>太多了，总之模型太复杂了造成了过拟合。</p>
<p>但也有一些<code>XGBoost</code>和<code>LightGBM</code>自身算法的缺陷因素。我们知道<code>LightGBM</code>在训练下一棵<code>tree</code>的时候，需要计算前面这些<code>tree</code>构成的加法模型在所有样本上的一阶梯度和二阶梯度（<code>Loss</code>对模型预测结果的导数），然后用这些梯度来决定下一棵树的结构和叶子节点取值。</p>
<p>但是我们计算的这些一阶梯度和二阶梯度值是有问题的。前面的这些<code>tree</code>都是在这些样本上训练的，现在我们又在这些样本上估计模型预测结果的一阶和二阶梯度。我们应该换一些新的样本才更合理。但是我们从哪里找这些新的样本呢？</p>
<p><code>CatBoost</code>的作者故伎重演。先将样本随机打乱，然后每个样本只使用<strong>排序在它前面的样本</strong>来训练模型。用这样的模型来估计这个样本预测结果的一阶和二阶梯度。然后用这些梯度构建一棵<code>tree</code>的结构，最终<code>tree</code>的每个叶子节点的取值，是使用全体样本进行计算的。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111601603.png" alt=""></p>
<h4 id="使用对称二叉树作为基模型"><a href="#使用对称二叉树作为基模型" class="headerlink" title="使用对称二叉树作为基模型"></a>使用对称二叉树作为基模型</h4><p><code>XGBoost</code>和<code>LightGBM</code>采用的基模型是普通的二叉树，但是<code>CatBoost</code>采用的是对称的二叉树。这种对树结构上的约束有一定的<strong>正则作用</strong>。更为重要的是，它可以让<code>CatBoost</code>模型的推断过程极快。对于<code>CatBoost</code>的<code>tree</code>的预测过程来说，每个特征的分裂都是独立的，不分先后顺序，多个样本可以一起预测。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303111619137.jpg" alt=""></p>
<h2 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h2><p>待总结</p>
<h1 id="解决样本不均衡的方法"><a href="#解决样本不均衡的方法" class="headerlink" title="解决样本不均衡的方法"></a>解决样本不均衡的方法</h1><ul>
<li>以多种数据组合形式训练模型并做模型融合。顾名思义，这种方法就是将全部的小样本数据和等量的大样本数据分别组合成几批训练数据集，并以此训练出几个不同的模型并做模型融合，这种方法能够有效的解决样本不均衡问题。</li>
<li>在计算损失函数时改变数据的权重，增加小样本数据的权重，减少大样本的权重。这种方法实际上参考了<code>focal loss</code>的思想，只不过解决的不是难易样本不均衡，而是样本数据量不均衡，同样能保证模型的泛化性能。</li>
<li>过采样小样本，欠采样大样本。也就是对于训练数据，把那些样本量较小的数据多在<code>load</code>数据时重复几遍，对于那些样本量较大的数据，则尽量减少<code>load</code>它们，以此来达到样本均衡的目的。</li>
</ul>
<h1 id="LableSmoothing"><a href="#LableSmoothing" class="headerlink" title="LableSmoothing"></a>LableSmoothing</h1><p>传统做图像分类采用的损失是交叉熵损失，具体形式是： $\mathcal{L}=-\sum_{i=1}^{m}t_{i}\log(y_{i})$ 。其中， $m$ 表示类别数， $y_{i}$ 表示<code>softmax</code>之后每个类的预测概率， $t_{i}$ 表示样本的真实标签值。然而，神经网络 有一个坏习惯，就是在训练过程中对预测变得”过于自信“，这可能会降低它们的泛化能力，从而在新的、看不见的未来数据上表现得同样出色。此外，大型数据集通常会包含标签错误的数据，这意味着神经网络在本质上应该对“正确答案”持怀疑态度，以减少一定程度上围绕错误答案的极端情况下的建模。</p>
<p>因此，标签平滑所做的就是通过训练网络向<code>1-adjustment</code>目标移动，然后在其余的类上除以这个<code>adjustment</code>，从而使它对自己的答案不那么自信，而不是简单的设为 $1$ 。新标签的表现形式为：$T^{\prime}=(1-\varepsilon)*T+\frac{\varepsilon}{N}$。其中， $N$ 表示类别的个数， $T$ 表示真实标签值， $T^{\prime}$ 表示平滑后的标签。</p>
<p>例如，原来的标签 $T$ 为 $[0,0,1,0,0]$ ， $\varepsilon=0.1$ ，经过<code>LabelSmoothing</code>之后的标签 $T^{\prime}$ 为 $[0.02,0.02,0.92,0.02,0.02]$ 。</p>
<h1 id="图像处理中平滑和锐化操作是什么？"><a href="#图像处理中平滑和锐化操作是什么？" class="headerlink" title="图像处理中平滑和锐化操作是什么？"></a>图像处理中平滑和锐化操作是什么？</h1><h2 id="概念-2"><a href="#概念-2" class="headerlink" title="概念"></a>概念</h2><p>锐化就是通过增强图像的高频信息，也就是纹理边缘来减少图像中的模糊细节，但是在增强纹理的时候也引入了图像噪声。</p>
<p>平滑处理<code>(smoothing)</code>也称模糊处理<code>(bluring)</code>，主要用于消除图像中的噪声部分，平滑处理常用的用途是用来减少图像上的噪点或失真，平滑主要使用图像滤波。在这里，我个人认为可以把图像平滑和图像滤波联系起来，因为图像平滑常用的方法就是图像滤波器。</p>
<h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><ul>
<li>在计算机视觉的一些任务中，涉及到图像重建的、如高精度的深度估计、医学图像分割、三维重建等任务，最终需要得到原始图像分辨率大小的输出，同时对图像的边缘清晰度也有较高的要求，这时候可以通过增强特征图中的高频分量，在计算损失函数的时候放大这些区域的损失，进而放大对应参数的梯度，使得网络往更突出边缘的方向上优化。</li>
<li>与上相反，如果是一些希望输出更加平滑的任务，则可以考虑对特征图进行平滑操作，进而减小高频区域的损失，减小对应参数的梯度，使得网络往更平滑的方向上优化，通常这种技术都会用在<code>smooth loss</code>中。</li>
</ul>
<h1 id="batchsize"><a href="#batchsize" class="headerlink" title="batchsize"></a>batchsize</h1><p>（1）<code>batchsize</code>：批大小。在深度学习中，一般采用<code>SGD</code>训练，即每次训练在训练集中取<code>batchsize</code>个样本训练。<br>（2）<code>iteration</code>：1个<code>iteration</code>等于使用<code>batchsize</code>个样本训练一次。<br>（3）<code>epoch</code>：1个<code>epoch</code>等于使用训练集中的全部样本训练一次。<br>如果数据集比较小，则完全可以采用全数据集的形式。这样做的好处有两点：</p>
<ol>
<li>全数据集的方向能够更好的代表样本总体，确定其极值所在。</li>
<li>由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。</li>
</ol>
<p>增大<code>batchsize</code>的好处有三点：</p>
<ol>
<li>内存的利用率提高了，大矩阵乘法的并行化效率提高。</li>
<li>跑完一次<code>epoch</code>（全数据集）所需迭代次数减少，对于相同的数据量的处理速度进一步加快。</li>
<li>一定范围内，<code>batchsize</code>越大，其确定的下降方向就越准，引起训练震荡越小。</li>
</ol>
<p>盲目增大<code>batchsize</code>的坏处有三点：</p>
<ol>
<li>当数据集太大时，内存撑不住。</li>
<li>跑完一次<code>epoch</code>（全数据集）所需迭代次数减少了，但要想达到相同的精度，时间开销太大，参数的修正更加缓慢。</li>
<li><code>batchsize</code>增大到一定的程度，其确定的下降方向已经基本不再变化。</li>
</ol>
<h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><p>支持向量机<code>(support vector machines，SVM)</code>是一种二分类模型，它将实例的特征向量映射为空间中的一些点，<code>SVM</code>的目的就是想要画出一条线，以 “最好地” 区分这两类点，以至如果以后有了新的点，这条线也能做出很好的分类。<code>SVM</code>适合中小型数据样本、非线性、高维的分类问题。</p>
<p>将实例的特征向量（以二维为例）映射为空间中的一些点，如下图的实心点和空心点，它们属于不同的两类。<code>SVM</code>的目的就是想要画出一条线，以“最好地”区分这两类点，以至如果以后有了新的点，这条线也能做出很好的分类。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071255957.png" alt=""></p>
<p><strong>Q1：能够画出多少条线对样本点进行区分？</strong><br>答：线是有无数条可以画的，区别就在于效果好不好，每条线都可以叫做一个划分超平面。比如上面的绿线就不好，蓝线还凑合，红线看起来就比较好。我们所希望找到的这条效果最好的线就是具有 “最大间隔的划分超平面”。</p>
<p><strong>Q2：为什么要叫作“超平面”呢？</strong><br>答：因为样本的特征很可能是高维的，此时样本空间的划分就不是一条线了。</p>
<p><strong>Q3：画线的标准是什么？/ 什么才叫这条线的效果好？/ 哪里好？</strong><br>答：<code>SVM</code>将会寻找可以区分两个类别并且能使间隔<code>(margin)</code>最大的划分超平面。比较好的划分超平面，样本局部扰动时对它的影响最小、产生的分类结果最鲁棒、对未见示例的泛化能力最强。</p>
<p><strong>Q4：间隔margin是什么？</strong><br>答：对于任意一个超平面，其两侧数据点都距离它有一个最小距离（垂直距离），这两个最小距离的和就是间隔。比如下图中两条虚线构成的带状区域就是<code>margin</code>，虚线是由距离中央实线最近的两个点所确定出来的（也就是由支持向量决定）。但此<code>margin</code>比较小，如果用第二种方式画，<code>margin</code>明显变大也更接近我们的目标。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071300267.png" alt=""></p>
<p><strong>Q5：为什么要让margin尽量大？</strong><br>答：因为大<code>margin</code>犯错的几率比较小，也就是更鲁棒啦。</p>
<p><strong>Q6：支持向量是什么？</strong><br>答：从上图可以看出，虚线上的点到划分超平面的距离都是一样的，实际上只有这几个点共同确定了超平面的位置，因此被称作 “支持向量<code>(support vectors)</code>”，“支持向量机” 也是由此来的。</p>
<p><strong>Q7：SVM 算法特性</strong></p>
<ol>
<li>训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以<code>SVM</code>不太容易产生<code>overfitting</code>。</li>
<li><code>SVM</code>训练出来的模型完全依赖于支持向量，即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果仍然会得到完全一样的模型。</li>
<li>一个<code>SVM</code>如果训练得出的支持向量个数比较少，那么<code>SVM</code>训练出的模型比较容易被泛化。</li>
<li><code>SVM</code>算法对大规模训练样本难以实施。</li>
<li>用<code>SVM</code>解决多分类问题存在困难。</li>
</ol>
<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>卷积的作用是<strong>提取特征</strong>，图像的空间联系是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。同时卷积通过权重共享降低参数量。</p>
<p>常见的卷积核选择都是<code>3x3</code>、<code>5x5</code>、<code>7x7</code>的，为什么很少见到偶数的卷积核呢？</p>
<ul>
<li>其主要原因是为了保护位置信息，使用奇数的卷积核，保证了中心点刚好在中间，避免了位置信息发生偏移。在需要使用位置信息的任务，如目标检测、目标识别、三维重建、图像重建等任务中非常有价值。</li>
<li>另一个就是因为<code>padding</code>时候能够保证左右对称，实际上也是为了位置信息。</li>
</ul>
<h3 id="普通卷积"><a href="#普通卷积" class="headerlink" title="普通卷积"></a>普通卷积</h3><p>在卷积神经网络中我们通常需要输入<code>in_channels</code>和<code>out_channels</code>，即输入通道数和输出通道数。</p>
<p>对于最初输入图片样本的通道数<code>in_channels</code>取决于图片的类型，如果是彩色的，即<code>RGB</code>类型，这时候通道数固定为 $3$ ，如果是灰度图，通道数为 $1$ 。<br>卷积完成之后，输出的通道数<code>out_channels</code>取决于过滤器的数量。从这个方向理解，这里的<code>out_channels</code>设置的就是过滤器的数目。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20230106231925.png" alt=""></p>
<p>如上图，输入的通道数为 $3$ ，所以卷积的时候每个需要对每个通道有个卷积核；输出的通道数为 $4$ ，输出的通道数就是我们设置的过滤器的数目。</p>
<h3 id="分组卷积-group-convolution"><a href="#分组卷积-group-convolution" class="headerlink" title="分组卷积(group convolution)"></a>分组卷积(group convolution)</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303212301932.png" alt=""></p>
<p><strong>我们用同等的参数量运算量生成了g个feature map！！！</strong></p>
<p>所以<code>group convolution</code>常用在轻量型高效网络中，因为它用少量的参数量和运算量就能生成大量的<code>feature map</code>，大量的<code>feature map</code>意味着能够编码更多的信息！</p>
<p>从分组卷积的角度来看，分组数 $g$ 就像一个控制旋钮，最小值是 $1$ ，此时 $g=1$ 的卷积就是普通卷积；最大值是输入<code>feature map</code>的通道数 $C$ ，此时 $g=C$ 的卷积就是<strong>depthwise sepereable convolution</strong>，即深度分离卷积，又叫逐通道卷积。</p>
<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化层是当前卷积神经网络中常用组件之一，它最早见于<code>LeNet</code>一文，称之为<code>Subsample</code>。自<code>AlexNet</code>之后采用<code>Pooling</code>命名。池化层是模仿人的视觉系统对数据进行降维，用更高层次的特征表示图像。</p>
<p>实施池化的目的：(1) 降低信息冗余；(2) 提升模型的尺度不变性、旋转不变性；(3) 降低特征维数，防止过拟合。</p>
<h2 id="神经网络中权值共享的理解？"><a href="#神经网络中权值共享的理解？" class="headerlink" title="神经网络中权值共享的理解？"></a>神经网络中权值共享的理解？</h2><p>所谓权值共享就是说给定一张输入图片，用一个卷积核来卷积这张图，卷积核里的值叫做权重，这张图的每个位置是被同一个卷积核扫的，即卷积的时候所用的权重是一样的。其实权值共享这个词说全了就是整张图片在使用同一个卷积核内的参数，比如一个 $3\times 3 \times 1$ 的卷积核，这个卷积核内 $9$ 个的参数被整张图共享，而不会因为图像内位置的不同而改变卷积核内的权系数。说的再直白一些，就是用一个卷积核不改变其内权系数的情况下卷积处理整张图片（当然<code>CNN</code>中每一层不会只有一个卷积核的，这样说只是为了方便解释而已）。<br>作用：大大减少网络训练参数的同时，还可以实现并行训练。</p>
<h2 id="对微调-fine-tuning-的理解，为什么要修改最后几层神经网络权值？"><a href="#对微调-fine-tuning-的理解，为什么要修改最后几层神经网络权值？" class="headerlink" title="对微调(fine-tuning)的理解，为什么要修改最后几层神经网络权值？"></a>对微调(fine-tuning)的理解，为什么要修改最后几层神经网络权值？</h2><p>使用预训练模型的好处，在于利用训练好的<code>SOTA</code>模型权重去做特征提取，可以节省我们训练模型和调参的时间。</p>
<p>为什么只微调最后几层神经网络权重，是因为：</p>
<ol>
<li><code>CNN</code>中更靠近底部的层（定义模型时先添加到模型中的层）编码的是更加通用的可复用特征，而更靠近顶部的层（最后添加到模型中的层）编码的是更专业化的特征。微调这些更专业化的特征更加有用，它更代表了新数据集上的有用特征。</li>
<li>训练的参数越多，过拟合的风险越大。很多<code>SOTA</code>模型拥有超过千万的参数，在一个不大的数据集上训练这么多参数是有过拟合风险的，除非你的数据集像<code>ImageNet</code>那样大。</li>
</ol>
<h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><p>优点：这种网络当时提出来成为了CNN标准的“模板”——叠加卷积层和池化层，并以一个全连接层结束网络。</p>
<p>缺点：当时一段时间并未火起来，原因在于当时历史背景，这个简单的网络仅有 $6000$ 多个参数，但训练起来费时且没有<code>GPU</code>加速，相比较于传统的<code>SVM</code>等算法，效率还是差了许多，所以并没有大放异彩。</p>
<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><ul>
<li>使用<code>ReLU</code>激活函数，不容易发生梯度消失问题。</li>
<li>对输入的数据进行了数据增强处理。（水平变换、光照增强、随机裁剪、平移变换等等）</li>
<li>首次使用<code>Dropout</code>防止过拟合。</li>
<li>采用两块<code>GPU</code>并行计算，每一层分两块进行计算，所以看着比较繁琐，这里也是由于<code>GPU</code>不是很好，所以要两块并行。</li>
</ul>
<h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><p>全部使用<code>3×3</code>卷积核的堆叠，来模拟更大的感受野，并且网络层数更深。<code>VGG</code>有五段卷积，每段卷积后接一层最大池化。卷积核数目逐渐增加。</p>
<p><strong>作者用的是多个3×3卷积叠加，而不是例如7×7、11×11的单个卷积，原因如下：</strong></p>
<ul>
<li><code>2</code>个<code>3×3</code>卷积叠加得到的理论感受野和一个<code>5×5</code>卷积的理论感受野是相同的，<code>3</code>个<code>3×3</code>卷积叠加得到的理论感受野和一个<code>7×7</code>卷积的理论感受野是相同的。</li>
<li>因为每个卷积层后面都会跟着一个<code>ReLU</code>，<code>3</code>个<code>3×3</code>卷积就会有<code>3</code>个<code>ReLU</code>，但是一个<code>7×7</code>的卷积只有一个，所以这么做可以使得模型的非线性拟合能力更强。</li>
<li>减少了参数数量。假设<code>3</code>个<code>3×3</code>卷积的输入和输出都是<code>C</code>个通道，那么参数数量为 $3 \times 3 \times 3 \times C \times C = 27C^{2}$  ，而<code>7×7</code>卷积的参数数量为 $7 \times 7 \times C \times C = 49C^{2}$ 。</li>
</ul>
<p><strong>1×1卷积的作用是什么？</strong></p>
<ul>
<li>为了在不影响卷积层感受野的前提下，增加模型的非线性。</li>
<li>可以压缩通道数，即减少特征的维度。</li>
</ul>
<h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p><code>GoogLeNet</code>专注于加深网络结构，与此同时引入了新的基本结构——<code>Inception</code>模块，从而来增加网络的宽度。每个原始<code>Inception</code>模块由<code>previous layer</code>、并行处理层及<code>filter concatenation</code>层组成。并行处理层包含 $4$ 个分支，即<code>1×1</code>卷积分支，<code>3×3</code>卷积分支，<code>5×5</code>卷积分支和<code>3×3</code>最大池化分支。一个关于原始<code>Inception</code>模块的最大问题是，<code>5×5</code>卷积分支即使采用中等规模的卷积核个数，在计算代价上也可能是无法承受的。这个问题在混合池化层之后会更为突出，很快的出现计算量的暴涨。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222128650.png" alt=""></p>
<p>为了克服原始<code>Inception</code>模块上的困难，<code>GoogLeNet</code>推出了一个新款，即采用<code>1×1</code>的卷积层来降低输入层的维度，使网络参数减少，因此减少网络的复杂性。因此得到降维<code>Inception</code>模块，称为<code>inception V1</code>。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222129537.png" alt=""></p>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><h3 id="ResNet中的一些亮点"><a href="#ResNet中的一些亮点" class="headerlink" title="ResNet中的一些亮点"></a>ResNet中的一些亮点</h3><ol>
<li>超深的网络结构（超过 $1000$ 层）。</li>
<li>提出<code>residual</code>（残差结构）模块。</li>
<li>使用<code>Batch Normalization</code>加速训练（丢弃<code>Dropout</code>）。</li>
</ol>
<h3 id="为什么采用residual"><a href="#为什么采用residual" class="headerlink" title="为什么采用residual?"></a>为什么采用residual?</h3><p>人们认为卷积层和池化层的层数越多，获取到的图片特征信息越全，学习效果也就越好。但是在实际的试验中发现，随着卷积层和池化层的叠加，不但没有出现学习效果越来越好的情况，反而两种问题：</p>
<ol>
<li>梯度消失和梯度爆炸<br>梯度消失：若每一层的误差梯度小于 $1$ ，反向传播时，网络越深，梯度越趋近于 $0$<br>梯度爆炸：若每一层的误差梯度大于 $1$ ，反向传播时，网络越深，梯度越来越大</li>
<li>退化问题<br>随着层数的增加，预测效果反而越来越差。</li>
</ol>
<ul>
<li>为了解决梯度消失或梯度爆炸问题，<code>ResNet</code>论文提出通过数据的预处理以及在网络中使用 <code>BN(Batch Normalization)</code>层来解决。</li>
<li>为了解决深层网络中的退化问题，可以人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系。这种神经网络被称为残差网络<code>(ResNets)</code>。<code>ResNet</code>论文提出了<code>residual</code>结构（残差结构）来减轻退化问题，随着网络的不断加深，效果并没有变差，而是变的更好了。</li>
</ul>
<h3 id="residual结构"><a href="#residual结构" class="headerlink" title="residual结构"></a>residual结构</h3><h4 id="residual的计算方式"><a href="#residual的计算方式" class="headerlink" title="residual的计算方式"></a>residual的计算方式</h4><p><code>residual</code>结构使用了一种<code>shortcut</code>的连接方式，也可理解为捷径。让特征矩阵隔层相加，注意 $\mathcal{F}(\mathbb{x})$ 和 $\mathbb{x}$ 形状要相同，所谓相加是特征矩阵相同位置上的数字进行相加。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317819.png" alt=""></p>
<h4 id="ResNet中两种不同的residual"><a href="#ResNet中两种不同的residual" class="headerlink" title="ResNet中两种不同的residual"></a>ResNet中两种不同的residual</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071317983.png" alt=""></p>
<ol>
<li>左侧残差结构称为<code>BasicBlock</code></li>
<li>右侧残差结构称为<code>Bottleneck</code><ul>
<li>其中第一层的 $1\times1$ 的卷积核的作用是对特征矩阵进行降维操作，将特征矩阵的深度由 $256$ 降为 $64$ ；</li>
<li>第三层的 $1\times1$ 的卷积核是对特征矩阵进行升维操作，将特征矩阵的深度由 $64$ 升成 $256$ 。<br>降低特征矩阵的深度主要是为了减少参数的个数。<br>如果采用<code>BasicBlock</code>，参数的个数应该是： $256\times256\times3\times3\times2=1179648$<br>采用<code>Bottleneck</code>，参数的个数是： $1\times1\times256\times64+3\times3\times64\times64+1\times1\times256\times64=69632$</li>
<li>先降后升为了主分支上输出的特征矩阵和捷径分支上输出的特征矩阵形状相同，以便进行加法操作。</li>
</ul>
</li>
</ol>
<h3 id="BatchNormalization"><a href="#BatchNormalization" class="headerlink" title="BatchNormalization"></a>BatchNormalization</h3><p><strong>BatchNormalization就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布。</strong></p>
<p>在神经网络中, 数据分布对训练会产生影响。比如某个神经元 $x$ 的值为 $1$ ，某个 <code>Weights</code> 的初始值为 $0.1$ ，这样后一层神经元计算结果就是 $Wx = 0.1$ ；又或者 $x = 20$ ，这样 $Wx$ 的结果就为 $2$ 。现在还不能看出什么问题,，但是，当我们加上一层激活函数，激活这个  $Wx$ 值的时候，问题就来了。如果使用像 <code>tanh</code> 的激活函数， $Wx$ 的激活值就变成了 $\approx 0.1$ 和 $\approx 1$, 接近于 $1$ 的部已经处在了激活函数的饱和阶段, 也就是 $x$ 无论再怎么扩大， <code>tanh</code> 激励函数输出值也还是接近 $1$ 。</p>
<p>我们为了避免这种情况，就会对数据进行归一化，<strong>对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0​，方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。</strong>同时了为了恢复出原始的某一层所学到的特征，我们引入了这个可学习重构参数 $\gamma$ 、$\beta$ ，让我们的网络可以学习恢复出原始网络所要学习的特征分布。</p>
<h3 id="ResNet为什么不用Dropout？"><a href="#ResNet为什么不用Dropout？" class="headerlink" title="ResNet为什么不用Dropout？"></a>ResNet为什么不用Dropout？</h3><p><code>Dropout</code>与<code>BN</code>不兼容。<code>BN</code>在训练过程对每个单个样本的<code>forward</code>均引入多个样本的统计信息，相当于自带一定噪音，起到正则效果，所以也就基本消除了<code>Dropout</code>的必要。</p>
<h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p>每个层从前面的所有层获得额外的输入，并将自己的特征映射传递到后续的所有层，使用级联<code>(Concatenation)</code>方式，每一层都在接受来自前几层的“集体知识”<code>(collective knowledge)</code>。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303222332494.png" alt=""></p>
<p>如图所示，第<code>i</code>层的输入不仅与<code>i-1</code>层的输出相关，还有所有之前层的输出有关，记作： $X_{l}=H_{l}([X_{0},\cdots,X_{l-1}])$ 。第<code>l</code>层产生 $k_{0}+(l-1)k$ 个<code>feature maps</code>，其中 $k$ 称为称为网络的增长率。</p>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>循环神经网络<code>(Recurrent Neural Network, RNN)</code>是一类以序列<code>(sequence)</code>数据为输入，在序列的演进方向进行递归<code>(recursion)</code>且所有节点（循环单元）按链式连接的递归神经网络<code>(recursive neural network)</code> 。</p>
<p>对循环神经网络的研究始于二十世纪八九十年代，并在二十一世纪初发展为深度学习算法之一 ，其中双向循环神经网络<code>(Bidirectional RNN, Bi-RNN)</code>和长短期记忆网络<code>(Long Short-Term Memory networks, LSTM)</code>是常见的循环神经网络。</p>
<h2 id="为什么需要RNN？"><a href="#为什么需要RNN？" class="headerlink" title="为什么需要RNN？"></a>为什么需要RNN？</h2><p>在<code>CNN</code>网络中的训练样本的数据为<code>IID</code>数据（独立同分布数据），所解决的问题也是分类问题或者回归问题或者是特征表达问题。<strong>但更多的数据是不满足IID的</strong>，如语言翻译，自动文本生成。它们是一个序列问题，包括时间序列和空间序列。比如时间序列数据，这类数据是在不同时间点上收集到的数据，反映了某一事物、现象等随时间的变化状态或程度。一般的神经网络，在训练数据足够、算法模型优越的情况下，给定特定的 $x$ ，就能得到期望 $y$ 。其一般处理单个的输入，前一个输入和后一个输入完全无关，但实际应用中，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。 这时就要用到<code>RNN</code>网络，<code>RNN</code>的结构图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071328562.png" alt=""></p>
<h2 id="RNN的主要应用领域"><a href="#RNN的主要应用领域" class="headerlink" title="RNN的主要应用领域"></a>RNN的主要应用领域</h2><p>可以说只要考虑时间先后顺序的问题都可以使用<code>RNN</code>来解决，这里主要说一下几个常见的应用领域：</p>
<ul>
<li>自然语言处理<code>(NLP)</code>：主要有视频处理，文本生成，语言模型，图像处理。</li>
<li>机器翻译，机器写文章。</li>
<li>语音识别。</li>
<li>图像描述生成。</li>
<li>文本相似度计算。</li>
<li>推荐系统。例如：音乐推荐、网易考拉商品推荐、<code>Youtube</code>视频推荐等新的应用领域。</li>
</ul>
<h2 id="RNN的计算过程"><a href="#RNN的计算过程" class="headerlink" title="RNN的计算过程"></a>RNN的计算过程</h2><p><code>RNN</code>引入了隐状态 $h$ ， $h$ 可对序列数据提取特征，接着再转换为输出。首先我们计算 $h_{1}$ ：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071332166.jpg" alt=""></p>
<p><code>RNN</code>中，每个步骤使用的参数 $U,W,b$ 相同， $h_{2},h_{3},h_{4}$ 的计算方式和 $h_{1}$ 类似，其计算结果如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071333924.jpg" alt=""></p>
<p>接下来，计算<code>RNN</code>的输出 $y_1$ ，采用<code>Softmax</code>作为激活函数，根据 $y_n=f(Wx+b)$ ，得到 $y_1$ :</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071335563.jpg" alt=""></p>
<p>使用和 $y_1$ 相同的参数 $V,c$ ，得到 $y_2,y_3,y_4$ 的输出结构：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071336003.jpg" alt=""></p>
<h2 id="RNN的建模方式"><a href="#RNN的建模方式" class="headerlink" title="RNN的建模方式"></a>RNN的建模方式</h2><h3 id="一对多-vector-to-sequence"><a href="#一对多-vector-to-sequence" class="headerlink" title="一对多(vector-to-sequence)"></a>一对多(vector-to-sequence)</h3><p>输入是一个单独的值，输出是一个序列。此时，有两种主要建模方式：</p>
<p>方式一：可只在其中的某一个序列进行计算，比如序列第一个进行输入计算，其建模方式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339814.jpg" alt=""></p>
<p>方式二：把输入信息 $X$ 作为每个阶段的输入，其建模方式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071339457.jpg" alt=""></p>
<h3 id="多对一-sequence-to-vector"><a href="#多对一-sequence-to-vector" class="headerlink" title="多对一(sequence-to-vector)"></a>多对一(sequence-to-vector)</h3><p>输入是一个序列，输出是一个单独的值，此时通常在最后的一个序列上进行输出变换，其建模如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071340668.jpg" alt=""></p>
<h3 id="多对多-Encoder-Decoder"><a href="#多对多-Encoder-Decoder" class="headerlink" title="多对多(Encoder-Decoder)"></a>多对多(Encoder-Decoder)</h3><p><strong>步骤一</strong>：将输入数据编码成一个上下文向量 $c$ ，这部分称为<code>Encoder</code>，得到 $c$ 有多种方式，最简单的方法就是把<code>Encoder</code>的最后一个隐状态赋值给 $c$ ，还可以对最后的隐状态做一个变换得到 $c$ ，也可以对所有的隐状态做变换。其示意如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071341013.jpg" alt=""></p>
<p><strong>步骤二</strong>：用另一个<code>RNN</code>网络（我们将其称为<code>Decoder</code>）对其进行编码。</p>
<p>方法一是将步骤一中的 $c$ 作为初始状态输入到<code>Decoder</code>，示意图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071342703.jpg" alt=""></p>
<p>方法二是将 $c$ 作为<code>Decoder</code>的每一步输入，示意图如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071343619.jpg" alt=""></p>
<h2 id="RNN中为什么会出现梯度消失？如何解决？"><a href="#RNN中为什么会出现梯度消失？如何解决？" class="headerlink" title="RNN中为什么会出现梯度消失？如何解决？"></a>RNN中为什么会出现梯度消失？如何解决？</h2><p><strong>梯度消失的原因：</strong><code>Sigmoid</code>函数的导数范围是 $(0,0.25]$，<code>tanh</code>函数的导数范围是 $(0,1]$ ，他们的导数最大都不大于 $1$ ，如果取<code>tanh</code>或<code>Sigmoid</code>函数作为激活函数嵌套到<code>RNN</code>中，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于 $0$ ，这就是“梯度消失“现象。实际使用中，会优先选择<code>tanh</code>函数，原因是<code>tanh</code>函数相对于<code>Sigmoid</code>函数来说梯度较大，收敛速度更快且引起梯度消失更慢。</p>
<p> <strong>解决RNN中的梯度消失方法主要有：</strong></p>
<ol>
<li>选取更好的激活函数，如<code>ReLU</code>激活函数。<code>ReLU</code>函数的左侧导数为 $0$ ，右侧导数恒为 $1$ ，这就避免了“梯度消失“的发生。但恒为 $1$ 的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。</li>
<li>加入<code>BN</code>层，其优点包括可加速收敛、控制过拟合，可以少用或不用<code>Dropout</code>和正则、降低网络对初始化权重不敏感，且能允许使用较大的学习率等。</li>
<li>改变传播结构，选择更高级的模型，例如：<code>LSTM</code>结构可以有效解决这个问题。</li>
</ol>
<h2 id="RNN的注意力机制"><a href="#RNN的注意力机制" class="headerlink" title="RNN的注意力机制"></a>RNN的注意力机制</h2><p>在上述的<code>Encoder-Decoder</code>结构中，<code>Encoder</code>把所有的输入序列都编码成一个统一的语义特征 $c$ 再解码。因此， $c$ 中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。如机器翻译问题，当要翻译的句子较长时，一个 $c$ 可能存不下那么多信息，就会造成翻译精度的下降。<code>Attention</code>机制通过在每个时间输入不同的 $c$ 来解决此问题。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303071356215.png" alt=""></p>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><h2 id="传统RNN存在的问题"><a href="#传统RNN存在的问题" class="headerlink" title="传统RNN存在的问题"></a>传统RNN存在的问题</h2><p><strong>长期依赖(Long Term Dependencies)</strong></p>
<p>在深度学习领域中（尤其是<code>RNN</code>），“长期依赖”问题是普遍存在的。长期依赖产生的原因是当神经网络的节点经过许多阶段的计算后，之前比较长的时间片的特征已经被覆盖，例如：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101147762.png" alt=""></p>
<p>我们想预测<code>full</code>之前系动词的单复数情况，显然<code>full</code>是取决于第二个单词<code>cat</code>的单复数情况，而非其前面的单词<code>food</code>。随着数据时间片的增加，<code>RNN</code>丧失了学习连接如此远的信息的能力。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101148082.png" alt=""></p>
<p><strong>梯度消失/爆炸</strong></p>
<p>梯度消失和梯度爆炸是困扰<code>RNN</code>模型训练的关键原因之一，产生梯度消失和梯度爆炸是由于<code>RNN</code>的权值矩阵循环相乘导致的，相同函数的多次组合会导致极端的非线性行为。梯度消失和梯度爆炸主要存在<code>RNN</code>中，因为<code>RNN</code>中每个时间片使用相同的权值矩阵。对于一个<code>DNN</code>，虽然也涉及多个矩阵的相乘，但是通过精心设计权值的比例可以避免梯度消失和梯度爆炸的问题。</p>
<p>处理梯度爆炸可以采用梯度截断的方法。所谓梯度截断是指将梯度值超过阈值 $\theta$ 的梯度手动降到 $\theta$ 。虽然梯度截断会一定程度上改变梯度的方向，但梯度截断的方向依旧是朝向损失函数减小的方向。</p>
<p>对比梯度爆炸，梯度消失不能简单的通过类似梯度截断的阈值式方法来解决，因为长期依赖的现象也会产生很小的梯度。在上面例子中，我们希望 $t_{9}$ 时刻能够读到 $t_{1}$ 时刻的特征，在这期间内我们自然不希望隐层节点状态发生很大的变化，所以 $[t_{2},t_{8}]$ 时刻的梯度要尽可能的小才能保证梯度变化小。很明显，如果我们刻意提高小梯度的值将会使模型失去捕捉长期依赖的能力。</p>
<h2 id="LSTM-1"><a href="#LSTM-1" class="headerlink" title="LSTM"></a>LSTM</h2><p><code>LSTM</code>的全称是<code>Long Short Term Memory</code>，顾名思义，它具有记忆长短期信息的能力的神经网络。<code>LSTM</code>提出的动机是为了解决上面我们提到的长期依赖问题。传统的<code>RNN</code>节点输出仅由权值，偏置以及激活函数决定。<code>RNN</code>是一个链式结构，每个时间片使用的是相同的参数。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101504342.png" alt=""></p>
<p>而<code>LSTM</code>之所以能够解决<code>RNN</code>的长期依赖问题，是因为<code>LSTM</code>引入了门<code>(gate)</code>机制用于控制特征的流通和损失。对于上面的例子，<code>LSTM</code>可以做到在 $t_{9}$ 时刻将 $t_{2}$ 时刻的特征传过来，这样就可以非常有效的判断 $t_{9}$ 时刻使用单数还是复数了。<code>LSTM</code>是由一系列<code>LSTM</code>单元<code>(LSTM Unit)</code>组成，其链式结构如下图。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101504156.png" alt=""></p>
<p><code>LSTM</code>的核心部分是在上图最上边类似于传送带的部分，这一部分一般叫做单元状态<code>(cell state)</code>它自始至终存在于<code>LSTM</code>的整个链式系统中。其中 $C_{t}=f_{t} \times C_{t-1} + i_{t} \times \tilde{C}_{t-1}$ ，其中 $f_{t}$ 叫做遗忘门，表示 $C_{t-1}$ 的哪些特征被用于计算 $C_{t}$ 。 $f_{t}$ 是一个向量，向量的每个元素均位于 $[0,1]$ 范围内。通常我们使用<code>Sigmoid</code>作为激活函数，<code>Sigmoid</code>的输出是一个介于 $[0,1]$ 区间内的值，但是当你观察一个训练好的<code>LSTM</code>时，你会发现门的值绝大多数都非常接近 $0$ 或者 $1$ ，其余的值少之又少。其中 $\otimes$ 是<code>LSTM</code>最重要的门机制，表示 $f_{t}$ 和 $C_{t-1}$ 之间的单位乘的关系。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101539655.png" alt=""></p>
<p> $\tilde{C}_{t}$ 表示单元状态更新值，由输入数据 $x_{t}$ 和隐节点 $h_{t-1}$ 经由一个神经网络层得到，单元状态更新值的激活函数通常使用<code>tanh</code>。 $i_{t}$ 叫做输入门，同 $f_{t}$ 一样也是一个元素介于 $[0,1]$ 区间内的向量，同样由 $x_{t}$ 和 $h_{t-1}$ 经由<code>Sigmoid</code>激活函数计算而成。 $i_{t}$ 用于控制 $\tilde{C}_{t}$ 的哪些特征用于更新 $C_{t}$ ，使用方式和 $f_t$ 相同。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101545859.png" alt=""></p>
<p>最后，为了计算预测值 $\hat{y}_{t}$ 和生成下个时间片完整的输入，我们需要计算隐节点的输出 $h_{t}$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101553212.png" alt=""></p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p><code>Transformer</code>是一个利用注意力机制来提高模型训练速度的模型。<code>Transformer</code>可以说是完全基于自注意力机制的一个深度学习模型，因为它适用于并行化计算，和它本身模型的复杂程度导致它在精度和性能上都要高于之前流行的<code>RNN</code>循环神经网络。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302227637.png" alt=""></p>
<p>当我输入一个文本的时候，该文本数据会先经过一个叫<code>Encoders</code>的模块，对该文本进行编码，然后将编码后的数据再传入一个叫<code>Decoders</code>的模块进行解码，解码后就得到了翻译后的文本，对应的我们称<code>Encoders</code>为编码器，<code>Decoders</code>为解码器。一般情况下，<code>Encoders</code>里边有 $6$ 个小编码器<code>(Encoder)</code>，<code>Decoders</code>里边有 $6$ 个小解码器<code>(Decoder)</code>。我们看到，在编码部分，每一个的小编码器的输入是前一个小编码器的输出，而每一个小解码器的输入不光是它的前一个解码器的输出，还包括了整个编码部分的输出。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302042116.png" alt=""></p>
<h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><h2 id="编码器-Encoder"><a href="#编码器-Encoder" class="headerlink" title="编码器(Encoder)"></a>编码器(Encoder)</h2><p>我们放大一个<code>Encoder</code>，发现里边的结构是一个多头自注意力机制加上一个前馈神经网络。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302053360.png" alt=""></p>
<p>像大部分<code>NLP</code>应用一样，我们首先将每个输入单词通过词嵌入算法转换为词向量，每个单词都被嵌入为 $512$ 维的向量。</p>
<ul>
<li>计算自注意力的第一步就是从每个编码器的输入向量（每个单词的词向量）中生成三个向量。也就是说对于每个单词，我们创造一个查询向量、一个键向量和一个值向量。这三个向量是通过词嵌入与三个权重矩阵后相乘创建的。可以发现这些新向量在维度上比词嵌入向量更低。他们的维度是 $64$ ，而词嵌入和编码器的输入/输出向量的维度是 $512$ 。但实际上不强求维度更小，这只是一种基于架构上的选择，它可以使多头注意力<code>(Multi-Head Attention)</code>的大部分计算保持不变。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302112845.png" alt=""></p>
<ul>
<li>计算自注意力的第二步是计算得分。这个得分是通过计算<code>Q</code>与各个单词的<code>K</code>向量的点积得到的。我们以<code>X1</code>为例，分别将<code>Q1</code>和<code>K1</code>、<code>K2</code>进行点积运算，假设分别得到得分 $112$ 和 $96$ 。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302133753.png" alt=""></p>
<ul>
<li>将得分分别除以一个特定数值 $8$ （<code>K</code>向量的维度的平方根，通常<code>K</code>向量的维度是 $64$ ）这能让梯度更加稳定，则得到结果如下：</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302134012.png" alt=""></p>
<ul>
<li>将上述结果进行<code>softmax</code>运算得到，<code>softmax</code>主要将分数标准化，使他们都是正数并且加起来等于 $1$ 。这个<code>softmax</code>分数决定了每个单词对编码当下位置的贡献。显然，已经在这个位置上的单词将获得最高的<code>softmax</code>分数，但有时关注另一个与当前单词相关的单词也会有帮助。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302137784.png" alt=""></p>
<ul>
<li>将<code>V</code>向量乘上<code>softmax</code>的结果，这个思想主要是为了保持我们想要关注的单词的值不变，而掩盖掉那些不相关的单词（例如将他们乘上很小的数字）。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302138098.png" alt=""></p>
<ul>
<li>将带权重的各个<code>V</code>向量加起来，至此，产生在这个位置上（第一个单词）的<code>self-attention</code>层的输出，其余位置的<code>self-attention</code>输出也是同样的计算方式。（注：自注意力的另一种解释就是在编码某个单词时，就是将所有单词的表示（值向量<code>V</code>）进行加权求和，而权重是通过该词的表示（键向量<code>K</code>）与被编码词表示（查询向量<code>Q</code>）的点积并通过<code>softmax</code>得到。）</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302139598.png" alt=""></p>
<p>论文为了进一步细化自注意力机制层，增加了<strong>多头注意力机制</strong>的概念，这从两个方面提高了自注意力层的性能。</p>
<ol>
<li>扩展了模型关注不同位置的能力，比如<code>Apple</code>可能和<code>Banana</code>比较相关，但是如果使用单个自注意力机制可能这种关系就被<code>Apple</code>自己支配了（体现在<code>softmax</code>之后的权重最大），而采用多头自注意力机制则可以缓解这种现象。</li>
<li>他给了自注意力层多个表示子空间。对于多头自注意力机制，我们不止有一组权重矩阵，而是有多组（论文中使用 $8$ 组），所以每个编码器/解码器使用 $8$ 个头（可以理解为 $8$ 个互不干扰自的注意力机制运算），每一组的<code>Q/K/V</code>都不相同。然后，得到 $8$ 个不同的权重矩阵 <code>Z</code>，每个权重矩阵被用来将输入向量投射到不同的表示子空间。论文中说到这样的好处是可以允许模型在不同的表示子空间里学习到相关的信息。输出矩阵的维度是（序列长度×单词向量长度）</li>
</ol>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303302156146.png" alt=""></p>
<p>为了解决梯度消失的问题，在<code>Encoder</code>和<code>Decoder</code>中都是用了<strong>残差神经网络</strong>的结构，即每一个前馈神经网络的输入不光包含上述<code>self-attention</code>的输出，还包含最原始的输入。</p>
<h2 id="解码器-Decoder"><a href="#解码器-Decoder" class="headerlink" title="解码器(Decoder)"></a>解码器(Decoder)</h2><p>同样的，在<code>Decoder</code>中使用的也是同样的结构。也是首先对输出计算自注意力得分，不同的地方在于，进行过自注意力机制后，将<code>self-attention</code>的输出再与<code>Encoder</code>模块的输出计算一遍注意力机制得分之后，再进入前馈神经网络模块。</p>
<h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p>解码器输出本来是一个浮点型的向量，怎么转化成这两个词呢？解决办法是在最后的线性层接上一个<code>softmax</code>，其中线性层是一个简单的全连接神经网络，它将解码器产生的向量投影到一个更高维度的向量<code>(logits)</code>上，假设我们模型的词汇表是 $10000$ 个词，那么<code>logits</code>就有 $10000$ 个维度，每个维度对应一个唯一的词的得分。之后的<code>softmax</code>层将这些分数转换为概率。选择概率最大的维度，并对应地生成与之关联的单词作为此时间步的输出就是最终的输出。</p>
<h2 id="位置编码-Positional-Encoding"><a href="#位置编码-Positional-Encoding" class="headerlink" title="位置编码(Positional Encoding)"></a>位置编码(Positional Encoding)</h2><p><code>Transformer</code>中没有考虑顺序信息，那怎么办呢，我们可以在输入中做手脚，把输入变得有位置信息。我们可以给每个词向量加上一个有顺序特征的向量，发现<code>sin</code>函数和<code>cos</code>函数能够很好的表达这种特征，所以通常位置向量用以下公式来表示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310947858.png" alt=""></p>
<p>也即第 $t$ 个位置的位置编码为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310948367.png" alt=""></p>
<p>对编码的可视化：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303310952548.png" alt=""></p>
<h3 id="Vanilla-Transformer的位置编码的特点"><a href="#Vanilla-Transformer的位置编码的特点" class="headerlink" title="Vanilla Transformer的位置编码的特点"></a>Vanilla Transformer的位置编码的特点</h3><ol>
<li>只要位置小于 $10000$ ，每一个位置的编码都是不同的。</li>
<li>奇数维度之间或者偶数维度之间周期不同。</li>
<li>也可以很好的表示相对位置信息。给定k,存在一个固定的与 $k$ 相关的线性变换矩阵，从而由 <code>pos</code> 的位置编码线性变换而得到 <code>pos+k</code> 的位置编码。这个相对位置信息可能可以被模型发现而利用。因为绝对位置信息只保证了各个位置不一样，但是并不是像 $0,1,2$ 这样的有明确前后关系的编码。</li>
</ol>
<p>我们拿出位置编码的两个维度出来做个例子，其他维度也是一样的，可以拼接起来变成完整的位置编码：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311001584.png" alt=""></p>
<p>其中 $M$ 为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311002558.png" alt=""></p>
<p>上面的操作也只可以看到线性关系，怎么可以更直白地知道每个<code>token</code>的距离关系？我们将两个位置编码对应相乘，可以发现发现相乘后的结果为一个余弦的加和。这里影响值的因素就是 $k$ 。如果两个<code>token</code>的距离越大，也就是 $k$ 越大，根据余弦函数的性质可以知道，两个位置编码相乘结果越小。这样的关系可以得到，如果两个<code>token</code>距离越远则乘积的结果越小。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311016054.png" alt=""></p>
<h3 id="其他编码方式"><a href="#其他编码方式" class="headerlink" title="其他编码方式"></a>其他编码方式</h3><p><strong>用[0,1]范围标记位置</strong></p>
<p>产生的问题是，当序列长度不同时，<code>token</code>间的相对距离是不一样的。例如在序列长度为 $3$ 时，<code>token</code>间的相对距离为 $0.5$ ；在序列长度为 $4$ 时，<code>token</code>间的相对距离就变为 $0.33$ 。</p>
<p><strong>用整型值标记位置</strong></p>
<p>存在的问题是：模型可能遇见比训练时所用的序列更长的序列，不利于模型的泛化；模型的位置表示是无界的，随着序列长度的增加，位置值会越来越大。</p>
<p><strong>用二进制向量标记位置</strong></p>
<p>这种编码方式也存在问题是编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的。</p>
<h3 id="Vanilla-Transformer位置编码的缺点以及改进"><a href="#Vanilla-Transformer位置编码的缺点以及改进" class="headerlink" title="Vanilla Transformer位置编码的缺点以及改进"></a>Vanilla Transformer位置编码的缺点以及改进</h3><p>看一个序列中，第 $i$ 个单词和第 $j$ 个单词的<code>attention score</code>的计算：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311035374.png" alt=""></p>
<p>其中 $W_{q}$ ， $W_{k}$ 分别是<code>Multi-Head Attention</code>给每个头加的<code>Query</code>和<code>Key</code>参数， $E_{x_{i}}$ 和 $E_{x_{j}}$ 是 $x_{i}$ 和 $x_{j}$ 的词嵌入， $U_{i}$ 和 $U_{j}$ 是第 $i$ 个位置和第 $j$ 个位置的位置向量。因式分解得到下式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311039224.png" alt=""></p>
<p>实际上，按照<code>Vanilla Transformer</code>的位置编码方法，如果没有 $W_{q}$ 和 $W_{k}$ 那么它是包含相对位置信息的，证明见上上小节。但是中间加入一个“不可知”的线性变换以后，就没有相对位置信息了，这个可以使用实验证明，具体如下图：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311047793.png" alt=""></p>
<p><strong>Transformer中加入相对位置信息的改进方法</strong></p>
<ul>
<li><p>既然相对位置信息是在<code>self-attention</code>计算时候丢失的，那么最直接的想法就是在计算<code>self-attention</code>的时候再加回来。该工作出自<code>Transformer</code>的原班人马，具体做法是在计算<code>attention score</code>和<code>weighted value</code>时各加入一个可训练的表示相对位置的参数，并且<code>multi-head</code>之间可以共享。</p>
</li>
<li><p>改写<code>self-attention</code>的计算公式</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202303311102674.png" alt=""></p>
</li>
<li><p>前面两个方法都是基于”词向量+位置向量”的模式，说白了是在“亡羊补牢”，而没有一开始就修建一个牢固的羊圈。而一种新的角度是推导出一个<strong>复数域</strong>的词向量方法，理论十分优美。</p>
</li>
</ul>
<h1 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a>Vision Transformer</h1><h2 id="模型组成"><a href="#模型组成" class="headerlink" title="模型组成"></a>模型组成</h2><p>模型由三个模块组成：</p>
<ul>
<li><code>Linear Projection of Flattened Patches</code>（<code>Embedding</code>层）</li>
<li><code>Transformer Encoder</code></li>
<li><code>MLP Head</code>（最终用于分类的层结构）</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101646509.png" alt=""></p>
<h2 id="Embedding层结构详解"><a href="#Embedding层结构详解" class="headerlink" title="Embedding层结构详解"></a>Embedding层结构详解</h2><p>对于标准的<code>Transformer</code>模块，要求输入的是<code>token</code>（向量）序列，即二维矩阵<code>[num_token, token_dim]</code>，<code>0-9</code>对应的<code>token</code>都是向量，以<code>ViT-B/16</code>为例，每个<code>token</code>向量长度为 $768$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101836412.png" alt=""></p>
<p>对于图像数据而言，其数据格式为<code>[H, W, C]</code>是三维矩阵明显不是<code>Transformer</code>想要的。所以需要先通过一个<code>Embedding</code>层来对数据做个变换。如上图所示，首先将一张图片按给定大小分成一堆<code>Patches</code>。以<code>ViT-B/16</code>为例，将输入图片<code>(224x224)</code>按照<code>16x16</code>大小的<code>Patch</code>进行划分，划分后会得到 $( 224 / 16 )^{2} = 196$ 个<code>Patches</code>。接着通过线性映射将每个<code>Patch</code>映射到一维向量中，以<code>ViT-B/16</code>为例，每个<code>Patch</code>数据<code>shape</code>为 $[16, 16, 3]$ 通过映射得到一个长度为 $768$ 的向量（后面都直接称为<code>token</code>）。</p>
<p><strong>在输入Transformer Encoder之前注意需要加上[class]token以及Position Embedding。</strong> 在原论文中，作者说参考<code>BERT</code>，在刚刚得到的一堆<code>tokens</code>中插入一个专门用于分类的<code>[class]token</code>，这个<code>[class]token</code>是一个可训练的参数，数据格式和其他<code>token</code>一样都是一个向量，以<code>ViT-B/16</code>为例，就是一个长度为<code>768</code>的向量，与之前从图片中生成的<code>tokens</code>拼接在一起， $([1, 768], [196, 768]) \rightarrow [197, 768]$ 。然后关于<code>Position Embedding</code>就是之前<code>Transformer</code>中讲到的<code>Positional Encoding</code>，这里的<code>Position Embedding</code>采用的是一个可训练的参数，是直接叠加在<code>tokens</code>上的<code>(add)</code>，所以<code>shape</code>要一样。以<code>ViT-B/16</code>为例，刚刚拼接<code>[class]token</code>后<code>shape</code>是 $[197, 768]$ ，那么这里的<code>Position Embedding</code>的<code>shape</code>也是 $[197, 768]$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101836048.png" alt=""></p>
<h2 id="Transformer-Encoder详解"><a href="#Transformer-Encoder详解" class="headerlink" title="Transformer Encoder详解"></a>Transformer Encoder详解</h2><p><code>Transformer Encoder</code>其实就是重复堆叠<code>Encoder Block</code> $L$ 次，主要由以下几部分组成：</p>
<ul>
<li><code>Layer Norm</code>，这种<code>Normalization</code>方法主要是针对<code>NLP</code>领域提出的，这里是对每个<code>token</code>进行<code>Norm</code>处理。</li>
<li><code>Multi-Head Attention</code>，也就是<code>Transformer</code>中的多头注意力。</li>
<li><code>Dropout/DropPath</code>，在原论文的代码中是直接使用的<code>Dropout</code>层，但<code>rwightman</code>实现的代码中使用的是<code>DropPath(stochastic depth)</code>，可能后者会更好一点。</li>
<li><code>MLP Block</code>，就是全连接+<code>GELU</code>激活函数+<code>Dropout</code>组成也非常简单，需要注意的是第一个全连接层会把输入节点个数翻 $4$ 倍 $[197, 768] \rightarrow [197, 3072]$ ，第二个全连接层会还原回原节点个数 $[197, 3072] \rightarrow [197, 768]$ 。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101738369.png" alt=""></p>
<h2 id="MLP-Head详解"><a href="#MLP-Head详解" class="headerlink" title="MLP Head详解"></a>MLP Head详解</h2><p>上面通过<code>Transformer Encoder</code>后输出的<code>shape</code>和输入的<code>shape</code>是保持不变的，以<code>ViT-B/16</code>为例，输入的是 $[197, 768]$ 输出的还是 $[197, 768]$。注意，在<code>Transformer Encoder</code>后其实还有一个<code>Layer Norm</code>没有画出来。这里我们只是需要分类的信息，所以我们只需要提取出<code>[class]token</code>生成的对应结果就行，即 $[197, 768]$ 中抽取出 $[class]token$ 对应的 $[1, 768]$ 。接着我们通过<code>MLP Head</code>得到我们最终的分类结果。<code>MLP Head</code>原论文中说在训练<code>ImageNet21K</code>时是由<code>Linear</code>+<code>tanh</code>激活函数+<code>Linear</code>组成。但是迁移到<code>ImageNet1K</code>上或者自己的数据上时，只用一个<code>Linear</code>即可。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304101841213.png" alt=""></p>
<h1 id="PyTorch和TensorFlow的区别"><a href="#PyTorch和TensorFlow的区别" class="headerlink" title="PyTorch和TensorFlow的区别"></a>PyTorch和TensorFlow的区别</h1><h2 id="动态图与静态图"><a href="#动态图与静态图" class="headerlink" title="动态图与静态图"></a>动态图与静态图</h2><p><code>TensorFlow</code>最初选择使用静态图，这样的设计带来了较高的性能，但在构建网络时较为烦琐，用户需要专门学习<code>TensorFlow</code>的语法架构才能搭建网络，同时很难调试。<code>PyTorch</code>选择使用动态图，动态图的设计模式更加符合人类的思考过程，方便查看、修改中间变量的值，用户可以轻松地搭建网络进行训练。目前，<code>TensorFlow 2.0</code>之后的版本已经支持动态图的构建，并且提供动态图与静态图的转换功能。</p>
<h2 id="部署和可扩展性"><a href="#部署和可扩展性" class="headerlink" title="部署和可扩展性"></a>部署和可扩展性</h2><p>如果您的项目范围很大，需要大规模部署，或者项目涉及跨平台，那么您的选择应该是<code>TensorFlow</code>，<code>TensorFlow</code>提供了<code>TensorFlow Serving</code>和<code>TensorFlow Lite</code>，可以便捷地将训练好的模型部署到集群以及移动设备上。如果它只是一个较小规模的研究项目的原型设计或类似的东西，那么<code>PyTorch</code>更好。</p>
<h2 id="资源优化和利用率"><a href="#资源优化和利用率" class="headerlink" title="资源优化和利用率"></a>资源优化和利用率</h2><p>如果您正在寻找更好的资源利用率和优化，如<code>GPU</code>，那么<code>PyTorch</code>肯定是首选。但是，当涉及到<code>TensorFlow</code>时，它使用当时可用的所有<code>GPU</code>容量，因此，功能略微缓慢。</p>
<h2 id="学术研究和开源代码"><a href="#学术研究和开源代码" class="headerlink" title="学术研究和开源代码"></a>学术研究和开源代码</h2><p>在学术界<code>PyTorch</code>有很多好评，其中四分之三的论文使用它。而且最早使用<code>TensorFlow</code>的研究人员中，很多人已经迁移到了<code>PyTorch</code>。正因如此，研究会影响教学，从而决定学生学到的是什么。所以如今，大学生对<code>PyTorch</code>了解的相对多一些。</p>
<h1 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p><strong>CTR(Click-Through-Rate)</strong>：即点击通过率，是互联网广告常用的术语，指网络广告的点击到达率，即该广告的实际点击次数（严格的来说，可以是到达目标页面的数量）除以广告的展现量。</p>
<p>点击量过小时，是由两种原因造成的，展现量过小，或点击数偏低。</p>
<ol>
<li>展现量低，进而点击数也小。展现量过低说明潜在受众搜索需求发生的较少，也即推广结果展现在潜在受众前的机会较少，推广商户可以通过拓展关键词来提高展现量，即提高推广信息展现的机会。</li>
<li>展现量高，但是点击数偏低，造成点击率偏低。这种情况可能的原因是：<ul>
<li>关键词与文案的相关性不高，所以无法满足潜在受众的需求，进而点击数小。可以通过改善文案写作，提高关键词与文案的相关性来提高点击率。</li>
<li>推广结果的平均排名较低，不具有竞争力。可以通过调高平均点击价格来提高排名。</li>
<li>关键词匹配模式的问题。例如，推广商户购买了“葡萄”等相关关键词，用户在搜索“葡萄牙”时商户的推广结果也可能会出现，这时推广结果就是无效展现，即为推广结果信息没有展现在潜在受众前。此种情况就需通过“否定匹配”模式来解决，将“葡萄牙”设置为否定匹配，即为用户在搜索“葡萄牙”时，推广商户的推广结果不会展现，降低了无效展现的风险。</li>
</ul>
</li>
</ol>
<h2 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h2><p>协同过滤算法<code>(Collaborative Filtering)</code>是比较经典常用的推荐算法，它是一种完全依赖用户和物品之间行为关系的推荐算法。我们从它的名字“协同过滤”中，也可以窥探到它背后的原理，就是 “协同大家的反馈、评价和意见，一起对海量的信息进行过滤，从中筛选出用户可能感兴趣的信息”。协同过滤算法主要分为三类：</p>
<ul>
<li>基于物品的协同过滤算法：给用户推荐与他之前喜欢的物品相似的物品。</li>
<li>基于用户的协同过滤算法：给用户推荐与他兴趣相似的用户喜欢的物品。</li>
<li>基于模型的协同过滤算法：</li>
</ul>
<h3 id="基于物品的协同过滤算法"><a href="#基于物品的协同过滤算法" class="headerlink" title="基于物品的协同过滤算法"></a>基于物品的协同过滤算法</h3><p><strong>基于物品的协同过滤(ItemCF)</strong>的基本思想是预先根据所有用户的历史偏好数据计算物品之间的相似性，然后把与用户喜欢的物品相类似的物品推荐给用户。比如物品<code>a</code>和<code>c</code>非常相似，因为喜欢<code>a</code>的用户同时也喜欢<code>c</code>，而用户<code>A</code>喜欢<code>a</code>，所以把<code>c</code>推荐给用户<code>A</code>。<code>ItemCF</code>算法并不利用物品的内容属性计算物品之间的相似度， 主要通过分析用户的行为记录计算物品之间的相似度， 该算法认为， 物品<code>a</code>和物品<code>c</code>具有很大的相似度是因为喜欢物品<code>a</code>的用户大都喜欢物品<code>c</code>。</p>
<h4 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h4><p>基于物品的协同过滤算法主要分为两步：</p>
<ul>
<li>计算物品之间的相似度；</li>
<li>根据物品的相似度和用户的历史行为给用户生成推荐列表（购买了该商品的用户也经常购买的其他商品）。</li>
</ul>
<h4 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h4><p><strong>购买了该商品的用户也经常购买的其他商品</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202215784.png" alt=""></p>
<p>分母 $\vert N(i) \vert$  是喜欢物品 $i$ 的用户数，而分子 $\vert N(i)\bigcap N(j)\vert$ 是同时喜欢物品 $i$ 和物品 $j$ 的用户数。因此，上述公式可以理解为喜欢物品 $i$ 的用户中有多少比例的用户也喜欢物品 $j$ 。<br>上述公式虽然看起来很有道理，但是却存在一个问题。如果物品 $j$ 很热门，很多人都喜欢，那么 $w_{ij}$ 就会很大，接近 $1$ 。因此，该公式会造成任何物品都会和热门的物品有很大的相似度，这对于致力于挖掘长尾信息的推荐系统来说显然不是一个好的特性。为了避免推荐出热门的物品，可以用下面的公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202218223.png" alt=""></p>
<p>这个公式惩罚了物品 $j$ 的权重，因此减轻了热门物品会和很多物品相似的可能性。</p>
<p><strong>相似度算法改进</strong></p>
<p>在协同过滤中两个物品产生相似度是因为它们共同出现在很多用户的兴趣列表中。也就是说，每个用户的兴趣列表都对物品的相似度产生贡献。那么是不是每个用户的贡献都相同呢?</p>
<p>假设有这么一个用户，他是开书店的，并且买了当当网上<code>80%</code>的书准备用来自己卖。那么他的购物车里包含当当网<code>80%</code>的书。假设当当网有<code>100</code>万本书，也就是说他买了<code>80</code>万本。从前面对<code>ItemCF</code>的讨论可以看到，这意味着因为存在这么一个用户，有<code>80</code>万本书两两之间就产生了相似度，也就是说，内存里即将诞生一个<code>80</code>万乘<code>80</code>万的稠密矩阵。</p>
<p><code>John S. Breese</code>中提出了一个称为<code>IUF(Inverse User Frequence)</code>，即用户活跃度对数的倒数的参数，他也认为活跃用户对物品相似度的贡献应该小于不活跃的用户，他提出应该增加<code>IUF</code>参数来修正物品相似度的计算公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210945142.png" alt=""></p>
<p>上述公式对活跃用户做了一种软性的惩罚，但是对于很多过于活跃的用户，比如上面那位买了当当网<code>80%</code>图书的用户，为了避免相似度矩阵过于稠密，我们在实际计算中一般直接忽略他的兴趣列表，而不将其纳入到相似度计算的数据集中。</p>
<p><strong>相似度矩阵归一化处理</strong></p>
<p><code>Karypis</code>在研究中发现如果将<code>ItemCF</code>的相似度矩阵按最大值归一化，可以提高推荐的准确率。其研究表明，如果已经得到了物品相似度矩阵 $w$ ，那么可以用如下公式得到归一化之后的相似度矩阵 $w^{\prime}$ ：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210948508.png" alt=""></p>
<p>归一化的好处不仅仅在于增加推荐的准确度，它还可以提高推荐的覆盖率和多样性。</p>
<p><strong>实例讲解</strong></p>
<p>下表是一个简易的原始数据集，也称之为<code>User-Item</code>表，即用户-物品列表，记录了每个用户喜爱的物品，数据表格如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>用户</th>
<th>喜爱的物品</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>{a,b,d}</td>
</tr>
<tr>
<td>B</td>
<td>{b,c,e}</td>
</tr>
<tr>
<td>C</td>
<td>{c,d}</td>
</tr>
<tr>
<td>D</td>
<td>{b,c,d}</td>
</tr>
<tr>
<td>E</td>
<td>{a,d}</td>
</tr>
</tbody>
</table>
</div>
<p>接着，我们分别建立用户<code>A</code>-<code>E</code>的共现矩阵，并将他们累加起来。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202254827.png" alt=""></p>
<p>接下来我们计算最终的物品相似度矩阵，以物品<code>a</code>和物品<code>b</code>的相似度计算为例，通过上面计算的计算可知 $C[a][b]=1$ ，即同时喜欢物品<code>a</code>和物品<code>b</code>的用户有一位。根据<code>User-Item</code>表可以统计出 $N(a)=2$ ， $N(b)=3$ ，那么物品<code>a</code>和物品<code>b</code>的相似度 $w_{ab}$ 计算如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210936324.png" alt=""></p>
<h4 id="结果预测"><a href="#结果预测" class="headerlink" title="结果预测"></a>结果预测</h4><p>在得到物品之间的相似度后，<code>ItemCF</code>通过如下公式计算用户 $u$ 对一个物品 $j$ 的兴趣：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210950233.png" alt=""></p>
<p>这里 $N(u)$ 是用户喜欢的物品的集合， $S(j,K)$ 是和物品 $j$ 最相似的 $K$个物品的集合， $w_{ji}$ 是物品 $j$ 和 $i$ 的相似度， $r_{ui}$ 是用户 $u$ 对物品 $i$ 的兴趣。（对于隐反馈数据集，如果用户 $u$ 对物品 $i$ 有过行为，即可令 $r_{ui}=1$ 。）该公式的含义是，和用户历史上感兴趣的物品越相似的物品，越有可能在用户的推荐列表中获得比较高的排名。</p>
<p><strong>实例讲解</strong></p>
<p>下图是一个基于物品推荐的简单例子。该例子中，用户喜欢《C++ Primer中文版》和《编程之美》两本书。然后<code>ItemCF</code>会为这两本书分别找出和它们最相似的 $3$ 本书，然后根据公式的定义计算用户对每本书的感兴趣程度。比如，<code>ItemCF</code>给用户推荐《算法导论》，是因为这本书和《C++Primer中文版》相似，相似度为 $0.4$ ，而且这本书也和《编程之美》相似，相似度是 $0.5$ 。考虑到用户对《C++ Primer中文版》的兴趣度是 $1.3$ ，对《编程之美》的兴趣度是 $0.9$ ，那么用户对《算法导论》的兴趣度就是 $1.3 \times 0.4 + 0.9\times0.5 = 0.97$ 。<br><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304210958573.png" alt=""></p>
<h3 id="基于用户的协同过滤算法"><a href="#基于用户的协同过滤算法" class="headerlink" title="基于用户的协同过滤算法"></a>基于用户的协同过滤算法</h3><p><strong>基于用户的协同过滤(UserCF)</strong>，思想其实比较简单，当一个用户<code>A</code>需要个性化推荐的时候， 我们可以先找到和他有相似兴趣的其他用户， 然后把那些用户喜欢的， 而用户<code>A</code>没有听说过的物品推荐给<code>A</code>。</p>
<h4 id="算法步骤-1"><a href="#算法步骤-1" class="headerlink" title="算法步骤"></a>算法步骤</h4><p>基于用户的协同过滤算法分为两步骤：</p>
<ul>
<li>找到与当前用户<code>A</code>相似的用户<code>B</code>；</li>
<li>将相似用户<code>B</code>喜欢的物品而用户<code>A</code>没有见过的物品推荐给用户<code>A</code>。</li>
</ul>
<h4 id="相似度计算-1"><a href="#相似度计算-1" class="headerlink" title="相似度计算"></a>相似度计算</h4><p><strong>欧式距离</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202054636.png" alt=""></p>
<p><strong>曼哈顿距离</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202055336.png" alt=""></p>
<p><strong>杰卡德(Jaccard)相似系数</strong></p>
<p>两个集合<code>A</code>和<code>B</code>交集元素的个数在<code>A</code>、<code>B</code>并集中所占的比例，称为这两个集合的杰卡德系数，用符号<code>J(A,B)</code>表示。杰卡德相似系数是衡量两个集合相似度的一种指标（余弦距离也可以用来衡量两个集合的相似度），<code>Jaccard</code>值越大说明相似度越高。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202037061.png" alt=""></p>
<p><strong>余弦相似度</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202037832.png" alt=""></p>
<p><strong>皮尔逊相关系数</strong></p>
<p>相比余弦相似度，皮尔逊相关系数通过使用用户平均分对各独立评分进行修正，减小了用户评分偏置的影响。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202048519.png" alt=""></p>
<p>其中 $R_{i,p}$ 表示用户 $i$ 对物品 $p$ 的评分， $\overline{R}_{i}$ 表示用户 $i$ 对所有物品评分的平均值， $P$ 代表所有物品的集合。</p>
<h4 id="结果预测-1"><a href="#结果预测-1" class="headerlink" title="结果预测"></a>结果预测</h4><p>根据上面的几种方法， 我们可以计算出向量之间的相似程度， 也就是可以计算出<code>Alice</code>和其他用户的相近程度， 这时候我们就可以选出与<code>Alice</code>最相近的前 $n$ 个用户， 基于他们对某一物品的评价猜测出<code>Alice</code>的打分值。</p>
<p>这里最常用的方式是利用用户相似度和相似用户的评价的加权平均获得目标用户的评价预测。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202104133.png" alt=""></p>
<p>其中 $w_{u,s}$ 是用户 $u$ 与用户 $s$ 的相似度， $R_{s,p}$ 是用户 $s$ 对物品 $p$ 的评分， $S$ 是所有用户的集合。</p>
<p>还有一种方式打分方式， 这种方式考虑的更加全面， 依然是用户相似度作为权值， 但后面不单纯的是其他用户对物品的评分， 而是该物品的评分与此用户的所有评分的差值进行加权平均， 这时候考虑到了有的用户内心的评分标准不一的情况， 即有的用户喜欢打高分， 有的用户喜欢打低分的情况。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304202135669.png" alt=""></p>
<p>其中， $w_{u,s}$ 是用户 $u$ 与用户 $s$ 的相似度， $R_{s,p}$ 是用户 $s$ 对物品 $p$ 的评分， $\overline{R}_{u}$ 是用户 $u$ 对所有物品打分的平均值， $\overline{R}_{s}$ 是用户 $s$ 对所有物品打分的平均值，其实就是对真实值与平均值的误差进行重要性加权。</p>
<h3 id="基于模型的协同过滤算法"><a href="#基于模型的协同过滤算法" class="headerlink" title="基于模型的协同过滤算法"></a>基于模型的协同过滤算法</h3><p>基于模型的协同过滤作为目前最主流的协同过滤类型，我们的问题是这样的： $m$ 个物品， $n$ 个用户的数据，只有部分用户和部分数据之间是有评分数据的，其它部分评分是空白。此时我们要用已有的部分稀疏数据来预测那些空白的物品和数据之间的评分关系，找到最高评分的物品推荐给用户。</p>
<p>对于这个问题，用机器学习的思想来建模解决，主流的方法可以分为：用关联算法，聚类算法，分类算法，回归算法，矩阵分解，神经网络，图模型以及隐语义模型来解决。</p>
<p><strong>用关联算法做协同过滤</strong><br>一般我们可以找出用户购买的所有物品数据里频繁出现的项集活序列，来做频繁集挖掘，找到满足支持度值的关联物品的频繁 $N$ 项集或者序列。如果用户购买了频繁 $N$ 项集或者序列里的部分物品，那么我们可以将频繁项集或序列里的其他物品按一定的评分准则推荐给用户，这个评分准则可以包括支持度，置信度和提升度等。</p>
<p><strong>用聚类算法做协同过滤</strong></p>
<p>用聚类算法做协同过滤就和前面的基于用户或者项目的协同过滤有些类似了。我们可以按照用户或者按照物品基于一定的距离度量来进行聚类。如果基于用户聚类，则可以将用户按照一定距离度量方式分成不同的目标人群，将同样目标人群评分高的物品推荐给目标用户。基于物品聚类的话，则是将用户评分高物品的相似同类物品推荐给用户。</p>
<p><strong>用分类算法做协同过滤</strong></p>
<p>如果我们根据用户评分的高低，将分数分成几段的话，则这个问题变成分类问题。比如最直接的，设置一份评分阈值，评分高于阈值的就是推荐，评分低于阈值就是不推荐，我们将问题变成了一个二分类问题。虽然分类问题的算法多如牛毛，但是目前使用最广泛的是逻辑回归。为啥是逻辑回归而不是看起来更加高大上的比如支持向量机呢？因为逻辑回归的解释性比较强，每个物品是否推荐我们都有一个明确的概率放在这，同时可以对数据的特征做工程化，得到调优的目的。</p>
<p><strong>用回归算法做协同过滤</strong></p>
<p>用回归算法做协同过滤比分类算法看起来更加的自然。我们的评分可以是一个连续的值而不是离散的值，通过回归模型我们可以得到目标用户对某商品的预测打分。</p>
<p><strong>用矩阵分解做协同过滤</strong></p>
<p>用矩阵分解做协同过滤是目前使用也很广泛的一种方法。由于传统的奇异值分解<code>SVD</code>要求矩阵不能有缺失数据，必须是稠密的，而我们的用户物品评分矩阵是一个很典型的稀疏矩阵，直接使用传统的<code>SVD</code>到协同过滤是比较复杂的。目前主流的矩阵分解推荐算法主要是<code>SVD</code>的一些变种，比如<code>FunkSVD</code>，<code>BiasSVD</code>和<code>SVD++</code>。</p>
<p><strong>用神经网络做协同过滤</strong></p>
<p>用神经网络乃至深度学习做协同过滤应该是以后的一个趋势。目前比较主流的用两层神经网络来做推荐算法的是限制玻尔兹曼机<code>(RBM)</code>。在目前的<code>Netflix</code>算法比赛中，<code>RBM</code>算法的表现很牛。当然如果用深层的神经网络来做协同过滤应该会更好，大厂商用深度学习的方法来做协同过滤应该是将来的一个趋势。</p>
<p><strong>用图模型做协同过滤</strong></p>
<p>用图模型做协同过滤，则将用户之间的相似度放到了一个图模型里面去考虑，常用的算法是<code>SimRank</code>系列算法和马尔科夫模型算法。对于<code>SimRank</code>系列算法，它的基本思想是被相似对象引用的两个对象也具有相似性。算法思想有点类似于大名鼎鼎的<code>PageRank</code>。而马尔科夫模型算法当然是基于马尔科夫链了，它的基本思想是基于传导性来找出普通距离度量算法难以找出的相似性。</p>
<p><strong>用隐语义模型做协同过滤</strong></p>
<p>隐语义模型主要是基于<code>NLP</code>的，涉及到对用户行为的语义分析来做评分推荐，主要方法有隐性语义分析<code>LSA</code>和隐含狄利克雷分布<code>LDA</code>。</p>
<h2 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h2><p>  矩阵分解是指将一个矩阵分解成两个或者多个矩阵的乘积，实际推荐计算时不再使用大矩阵，而是用分解得到的两个小矩阵：一个是由代表用户偏好的用户隐因子向量组成，另一个是由代表物品语义主题的隐因子向量组成。</p>
<p>对于下图的<code>User-Item</code>矩阵（评分矩阵），记为 $R_{n \times m}$ 。可以将其分解成两个或者多个矩阵的乘积，假设分解成两个矩阵 $U_{n \times k}$ 和 $V_{k \times m}$ ，我们要使得矩阵 $U_{n \times k}$ 和 $V_{k \times m}$ 的乘积能够还原原始的矩阵 $R$ ，即 $R_{n \times m}=U_{n \times k} \ast V_{k \times m}$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231131422.png" alt=""></p>
<p>在我们得到用户对每个标的物的评分后，从该评分中过滤掉用户已经操作过的标的物，针对剩下的标的物得分做降序排列取<code>topN</code>推荐给用户。矩阵分解算法的核心思想是将用户行为矩阵分解为两个低秩矩阵的乘积，通过分解，我们分别将用户和标的物嵌入到了同一个 $k$ 维的向量空间（ $k$ 一般很小，几十到上百），用户向量和标的物向量的内积代表了用户对标的物的偏好度。所以，矩阵分解算法本质上也是一种<strong>嵌入方法</strong>。上面提到的k维向量空间的每一个维度是<strong>隐因子(latent factor)</strong>，之所以叫隐因子，是因为每个维度不具备与现实场景对应的具体的可解释的含义，所以矩阵分解算法也是一类隐因子算法。这k个维度代表的是某种行为特性，但是这个行为特性又是无法用具体的特征解释的，从这点也可以看出，矩阵分解算法的可解释性不强，我们比较难以解释矩阵分解算法为什么这么推荐。</p>
<h3 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h3><p>通过矩阵分解将用户 $u$ 和标的物 $v$ 嵌入如下的 $k$ 维隐式特征空间向量：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231137649.png" alt=""></p>
<p>那么用户 $u$ 对标的物 $v$ 的预测评分为 $\hat{r_{uv}}=p_{u} \ast q_{v}^{T}$ ，真实值与预测值之间的误差为 $\Delta r=r_{uv}-\hat{r_{uv}}$ 。如果预测得越准，那么 $| \Delta r |$ 越小，针对所有用户评分过的 $(u,v)$ 对，如果我们可以保证这些误差之和尽量小，那么有理由认为我们的预测是精准的。有了上面的分析，我们就可以将矩阵分解转化为一个机器学习问题。具体地说，我们可以将矩阵分解转化为如下等价的求最小值的最优化问题。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231146383.png" alt=""></p>
<p>其中 $\lambda$ 是超参数， $| p_{u} |^{2}+| q_{v} |^{2}$ 是正则化项。</p>
<h3 id="求解方法"><a href="#求解方法" class="headerlink" title="求解方法"></a>求解方法</h3><p>对于上一节讲到的最优化问题，在工程上一般有两种求解方法，<code>SGD(Stochastic Gradient Descent)</code>和<code>ALS(Alternating Least Squares)</code>。</p>
<h4 id="利用SGD来求解矩阵分解"><a href="#利用SGD来求解矩阵分解" class="headerlink" title="利用SGD来求解矩阵分解"></a>利用SGD来求解矩阵分解</h4><p>我们定义真实评分和预测评分的误差为： $e_{uv}=r_{uv}-p_{u} \ast q_{v}$ ，我们可以将上面的优化问题改写成如下函数：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231152053.png" alt=""></p>
<p>对 $p_u$ 和 $q_{v}$ 求偏导数，我们可以得到：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231153530.png" alt=""></p>
<p>有了偏导数，我们沿着导数(梯度)相反的方向更新：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231154583.png" alt=""></p>
<h4 id="利用ALS来求解矩阵分解"><a href="#利用ALS来求解矩阵分解" class="headerlink" title="利用ALS来求解矩阵分解"></a>利用ALS来求解矩阵分解</h4><p><code>ALS</code>方法是一个高效的求解矩阵分解的算法，目前<code>Spark Mllib</code>中的协同过滤算法就是基于<code>ALS</code>求解的矩阵分解算法，它可以很好地拓展到分布式计算场景，轻松应对大规模训练数据的情况。下面对ALS算法原理及特点做一个简单介绍。<code>ALS</code>算法的原理基本就是名字表达的意思，通过交替优化求得极值。</p>
<p>一般过程是先固定 $p_{u}$ ，那么上述优化问题就变成了一个关于 $q_{v}$ 的二次函数，可以作为最小二乘问题来解决，求出最优的 $q^{\ast}_{v}$ 后，固定 $q^{\ast}_{v}$ ，再解关于 $p_{u}$ 的最小二乘问题，交替进行直到收敛。</p>
<p><code>ALS</code>算法有如下两个优势：</p>
<ul>
<li><p><strong>可以并行处理</strong></p>
<p>当固定某一个参数时，另一个参数的迭代更新只依赖自己，不依赖于其他的标的物的特征向量，所以可以将不同的参数更新放到不同的服务器上执行。<code>Spark</code>的<code>ALS</code>算法就是采用这样的方式做到并行化的。</p>
</li>
<li><p><strong>对于隐式特征问题比较合适</strong></p>
<p>用户真正的评分是很稀少的，所以利用隐式行为是更好的选择（其实也是不得已的选择）。当利用了隐式行为，那么用户行为矩阵就不会那么稀疏了，即有非常多的 $(u,v)$ 对是非空的，计算量会更大，这时采用<code>ALS</code>算法是更合适的，因为固定 $p_{u}$ 或者 $q_{v}$ ，让整个计算问题更加简单，容易求目标函数的极值。</p>
</li>
</ul>
<h3 id="矩阵分解推荐算法的拓展与优化"><a href="#矩阵分解推荐算法的拓展与优化" class="headerlink" title="矩阵分解推荐算法的拓展与优化"></a>矩阵分解推荐算法的拓展与优化</h3><p>矩阵分解算法是一个非常容易理解并易于分布式实现的算法。不光如此，矩阵分解算法的框架还是一个非常容易拓展的框架，可以整合非常多的其他信息及特性到该框架之下，从而丰富模型的表达空间，提升预测的准确度。本节我们就来总结和梳理一下矩阵分解算法可以进行哪些拓展与优化。</p>
<p><strong>整合偏差(bias)项</strong></p>
<p>不同的人对标的物的评价可能是不一样的，有的人倾向于给更高的评分，而有的人倾向于给更低的评分。对于同一个标的物，也会受到外界其他信息的干扰，影响人们对它的评价（比如视频，可能由于主演的热点事件导致该视频突然变火），这两种情况是由于用户和标的物引起的偏差。我们可以在这里引入<code>Bias</code>项，将评分表中观察到的值分解为 $4$ 个部分：全局均值<code>(global average)</code>，标的物偏差<code>(item bias)</code>，用户偏差<code>(user bias)</code>和用户标的物交叉项<code>(user-item interaction)</code>。这时，我们可以用如下公式来预测用户 $u$ 对标的物 $v$ 的评分：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231206104.png" alt=""></p>
<p><strong>增加更多的用户信息输入</strong></p>
<p>由于用户一般只对很少的标的物评分，导致评分过少，可能无法给该用户做出较好的推荐，这时可以通过引入更多的信息来缓解评分过少的问题。具体来说，我们可以整合用户隐式反馈（收藏、点赞、分享等）和用户人口统计学信息（年龄、性别、地域、收入等）到矩阵分解模型中。</p>
<p><strong>整合时间因素</strong></p>
<p>到目前为止，我们的模型都是静态的。实际上，用户的偏好、用户对标的物的评分趋势、以及标的物的受欢迎程度都是随着时间变化的。拿电影来说，用户可能原来喜欢爱情类的电影，后面可能会转而喜欢科幻喜剧类电影，所以我们用包含时间的 $p_{u}(t)$ 来表示用户的偏好特性向量。用户开始对某个视频偏向于打高分，经过一段时间后，用户看的电影多了起来，用户的审美越来越挑剔，所以一般不会再对一个电影打很高的分数了，除非他觉得真的特别好，因此，我们可以用包含时间的 $b_{u}(t)$ 来表示用户的偏差随着时间而变化。对于标的物偏差也一样，一个电影可能开始不是很火，但是如果它的主演后面演了一部非常火的电影，也会将原来的电影热度带到一个新的高度。比如，前两年比较火的李现演的《亲爱的，热爱的》，导致李现人气高涨，他原来演的《南方有乔木》的百度搜索指数在《亲爱的，热爱的》播出期间高涨。因此，我们可以用包含时间的 $b_{v}(t)$ 来表示标的物偏差随着时间的变化而变化的趋势。标的物本身的特征 $q_{v}$ ，我们可以认为是稳定的，它代表的是标的物本身的固有属性或者品质，所以不会随着时间而变化。</p>
<p>基于上面的分析，我们最终的预测用户评分的公式整合时间因素后可以表达为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231212913.png" alt=""></p>
<p><strong>整合用户对评分的置信度</strong></p>
<p>一般来说，用户对不同标的物的评分不是完全一样可信的，可能会受到外界其他因素的影响，比如某个视频播出后，主播发生了热点事件，肯定会影响用户对该视频的评价，节假日，特殊事件也会影响用户的评价。对于隐式反馈，一般我们用 $0$ 和 $1$ 来表示用户是否喜欢该标的物，多少有点绝对，更好的方式是引入一个喜欢的概率/置信度，用户对该标的物操作次数越多、时间越长、付出越大，相应的置信度也越大。因此，我们可以在用户对标的物的评分中增加一个置信度的因子 $c_{uv}$ ，那么最终的优化公式就变为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231213243.png" alt=""></p>
<p><strong>隐式反馈</strong></p>
<p>用二元变量 $p_{uv}$ 表示用户 $u$ 对标的物的偏好， $p_{uv}=1$ 表示用户 $u$ 对标的物 $v$ 有兴趣， $p_{uv}=0$ 表示对标的物 $v$ 无兴趣。 $r_{uv}$ 是用户 $u$ 对标的物的隐式反馈，如观看视频的时长，点击次数等等。 $r_{uv}$ 和 $p_{uv}$ 的关系见下面公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231220341.png" alt=""></p>
<p>$r_{uv}$ 越大，有理由认为用户对标的物兴趣的置信度越高，比如一个文章读者看了好几篇，肯定比看一遍更能反映出读者对这篇文章的喜爱。具体可以用下面的公式来衡量用户 $u$ 对标的物 $v$ 的置信度，其中 $\alpha$ 是一个超参数，作者建议取 $\alpha=40$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231221419.png" alt=""></p>
<p>于是我们可以定义如下公式</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304231225576.png" alt=""></p>
<p>将用户的操作 $r_{uv}$ 分解为置信度 $c_{uv}$ 和偏好 $p_{uv}$ 能够更好地反映隐式行为的特征，并且从实践上可以大幅提升预测的准确度。同时，通过该分解，利用代数上的一些技巧及该模型的巧妙设计，该算法的时间复杂度与用户操作行为总次数线性相关，不依赖于用户数和标的物数，因此非常容易并行化。</p>
<p>隐式反馈也有一些缺点，不像明确的用户评分，无法很好地表达负向反馈，用户购买一个物品可能是作为礼物送给别人的，他自己可能不喜欢这个物品，用户观看了某个视频，有可能是产品进入视频详情页时是自动起播的，这些行为是包含很多噪音的。</p>
<p><strong>整合用户和标的物metadata信息</strong></p>
<p>利用特征的嵌入向量之和来表示用户或者标的物向量，这就很好地将<code>metadata</code>信息整合到了用户和标的物向量中了，再利用用户向量 $p_{u}$  和标的物向量 $q_{i}$ 的内积加上<code>bias</code>项，通过一个<code>logistic</code>函数来获得用户 $u$ 对标的物 $i$ 的偏好概率/得分，从这里的介绍可以看到，该模型很好地将矩阵分解和<code>metadata</code>信息整合到了一个框架之下。</p>
<h2 id="FM模型"><a href="#FM模型" class="headerlink" title="FM模型"></a>FM模型</h2><p><code>FM(Factorization Machine)</code>，即因式分解机。</p>
<h3 id="为什么需要FM模型"><a href="#为什么需要FM模型" class="headerlink" title="为什么需要FM模型"></a>为什么需要FM模型</h3><ol>
<li>特征组合是许多机器学习建模过程中遇到的问题，如果对特征直接建模，很有可能会忽略掉特征与特征之间的关联信息，因此，可以通过构建新的交叉特征这一特征组合方式提高模型的效果。</li>
<li>高维的稀疏矩阵是实际工程中常见的问题，并直接会导致计算量过大，特征权值更新缓慢。试想一个 $10000\times100$ 的表，每一列都有 $8$ 种元素，经过<code>One-Hot</code>独热编码之后，会产生一个 $10000\times800$ 的表。因此表中每行元素只有 $100$ 个值为 $1$ ， $700$ 个值为 $0$ 。</li>
</ol>
<p>而<code>FM</code>的优势就在于对这两方面问题的处理。首先是特征组合，通过对两两特征组合，引入交叉项特征，提高模型得分；其次是高维灾难，通过引入隐向量（对参数矩阵进行矩阵分解），完成对特征的参数估计。</p>
<h3 id="FM模型的应用场景"><a href="#FM模型的应用场景" class="headerlink" title="FM模型的应用场景"></a>FM模型的应用场景</h3><p>我们已经知道了<code>FM</code>可以解决特征组合以及高维稀疏矩阵问题，而实际业务场景中，电商、豆瓣等推荐系统的场景是使用最广的领域，打个比方，小王只在豆瓣上浏览过 $20$ 部电影，而豆瓣上面有 $20000$ 部电影，如果构建一个基于小王的电影矩阵，毫无疑问，里面将有 $199980$ 个元素全为 $0$ 。而类似于这样的问题就可以通过<code>FM</code>来解决。</p>
<h3 id="FM模型的具体形式"><a href="#FM模型的具体形式" class="headerlink" title="FM模型的具体形式"></a>FM模型的具体形式</h3><p>首先我们回顾一下最常见的线性表达式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304191959346.png" alt=""></p>
<p>其中 $w_{0}$ 为初始权值，或者理解为偏置项， $w_{i}$ 为每个特征 $x_{i}$ 对应的权值。可以看到，这种线性表达式只描述了每个特征与输出的关系。</p>
<p><code>FM</code>的表达式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192002037.png" alt=""></p>
<p>可观察到，只是在线性表达式后面加入了新的交叉项特征及对应的权值。这里 $x_{i}$ 和 $x_{j}$ 分别表示两个不同的特征取值，对于 $n$ 维的特征来说，这样的组合应该一共有 $C_{n}^{2}$ 种，也就意味着我们需要同样数量的权重参数。</p>
<h3 id="FM模型的解决方法"><a href="#FM模型的解决方法" class="headerlink" title="FM模型的解决方法"></a>FM模型的解决方法</h3><p><code>FM</code>解决这个问题的方法非常简单，它不再是简单地为交叉之后的特征对设置参数，而是设置了一种计算特征参数的方法。<code>FM</code>模型引入了新的矩阵 $V$，矩阵 $V$ 是一个 $n \times k$ 的二维矩阵。这里的 $k$ 是我们设置的参数，一般不会很大，比如 $16$ 、 $32$ 之类。对于特征每一个维度 $i$ ，我们都可以找到一个 $v_{j}$ ，它表示一个 $1 \times k$ 的向量。于是我们可以用 $v_{i}$ 和 $v_{j}$ 来计算得出上式中的 $w_{ij}$ ，即 $w_{ij}=v_{i}v_{j}^{T}$ 。也就是说我们<strong>用向量的内积来计算得到了就交叉特征的系数</strong>，相比于原先 $O(n^{2})$ 量级的参数而言，我们将参数的量级降低到了 $O(n)$ 。</p>
<p>有了上面的方法，我们就能表示出交叉项，具体过程如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192107149.png" alt=""></p>
<h3 id="FM模型的训练"><a href="#FM模型的训练" class="headerlink" title="FM模型的训练"></a>FM模型的训练</h3><p>经过上述步骤，我们能够得到变形之后的原式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192121730.png" alt=""></p>
<p>首先需要明确的是我们想要优化的参数是 $w_{0}$ ， $w_{i}$ 和 $w_{ij}$ ，所以我们对损失函数求导即可得到：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192201890.png" alt=""></p>
<p>其中 $\sum\limits_{j=1}^{n}v_{j,f}x_{j}$ 和 $i$ 是独立的，所以它是可以提前算好的，这样一来对于所有参数项，我们都可以在 $O(1)$ 的时间内计算出它们的梯度。</p>
<h3 id="FM模型的高维扩展"><a href="#FM模型的高维扩展" class="headerlink" title="FM模型的高维扩展"></a>FM模型的高维扩展</h3><p>我们仿照刚才的公式，可以写出<code>FM</code>模型推广到 $d$ 维的方程：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192209525.png" alt=""></p>
<p>前面两项都很好理解，我们着重来看第三项。第三项当中包含了从 $2$ 维到 $d$ 维交叉特征的情况，我们以 $d=3$ 为例，那么这一项当中应该包含二维的交叉项以及三维的交叉项，应该是这样的：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304192210293.png" alt=""></p>
<p>这个式子<strong>整体上和之前的形式是一样的</strong>，我们不难分析出它的复杂度是 $O(kn^{d})$ 。当 $d=2$ 的时候，我们通过一系列变形将它的复杂度优化到了 $O(kn)$ ，而当 $d&gt;2$ 的时候，没有很好的优化方法，而且三重特征的交叉往往没有意义，并且会过于稀疏，所以我们一般情况下只会使用 $d = 2$ 的情况。</p>
<h2 id="FFM模型"><a href="#FFM模型" class="headerlink" title="FFM模型"></a>FFM模型</h2><p><code>FFM(Field-aware Factorization Machine)</code>其实就是<code>FM</code>的进阶版，<code>FFM</code>将隐向量 $v$ 又进一步细化（引入了<code>field</code>的概念，即将特征所在的不同的<code>field</code>这个信息也考虑进去），其公式如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201650996.png" alt=""></p>
<p>其中， $f_{j}$ 是第 $j$ 个特征所属的<code>field</code>。如果隐向量的长度为 $k$ ，那么<code>FFM</code>的二次参数有 $nfk$ 个，远多于<code>FM</code>模型的 $nk$ 个。此外，由于隐向量与<code>field</code>相关，<code>FFM</code>二次项并不能够化简，其预测复杂度是 $O(kn^{2})$ 。</p>
<h3 id="FFM模型的特征组合方式"><a href="#FFM模型的特征组合方式" class="headerlink" title="FFM模型的特征组合方式"></a>FFM模型的特征组合方式</h3><p>举一个例子进行辅助说明：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>User</th>
<th>Movie</th>
<th>Genre</th>
<th>Price</th>
</tr>
</thead>
<tbody>
<tr>
<td>YuChin</td>
<td>3Idiots</td>
<td>Comedy, Drama</td>
<td>$9.99</td>
</tr>
</tbody>
</table>
</div>
<p>对上面出现的信息进行分类标注，<strong>红字代表所在的field,蓝字代表特征,绿字代表特征的值。</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201703206.png" alt=""></p>
<p><strong>FM的特征组合方式为：</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201704698.png" alt=""></p>
<p><strong>FFM的特征组合方式为：</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202304201707323.png" alt=""></p>
<p>不难看出，<code>FM</code>中有 $w_{1}$ ， $w_{2}$ ， $w_{3}$ ， $w_{4}$ ，$w_{5}$ 五个隐向量，但到了<code>FFM</code>中有 $4\times5=20$ 个，选 $x_{i}$ 与 $x_{j}$ 对应的域 $f_{j}$ 对应的隐向量 $v_{i,f_{j}}$ 进行交叉。</p>
<h3 id="FFM模型的应用场景"><a href="#FFM模型的应用场景" class="headerlink" title="FFM模型的应用场景"></a>FFM模型的应用场景</h3><p>和<code>FM</code>算法一样，<code>FFM</code>主要应用在推荐算法中的<code>CTR</code>点击率预估（排序）问题，推荐系统一般可以分成两个模块，召回和排序。比如对于电影推荐，召回模块会针对用户生成一个推荐电影列表，而排序模块则负责对这个电影列表根据用户的兴趣做排序。当把<code>FFM</code>算法应用到推荐系统中时，具体地是应用在排序模块。</p>
<h2 id="Wide-amp-Deep模型"><a href="#Wide-amp-Deep模型" class="headerlink" title="Wide&amp;Deep模型"></a>Wide&amp;Deep模型</h2>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%80%BB%E7%BB%93/" rel="tag"># 总结</a>
              <a href="/tags/%E9%9D%A2%E8%AF%95/" rel="tag"># 面试</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/11/22/%E7%AE%97%E6%B3%95Tricks/" rel="prev" title="算法Tricks">
                  <i class="fa fa-chevron-left"></i> 算法Tricks
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">木霈玖</span>
</div>

    </div>
  </footer>

  
  <script src="//unpkg.com/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//unpkg.com/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
