<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//unpkg.com/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//unpkg.com/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="前言开新坑啦，强化学习搞起！！！">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习">
<meta property="og:url" content="http://example.com/2023/10/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="木霈玖的博客">
<meta property="og:description" content="前言开新坑啦，强化学习搞起！！！">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081106986.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241533701.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241535831.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241550023.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241551667.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310301135977.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310311129686.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310311130831.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061010578.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061026274.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061027021.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061041371.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061045825.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061045446.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061059726.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061103338.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061115977.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061123723.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061124112.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061151736.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061344602.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061423882.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061426853.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061432650.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061433344.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061441622.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061441784.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061456984.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061459433.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061458839.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061458666.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061546075.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061547490.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061626343.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061637403.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061736922.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310111028623.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310111030228.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310111500195.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241146883.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310111356360.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310111357823.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081137646.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081152577.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081419935.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081424056.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081540233.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310090924546.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081802820.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081803143.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081805118.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081811253.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081812422.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091047168.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091047842.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091048744.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091050987.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091051202.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091051920.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091056285.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091056136.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091057874.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091058155.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091140788.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091405091.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091407116.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091408114.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091436727.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091513535.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091513060.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091514291.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091517811.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091501870.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091503001.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091503352.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091508292.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091633242.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310101539072.png">
<meta property="article:published_time" content="2023-10-08T02:57:25.000Z">
<meta property="article:modified_time" content="2023-11-06T09:53:29.290Z">
<meta property="article:author" content="木霈玖">
<meta property="article:tag" content="总结">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081106986.png">


<link rel="canonical" href="http://example.com/2023/10/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>强化学习 | 木霈玖的博客</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">木霈玖的博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%AA%E8%AE%BA"><span class="nav-number">2.</span> <span class="nav-text">绪论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0"><span class="nav-number">2.1.</span> <span class="nav-text">强化学习概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.1.1.</span> <span class="nav-text">强化学习与监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%89%B9%E5%BE%81"><span class="nav-number">2.1.2.</span> <span class="nav-text">强化学习的特征</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E5%86%B3%E7%AD%96"><span class="nav-number">2.2.</span> <span class="nav-text">序列决策</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%99%BA%E8%83%BD%E4%BD%93%E5%92%8C%E7%8E%AF%E5%A2%83"><span class="nav-number">2.2.1.</span> <span class="nav-text">智能体和环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A5%96%E5%8A%B1"><span class="nav-number">2.2.2.</span> <span class="nav-text">奖励</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E5%86%B3%E7%AD%96-1"><span class="nav-number">2.2.3.</span> <span class="nav-text">序列决策</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E4%BD%9C%E7%A9%BA%E9%97%B4"><span class="nav-number">2.3.</span> <span class="nav-text">动作空间</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%BB%84%E6%88%90%E6%88%90%E5%88%86%E5%92%8C%E7%B1%BB%E5%9E%8B"><span class="nav-number">2.4.</span> <span class="nav-text">强化学习智能体的组成成分和类型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5"><span class="nav-number">2.4.1.</span> <span class="nav-text">策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">2.4.2.</span> <span class="nav-text">价值函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.4.3.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="nav-number">2.4.4.</span> <span class="nav-text">强化学习智能体的类型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%E7%9A%84%E6%99%BA%E8%83%BD%E4%BD%93%E4%B8%8E%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E7%9A%84%E6%99%BA%E8%83%BD%E4%BD%93"><span class="nav-number">2.4.4.1.</span> <span class="nav-text">基于价值的智能体与基于策略的智能体</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%89%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%99%BA%E8%83%BD%E4%BD%93%E4%B8%8E%E5%85%8D%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%99%BA%E8%83%BD%E4%BD%93"><span class="nav-number">2.4.4.2.</span> <span class="nav-text">有模型强化学习智能体与免模型强化学习智能体</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%A7%84%E5%88%92"><span class="nav-number">2.5.</span> <span class="nav-text">学习与规划</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8"><span class="nav-number">2.6.</span> <span class="nav-text">探索和利用</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B"><span class="nav-number">3.</span> <span class="nav-text">马尔可夫决策过程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B"><span class="nav-number">3.1.</span> <span class="nav-text">马尔可夫过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8"><span class="nav-number">3.1.1.</span> <span class="nav-text">马尔可夫性质</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE"><span class="nav-number">3.1.2.</span> <span class="nav-text">马尔可夫链</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%A5%96%E5%8A%B1%E8%BF%87%E7%A8%8B"><span class="nav-number">3.2.</span> <span class="nav-text">马尔可夫奖励过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E6%8A%A5%E4%B8%8E%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">3.2.1.</span> <span class="nav-text">回报与价值函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B"><span class="nav-number">3.2.2.</span> <span class="nav-text">贝尔曼方程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%A8%E6%9C%9F%E6%9C%9B%E5%85%AC%E5%BC%8F"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">全期望公式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B%E6%8E%A8%E5%AF%BC"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">贝尔曼方程推导</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%A5%96%E5%8A%B1%E8%BF%87%E7%A8%8B%E4%BB%B7%E5%80%BC%E7%9A%84%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95"><span class="nav-number">3.2.3.</span> <span class="nav-text">计算马尔可夫奖励过程价值的迭代算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B-1"><span class="nav-number">3.3.</span> <span class="nav-text">马尔可夫决策过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%AD%96%E7%95%A5"><span class="nav-number">3.3.1.</span> <span class="nav-text">马尔可夫决策过程中的策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">3.3.2.</span> <span class="nav-text">马尔可夫决策过程中的价值函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%9F%E6%9C%9B%E6%96%B9%E7%A8%8B"><span class="nav-number">3.3.3.</span> <span class="nav-text">贝尔曼期望方程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%87%E4%BB%BD%E5%9B%BE"><span class="nav-number">3.3.4.</span> <span class="nav-text">备份图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0"><span class="nav-number">3.3.5.</span> <span class="nav-text">策略评估</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E4%B8%8E%E6%8E%A7%E5%88%B6"><span class="nav-number">3.3.6.</span> <span class="nav-text">预测与控制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92"><span class="nav-number">3.3.7.</span> <span class="nav-text">动态规划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0"><span class="nav-number">3.3.8.</span> <span class="nav-text">马尔可夫决策过程中的策略评估</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E6%8E%A7%E5%88%B6"><span class="nav-number">3.3.9.</span> <span class="nav-text">马尔可夫决策过程控制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="nav-number">3.3.10.</span> <span class="nav-text">策略迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E6%96%B9%E7%A8%8B"><span class="nav-number">3.3.10.1.</span> <span class="nav-text">贝尔曼最优方程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-number">3.3.11.</span> <span class="nav-text">价值迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E6%80%A7%E5%8E%9F%E7%90%86"><span class="nav-number">3.3.11.1.</span> <span class="nav-text">最优性原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A1%AE%E8%AE%A4%E6%80%A7%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-number">3.3.11.2.</span> <span class="nav-text">确认性价值迭代</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">3.4.</span> <span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95"><span class="nav-number">3.5.</span> <span class="nav-text">常用算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Q-Learning"><span class="nav-number">3.5.1.</span> <span class="nav-text">Q-Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DQN"><span class="nav-number">3.5.2.</span> <span class="nav-text">DQN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C"><span class="nav-number">3.5.2.1.</span> <span class="nav-text">目标网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8E%A2%E7%B4%A2"><span class="nav-number">3.5.2.2.</span> <span class="nav-text">探索</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE"><span class="nav-number">3.5.2.3.</span> <span class="nav-text">经验回放</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DDQN"><span class="nav-number">3.5.3.</span> <span class="nav-text">DDQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dueling-DQN"><span class="nav-number">3.5.4.</span> <span class="nav-text">Dueling DQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Noisy-DQN"><span class="nav-number">3.5.5.</span> <span class="nav-text">Noisy DQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Prioritized-DQN"><span class="nav-number">3.5.6.</span> <span class="nav-text">Prioritized DQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Distributional-DQN"><span class="nav-number">3.5.7.</span> <span class="nav-text">Distributional DQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Step-Bootstrapping"><span class="nav-number">3.5.8.</span> <span class="nav-text">Multi-Step Bootstrapping</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Rainbow-DQN"><span class="nav-number">3.5.9.</span> <span class="nav-text">Rainbow DQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Policy-Gradient"><span class="nav-number">3.5.10.</span> <span class="nav-text">Policy Gradient</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Actor-Critic"><span class="nav-number">3.5.11.</span> <span class="nav-text">Actor Critic</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A2C"><span class="nav-number">3.5.12.</span> <span class="nav-text">A2C</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A3C"><span class="nav-number">3.5.13.</span> <span class="nav-text">A3C</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">木霈玖</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/10/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="木霈玖">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="木霈玖的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-08 10:57:25" itemprop="dateCreated datePublished" datetime="2023-10-08T10:57:25+08:00">2023-10-08</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2023-11-06 17:53:29" itemprop="dateModified" datetime="2023-11-06T17:53:29+08:00">2023-11-06</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">总结</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>开新坑啦，强化学习搞起！！！</p>
<a id="more"></a>
<h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><h2 id="强化学习概述"><a href="#强化学习概述" class="headerlink" title="强化学习概述"></a>强化学习概述</h2><p>强化学习（Reinforcement Learning，简称RL）是机器学习的一种重要方法，它通过智能体（agent）与环境（environment）的交互来实现自主学习和决策。在强化学习中，智能体会采取一系列的行动，环境会根据智能体的行动给出奖励或惩罚，智能体的目标是最大化累积奖励。强化学习在许多实际应用中都有着广泛的应用，例如自动驾驶、机器人控制、金融交易、游戏等。<strong>智能体的目的就是尽可能多地从环境中获取奖励。</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081106986.png" alt=""></p>
<h3 id="强化学习与监督学习"><a href="#强化学习与监督学习" class="headerlink" title="强化学习与监督学习"></a>强化学习与监督学习</h3><p>监督学习：</p>
<ul>
<li>输入的数据应该是没有关联的（独立同分布）</li>
<li>我们告诉学习器正确的标签是什么</li>
</ul>
<p>强化学习：</p>
<ul>
<li>观测（observation）不是独立同分布的</li>
<li>并没有立刻获得反馈</li>
</ul>
<h3 id="强化学习的特征"><a href="#强化学习的特征" class="headerlink" title="强化学习的特征"></a>强化学习的特征</h3><ul>
<li>强化学习会试错探索，它通过探索环境来获取对环境的理解。</li>
<li>强化学习智能体会从环境里面获得延迟的奖励。</li>
<li>在强化学习的训练过程中，时间非常重要。</li>
<li>智能体的动作会影响它随后得到的数据。</li>
</ul>
<h2 id="序列决策"><a href="#序列决策" class="headerlink" title="序列决策"></a>序列决策</h2><h3 id="智能体和环境"><a href="#智能体和环境" class="headerlink" title="智能体和环境"></a>智能体和环境</h3><p>强化学习研究的问题是智能体与环境交互的问题。智能体把它的动作输出给环境，环境取得这个动作后会进行下一步，把下一步的观测与这个动作带来的奖励返还给智能体。这样的交互会产生很多观测，智能体的目的是从这些观测之中学到能最大化奖励的策略。</p>
<h3 id="奖励"><a href="#奖励" class="headerlink" title="奖励"></a>奖励</h3><p>奖励是由环境给的一种标量的反馈信号（scalar feedback signal），这种信号可显示智能体在某一步采取某个策略的表现如何。强化学习的目的就是最大化智能体可以获得的奖励，智能体在环境里面存在的目的就是最大化它的期望的累积奖励（expected cumulative reward）。</p>
<h3 id="序列决策-1"><a href="#序列决策-1" class="headerlink" title="序列决策"></a>序列决策</h3><p>在一个强化学习环境里面，智能体的目的就是选取一系列的动作来最大化奖励，所以这些选取的动作必须有长期的影响。但在这个过程里面，智能体的奖励其实是被延迟了的，就是我们现在选取的某一步动作，可能要等到很久后才知道这一步到底产生了什么样的影响。</p>
<p><strong>状态</strong>是对世界的完整描述，不会隐藏世界的信息。 <strong>观测</strong>是对状态的部分描述，可能会遗漏一些信息。</p>
<p>当智能体的状态与环境的状态等价的时候，即当智能体能够观察到环境的所有状态时，我们称这个环境是<strong>完全可观测的（fully observed）</strong>。在这种情况下面，强化学习通常被建模成一个<strong>马尔可夫决策过程（Markov decision process，MDP）</strong>的问题。</p>
<p>当智能体只能看到部分的观测，我们就称这个环境是<strong>部分可观测的（partially observed）</strong>。在这种情况下，强化学习通常被建模成<strong>部分可观测马尔可夫决策过程（partially observable Markov decision process, POMDP）</strong>的问题。</p>
<h2 id="动作空间"><a href="#动作空间" class="headerlink" title="动作空间"></a>动作空间</h2><p>在给定的环境中，有效动作的集合经常被称为<strong>动作空间（action space）</strong>。像雅达利游戏和围棋（Go）这样的环境有<strong>离散动作空间（discrete action space）</strong>，在这个动作空间里，智能体的动作数量是有限的。在其他环境，比如在物理世界中控制一个智能体，在这个环境中就有<strong>连续动作空间（continuous action space）</strong>。在连续动作空间中，动作是实值的向量。</p>
<h2 id="强化学习智能体的组成成分和类型"><a href="#强化学习智能体的组成成分和类型" class="headerlink" title="强化学习智能体的组成成分和类型"></a>强化学习智能体的组成成分和类型</h2><ul>
<li>策略（policy）：智能体会用策略来选取下一步的动作。</li>
<li>价值函数（value function）：我们用价值函数来对当前状态进行评估。价值函数用于评估智能体进入某个状态后，可以对后面的奖励带来多大的影响。价值函数值越大，说明智能体进入这个状态越有利。</li>
<li>模型（model）。模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式。</li>
</ul>
<h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><p>策略是智能体的动作模型，它决定了智能体的动作。它其实是一个函数，用于把输入的状态变成动作。策略可分为两种：随机性策略和确定性策略。<br><strong>随机性策略（stochastic policy）</strong>就是 $\pi$ 函数，即 $π(a|s) = p (a_{t} = a|s_{t} = s)$ 。输入一个状态 $s$ ，输出一个概率。这个概率是智能体所有动作的概率，然后对这个概率分布进行采样，可得到智能体将采取的动作。比如可能是有 $0.7$ 的概率往左， $0.3$ 的概率往右，那么通过采样就可以得到智能体将采取的动作。<br><strong>确定性策略（deterministic policy）</strong>就是智能体直接采取最有可能的动作，即 $a^{\ast} = \mathop{\mathrm{argmax}}\limits_{a}{\pi(a | s)}$ 。  </p>
<p>通常情况下，强化学习一般使用随机性策略，随机性策略有很多优点。</p>
<ul>
<li>在学习时可以通过引入一定的随机性来更好地探索环境；</li>
<li>随机性策略的动作具有多样性，这一点在多个智能体博弈时非常重要；</li>
<li>采用确定性策略的智能体总是对同样的状态采取相同的动作，这会导致它的策略很容易被对手预测。</li>
</ul>
<h3 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h3><p>价值函数的值是对未来奖励的预测，我们用它来评估状态的好坏。价值函数里面有一个折扣因子（discount factor），我们希望在尽可能短的时间里面得到尽可能多的奖励。</p>
<p>因此，我们可以把折扣因子放到价值函数的定义里面，价值函数的定义为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241533701.png" alt=""></p>
<p>我们还有一种价值函数： Q 函数。 Q 函数里面包含两个变量：状态和动作。其定义为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241535831.png" alt=""></p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>模型决定了下一步的状态。下一步的状态取决于当前的状态以及当前采取的动作。它由状态转移概率和奖励函数两个部分组成。状态转移概率即：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241550023.png" alt=""></p>
<p>奖励函数是指我们在当前状态采取了某个动作，可以得到多大的奖励，即：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241551667.png" alt=""></p>
<h3 id="强化学习智能体的类型"><a href="#强化学习智能体的类型" class="headerlink" title="强化学习智能体的类型"></a>强化学习智能体的类型</h3><h4 id="基于价值的智能体与基于策略的智能体"><a href="#基于价值的智能体与基于策略的智能体" class="headerlink" title="基于价值的智能体与基于策略的智能体"></a>基于价值的智能体与基于策略的智能体</h4><p><strong>基于价值的智能体（value-based agent）</strong>显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的。<strong>基于策略的智能体（policy-based agent）</strong> 直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。把基于价值的智能体和基于策略的智能体结合起来就有了<strong>演员-评论员智能体（actor-critic agent）</strong>。这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作。</p>
<h4 id="有模型强化学习智能体与免模型强化学习智能体"><a href="#有模型强化学习智能体与免模型强化学习智能体" class="headerlink" title="有模型强化学习智能体与免模型强化学习智能体"></a>有模型强化学习智能体与免模型强化学习智能体</h4><p><strong>有模型（model-based）强化学习智能体</strong>通过学习状态的转移来采取动作。<strong>免模型（model-free）强化学习智能体</strong>没有去直接估计状态的转移，也没有得到环境的具体转移变量，它通过学习价值函数和策略函数进行决策。免模型强化学习智能体的模型里面没有环境转移的模型。</p>
<h2 id="学习与规划"><a href="#学习与规划" class="headerlink" title="学习与规划"></a>学习与规划</h2><p>在规划中，环境是已知的，智能体被告知了整个环境的运作规则的详细信息。</p>
<h2 id="探索和利用"><a href="#探索和利用" class="headerlink" title="探索和利用"></a>探索和利用</h2><p><strong>探索</strong>即我们去探索环境，通过尝试不同的动作来得到最佳的策略（带来最大奖励的策略）。<strong>利用</strong>即我们不去尝试新的动作，而是采取已知的可以带来很大奖励的动作。</p>
<h1 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h1><h2 id="马尔可夫过程"><a href="#马尔可夫过程" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h2><h3 id="马尔可夫性质"><a href="#马尔可夫性质" class="headerlink" title="马尔可夫性质"></a>马尔可夫性质</h3><p>在随机过程中， 马尔可夫性质（Markov property）是指一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310301135977.jpg" alt=""></p>
<h3 id="马尔可夫链"><a href="#马尔可夫链" class="headerlink" title="马尔可夫链"></a>马尔可夫链</h3><p>离散时间的马尔可夫过程也称为马尔可夫链（Markov chain）。  </p>
<h2 id="马尔可夫奖励过程"><a href="#马尔可夫奖励过程" class="headerlink" title="马尔可夫奖励过程"></a>马尔可夫奖励过程</h2><p>马尔可夫奖励过程（Markov reward process, MRP）是马尔可夫链加上奖励函数。</p>
<h3 id="回报与价值函数"><a href="#回报与价值函数" class="headerlink" title="回报与价值函数"></a>回报与价值函数</h3><p>范围（horizon）是指一个回合的长度（每个回合最大的时间步数），它是由有限个步数决定的。</p>
<p>回报（return） 可以定义为奖励的逐步叠加，假设时刻 t 后的奖励序列为 $r_{t+1}, r_{t+2}, r_{t+3}, \cdots ，$ 则回报为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310311129686.png" alt=""></p>
<p>对于马尔可夫奖励过程，状态价值函数被定义成回报的期望：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310311130831.png" alt=""></p>
<h3 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h3><p>贝尔曼方程定义了当前状态与未来状态之间的关系。未来奖励的折扣总和加上即时奖励，就组成了贝尔曼方程。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061010578.png" alt=""></p>
<h4 id="全期望公式"><a href="#全期望公式" class="headerlink" title="全期望公式"></a>全期望公式</h4><p>在推导贝尔曼方程之前，首先需要仿照全期望公式（law of total expectation） 的证明过程来证明：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061026274.png" alt=""></p>
<p>为了表达方便，我们令 $s = s_{t}$ ，$ g^{\prime} = G_{t+1}$ ， $s^{\prime} = s_{t+1}$ ，那么我们可以得到如下推导：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061027021.png" alt=""></p>
<h4 id="贝尔曼方程推导"><a href="#贝尔曼方程推导" class="headerlink" title="贝尔曼方程推导"></a>贝尔曼方程推导</h4><p>贝尔曼方程的推导过程如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061041371.png" alt=""></p>
<p>我们可以把贝尔曼方程写成矩阵的形式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061045825.png" alt=""></p>
<p>当我们把贝尔曼方程写成矩阵形式后，可以直接求解：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061045446.png" alt=""></p>
<h3 id="计算马尔可夫奖励过程价值的迭代算法"><a href="#计算马尔可夫奖励过程价值的迭代算法" class="headerlink" title="计算马尔可夫奖励过程价值的迭代算法"></a>计算马尔可夫奖励过程价值的迭代算法</h3><p>我们可以将迭代的方法应用于状态非常多的马尔可夫奖励过程（large MRP），比如：动态规划的方法，蒙特卡洛的方法（通过采样的办法计算它），时序差分学习（temporal-difference learning， TD learning）的方法（时序差分学习是动态规划和蒙特卡洛方法的一个结合）。</p>
<p>蒙特卡洛方法就是当得到一个马尔可夫奖励过程后，我们可以从某个状态开始，把小船放到状态转移矩阵里面，让它“随波逐流”，这样就会产生一个轨迹。产生一个轨迹之后，就会得到一个奖励，那么直接把折扣的奖励即回报 $g$ 算出来。算出来之后将它积累起来，得到回报 $G_{t}$。当积累了一定数量的轨迹之后，我们直接用 $G_{t}$ 除以轨迹数量，就会得到某个状态的价值。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061059726.png" alt=""></p>
<p>动态规划方法就是一直迭代贝尔曼方程，直到价值函数收敛，我们就可以得到某个状态的价值。我们通过自举（bootstrapping）的方法不停地迭代贝尔曼方程，当最后更新的状态与我们上一个状态的区别并不大的时候，更新就可以停止，我们就可以输出最新的 $V^{\prime}(s)$ 作为它当前的状态的价值。这里就是把贝尔曼方程变成一个贝尔曼更新（Bellman update），这样就可以得到状态的价值。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061103338.png" alt=""></p>
<h2 id="马尔可夫决策过程-1"><a href="#马尔可夫决策过程-1" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2><p>相对于马尔可夫奖励过程，马尔可夫决策过程多了决策（决策是指动作），其他的定义与马尔可夫奖励过程的是类似的。此外，状态转移也多了一个条件，变成了 $p (s_{t+1}=s^{\prime}|s_{t}=s, a_{t}=a)$。未来的状态不仅依赖于当前的状态，也依赖于在当前状态智能体采取的动作。</p>
<h3 id="马尔可夫决策过程中的策略"><a href="#马尔可夫决策过程中的策略" class="headerlink" title="马尔可夫决策过程中的策略"></a>马尔可夫决策过程中的策略</h3><p>策略定义了在某一个状态应该采取什么样的动作。知道当前状态后，我们可以把当前状态代入策略函数来得到一个概率，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061115977.png" alt=""></p>
<p>已知马尔可夫决策过程和策略 $\pi$，我们可以把马尔可夫决策过程转换成马尔可夫奖励过程。在马尔可夫决策过程里面，状态转移函数 $P(s^{\prime}|s, a)$ 基于它当前的状态以及它当前的动作。因为我们现在已知策略函数，也就是已知在每一个状态下，可能采取的动作的概率，所以我们就可以直接把动作进行加和，去掉 $a$，这样我们就可以得到对于马尔可夫奖励过程的转移，这里就没有动作，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061123723.png" alt=""></p>
<p>对于奖励函数，我们也可以把动作去掉，这样就会得到类似于马尔可夫奖励过程的奖励函数，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061124112.png" alt=""></p>
<h3 id="马尔可夫决策过程中的价值函数"><a href="#马尔可夫决策过程中的价值函数" class="headerlink" title="马尔可夫决策过程中的价值函数"></a>马尔可夫决策过程中的价值函数</h3><p>马尔可夫决策过程中的价值函数的定义如下，其中，期望基于我们采取的策略。当策略决定后，我们通过对策略进行采样来得到一个期望，计算出它的价值函数。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061151736.png" alt=""></p>
<p>这里另外引入了一个 $Q$ 函数。 $Q$ 函数也被称为动作价值函数（action-value function）。 $Q$ 函数定义的是在某一个状态采取某一个动作，它有可能得到的回报的一个期望，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061344602.png" alt=""></p>
<p>状态价值函数可以理解为在状态s的情况下，未采取动作之前期望的回报值，即所有可能动作的奖励之和。那么我们可以得到如下转化关系公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061423882.png" alt=""></p>
<p>动作价值函数可以理解为在状态 $s$ 下，采取动作 $a$ 之后，转变到下一个状态 $s^{\prime}$ 产生的奖励与下一个状态的期望奖励 $v_{π}(s^{\prime})$ 之和。那么我们可以得到如下转化关系公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061426853.png" alt=""></p>
<h3 id="贝尔曼期望方程"><a href="#贝尔曼期望方程" class="headerlink" title="贝尔曼期望方程"></a>贝尔曼期望方程</h3><p>我们可以把状态价值函数和 $Q$ 函数拆解成两个部分：即时奖励和后续状态的折扣价值（discounted value of successor state）。通过对状态价值函数进行分解，我们就可以得到一个类似于之前马尔可夫奖励过程的贝尔曼方程——贝尔曼期望方程（Bellman expectation equation）：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061432650.png" alt=""></p>
<p>对于 $Q$ 函数，我们也可以做类似的分解，得到 $Q$ 函数的贝尔曼期望方程：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061433344.png" alt=""></p>
<p>同时根据上一节得到的状态价值函数和动作价值函数的表达式，我们可以得到贝尔曼期望方程的另一种形式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061441622.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061441784.png" alt=""></p>
<h3 id="备份图"><a href="#备份图" class="headerlink" title="备份图"></a>备份图</h3><p>备份图用来定义未来下一时刻的状态价值函数（动作价值函数）与上一时刻的状态价值函数（动作价值函数）之间的关联。</p>
<p>状态价值函数 $V_{\pi}$ 的备份图和计算分解：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061456984.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061459433.png" alt=""></p>
<p>  动作价值函数 $Q_{\pi}$ 的备份图和计算分解：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061458839.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061458666.png" alt=""></p>
<h3 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h3><p>已知马尔可夫决策过程以及要采取的策略 $\pi$ ，计算价值函数 $V_{\pi}(s)$ 的过程就是策略评估。</p>
<h3 id="预测与控制"><a href="#预测与控制" class="headerlink" title="预测与控制"></a>预测与控制</h3><p>预测（prediction）和控制（control）是马尔可夫决策过程里面的核心问题。<br>预测（评估一个给定的策略）的输入是马尔可夫决策过程 $&lt; S, A, P, R, \gamma &gt;$ 和策略 $\pi$，输出是价值函数 $V_{\pi}$。预测是指给定一个马尔可夫决策过程以及一个策略 $\pi$ ，计算它的价值函数，也就是计算每个状态的价值。<br>控制（搜索最佳策略）的输入是马尔可夫决策过程 $&lt; S, A, P, R, \gamma &gt;$，输出是最佳价值函数（optimalv alue function） $V^{\ast}$ 和最佳策略（optimal policy） $\pi^{\ast}$。控制就是我们去寻找一个最佳的策略，然后同时输出它的最佳价值函数以及最佳策略。</p>
<h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>动态规划（dynamic programming，DP）适合解决满足最优子结构（optimal substructure）和重叠子问题（overlapping subproblem）两个性质的问题。</p>
<h3 id="马尔可夫决策过程中的策略评估"><a href="#马尔可夫决策过程中的策略评估" class="headerlink" title="马尔可夫决策过程中的策略评估"></a>马尔可夫决策过程中的策略评估</h3><p>  策略评估就是给定马尔可夫决策过程和策略，评估我们可以获得多少价值，即对于当前策略，我们可以得到多大的价值。</p>
<h3 id="马尔可夫决策过程控制"><a href="#马尔可夫决策过程控制" class="headerlink" title="马尔可夫决策过程控制"></a>马尔可夫决策过程控制</h3><p>策略评估是指给定马尔可夫决策过程和策略，我们可以估算出价值函数的值。如果我们只有马尔可夫决策过程，那么应该如何寻找最佳的策略，从而得到最佳价值函数（optimal value function）呢？  </p>
<p>最佳价值函数的定义为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061546075.png" alt=""></p>
<p>最佳价值函数是指，我们搜索一种策略 $\pi$ 让每个状态的价值最大。 $V^{\ast}$ 就是到达每一个状态，它的值的最大化情况。在这种最大化情况中，我们得到的策略就是最佳策略，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061547490.png" alt=""></p>
<p>最佳策略使得每个状态的价值函数都取得最大值。我们可以通过策略迭代和价值迭代来解决马尔可夫决策过程的控制问题。</p>
<h3 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h3><p>历程：迭代执行 $\pi \rightarrow V_{\pi} \rightarrow Q_{\pi} \rightarrow \pi^{\prime}$ ，最终得到 $V^{\ast}$</p>
<p>策略迭代由两个步骤组成：策略评估和策略改进。第一个步骤是策略评估，当前我们在优化策略 $\pi$，在优化过程中得到一个最新的策略。我们先保证这个策略不变，然后估计它的价值，即给定当前的策略函数来估计状态价值函数。第二个步骤是策略改进，得到状态价值函数后，我们可以进一步推算出它的 $Q$ 函数。得到 $Q$ 函数后，我们直接对 $Q$ 函数进行最大化，通过在 $Q$ 函数做一个贪心的搜索来进一步改进策略。这两个步骤一直在迭代进行。</p>
<h4 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h4><p>当我们一直采取 $\arg \max$ 操作的时候，我们会得到一个单调的递增。通过采取这种贪心操作，我们就会得到更好的或者不变的策略，而不会使价值函数变差。所以当改进停止后，我们就会得到一个最佳策略。当改进停止后，我们取让 $Q$ 函数值最大化的动作， $Q$ 函数就会直接变成价值函数，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061626343.png" alt=""></p>
<p>我们也就可以得到贝尔曼最优方程（Bellman optimality equation）。贝尔曼最优方程表明：最佳策略下的一个状态的价值必须等于在这个状态下采取最好动作得到的回报的期望。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061637403.png" alt=""></p>
<h3 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h3><p>历程：迭代执行 $Q_{\pi} \rightarrow V_{\pi}$ ，最终得到 $\pi^{\ast}$</p>
<h4 id="最优性原理"><a href="#最优性原理" class="headerlink" title="最优性原理"></a>最优性原理</h4><p>最优性原理定理（principle of optimality theorem）：一个策略 $\pi(a|s)$ 在状态 $s$ 达到了最优价值，也就是 $V_{\pi}(s) = V^{\ast}(s)$ 成立，当且仅当对于任何能够从 $s$ 到达的 $s^{\prime}$ ，都已经达到了最优价值。也就是对于所有的 $s^{\prime}$，$ V_{\pi}(s^{\prime}) = V^{\ast}(s^{\prime})$ 恒成立。</p>
<h4 id="确认性价值迭代"><a href="#确认性价值迭代" class="headerlink" title="确认性价值迭代"></a>确认性价值迭代</h4><p>如果我们知道子问题 $V^{\ast}(s^{\prime})$ 的最优解，就可以通过价值迭代来得到最优的 $V^{\ast}(s)$ 的解。价值迭代就是把贝尔曼最优方程当成一个更新规则来进行，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061736922.png" alt=""></p>
<p>上式只有当整个马尔可夫决策过程已经达到最佳的状态时才满足。我们可以把它转换成一个备份的等式。备份的等式就是一个迭代的等式。我们不停地迭代贝尔曼最优方程，价值函数就能逐渐趋向于最佳的价值函数，这是价值迭代算法的精髓。</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul>
<li><p>智能体（Agent）：在强化学习中，智能体是一个自主决策的实体，它可以观察环境、采取行动、获得奖励。</p>
</li>
<li><p>环境（Environment）：环境是智能体所处的外部世界，它会根据智能体的行动给出奖励或惩罚，并提供新的状态信息。</p>
</li>
<li><p>状态（State）：状态是描述环境的信息，它是智能体决策的依据。</p>
</li>
<li><p>行动（Action）：行动是智能体在某个状态下可以采取的操作。</p>
</li>
<li><p>奖励（Reward）：奖励是环境根据智能体的行动给出的反馈，它可以是正数（奖励）或负数（惩罚）。</p>
</li>
<li><p>策略（Policy）：策略是智能体决策的规则，它是一个从状态到行动的映射函数。</p>
</li>
<li><p>Off-policy：行动策略和目标策略不是同一个策略</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310111028623.png" alt=""></p>
</li>
<li><p>On-policy：行动策略和目标策略是同一个策略</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310111030228.png" alt=""></p>
</li>
<li><p>状态价值函数（动作1概率（策略$\pi(a|s)$） × 动作1的动作价值函数 + 动作2概率 × 动作2的动作价值函数 + 动作i概率 × 动作i的动作价值函数）和动作价值函数（状态转移概率1（$p(s^{\prime},r|s,a)$）×（reward1+s1的状态价值函数）+状态转移概率2×（reward2+s2的状态价值函数）+状态转移概率i×（rewardi+si的状态价值函数））：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310111500195.png" alt=""></p>
<p>递归写法为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241146883.png" alt=""></p>
</li>
<li><p>贝尔曼方程：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310111356360.png" alt=""></p>
<p>推导过程如下（之所以$G_{t+1}$可以直接变成$S_{t+1}$是因为：等式左边的$v$表示的是具体某个状态下的状态值函数，等式右边的$v$是随机变量的值函数，因为在MDP中，状态转移是不确定的，是按转移概率进行的，因此下个状态是随机变量，所以要对下个状态求期望$E[v(S_{t+1})]$）：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310111357823.png" alt=""></p>
</li>
</ul>
<h2 id="常用算法"><a href="#常用算法" class="headerlink" title="常用算法"></a>常用算法</h2><h3 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h3><p>Q-Learning算法的核心思想是通过学习一个Q值函数（Q-value Function）来估计在某个状态下采取某个行动的长期回报。Q值函数记作：Q(s, a)，其中s表示状态，a表示行动。</p>
<p>具体更新公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081137646.png" alt=""></p>
<p>算法的伪代码：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081152577.png" alt=""></p>
<p>在每个时刻，智能体（Agent）根据当前状态选择一个行动，并根据环境的反馈获得奖励和下一个状态，然后根据上述公式更新Q值函数。智能体的行动选择可以采用贪婪策略（Greedy Policy）或 $\epsilon$-贪婪策略（$\epsilon$-Greedy Policy）。</p>
<h3 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h3><p>DQN 是 Deep Q Network的缩写。顾名思义，DQN是Q-Learning的升级版，通常使用深度神经网络来代替经典Q-Learning中的Q表。在经典Q-Learning中，Q表的构建和使用是算法的核心步骤，获取特定状态对应的动作价值需要查Q表，但是当环境过于复杂时，难以用Q表来进行高效的描述和查询，这便需要借助深度神经网络强大的映射能力来构造一个Q函数，这个Q函数的输入通常是某种状态，而输出是所有可能动作所对应的价值。DQN 除了具有更强的学习和映射能力，还引入了一些有趣的技巧，例如 Experience replay 和 Fixed Q-targets，这些技巧加强了DQN对长跨度动作的洞察，使得DQN具有更好的全局视野，以至于在一些游戏（如CartPole）上DQN能够轻而易举地战胜人类。</p>
<p>算法框架图：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081419935.png" alt=""></p>
<h4 id="目标网络"><a href="#目标网络" class="headerlink" title="目标网络"></a>目标网络</h4><p>目标网络引入的目的：为了避免在更新Q函数时，一边获取Q值一边更新Q函数带来的不稳定问题，这种只针对一个网络的Q函数，实时更新，会导致轨迹很乱，不好训练。所以，引入一个目标网络，从目标网络获取Q值，然后进行Q函数的更新，一段时间后，使用新的Q函数更新目标网络。</p>
<p><strong>在强化学习中，自举是指用后继的估算值，来更新现在状态的估算值。</strong>在target当中，$r_{t}$是实际观测得到的值，$Q^{\pi}(s_{t+1},\pi(s_{t+1}))$是根据Q函数在$s_{t+1}$时做出的估算值，因此target有部分是来自Q函数的估算，而我们用target来更新Q函数本身，所以这属于自举。计算target的时候，我们是最大化Q值的，即$Q^{\pi}(s_{t+1},\pi(s_{t+1}))=\max_{a}Q^{\pi}(s_{t+1},a)$，这里会引起<strong>高估</strong>的问题，利用<strong>目标网络</strong>可以一定程度避免自举，减缓高估问题。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081424056.png" alt=""></p>
<h4 id="探索"><a href="#探索" class="headerlink" title="探索"></a>探索</h4><p>引入探索的目的：避免在进行action的选择时，只选择同一动作，而无法变换。解决探索-利用窘境（exploration-exploitation dilemma）问题。解决方法：$\epsilon$-贪心和玻尔兹曼探索。</p>
<p>$\epsilon$-贪心：我们有 $1-\epsilon$ 的概率会按照 Q 函数来决定动作。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081540233.png" alt=""></p>
<p>玻尔兹曼探索：<strong>就是计算概率分布，然后根据概率来选择动作。</strong>这个方法就比较像是策略梯度。在策略梯度里面，网络的输出是一个期望的动作空间上面的一个的概率分布，再根据概率分布去做采样。你也可以根据Q值去定一个概率分布，假设某一个动作的Q值越大，代表它越好，我们采取这个动作的概率就越高。但是某一个动作的Q值小，不代表我们不能尝试。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310090924546.png" alt=""></p>
<h4 id="经验回放"><a href="#经验回放" class="headerlink" title="经验回放"></a>经验回放</h4><p>引入经验回放的目的：用来实现off-policy性质的训练性能提升，经验回放会构建一个<strong>回放缓冲区（replay buffer）</strong>，回放缓冲区又被称为<strong>回放内存（replay memory）</strong>。回放缓冲区是说现在会有某一个策略 $\pi$ 去跟环境做互动，它会去收集数据。</p>
<p>第一个好处，其实在做强化学习的时候，往往最花时间的步骤是在跟环境做互动，训练网络反而是比较快的。因为我们用 GPU 训练其实很快，真正花时间的往往是在跟环境做互动。用回放缓冲区<strong>可以减少跟环境做互动的次数</strong>，因为在做训练的时候，经验不需要通通来自于某一个策略。一些过去的策略所得到的经验可以放在数据缓冲区里面被使用很多次，被反复的再利用，这样让采样到经验的利用是比较高效的。</p>
<p>第二个好处，在训练网络的时候，其实我们希望<strong>一个批量里面的数据越多样（diverse）越好</strong>。如果批量里面的数据都是同样性质的，我们训练下去是容易坏掉的。如果批量里面都是一样的数据，训练的时候，性能会比较差。我们希望批量的数据越多样越好。如果数据缓冲区里面的经验通通来自于不同的策略，我们采样到的一个批量里面的数据会是比较多样的。</p>
<h3 id="DDQN"><a href="#DDQN" class="headerlink" title="DDQN"></a>DDQN</h3><p>引入目标网络可以一定程度减缓高估问题，但是还是有最大化操作，高估问题还是很严重，而Double DQN可以更好地缓解高估问题（但也没有彻底根除高估问题）。</p>
<p>Double DQN做的改进其实很简单：我们用原始网络$Q(s,a;w)$选出使Q值最大化的那个动作，记为$a^{\ast}$，再用目标网络使用这个$a^{\ast}$计算目标值，得到$y_{t}=r_{t}+\gamma\cdot Q(s_{t+1}, a^{\ast}; w^{-})$。由于$Q(s_{t+1}, a^{\ast}; w^{-}) \leq \max_{a}Q(s_{t+1}, a; w^{-})$，所以进一步减缓了最大化带来的高估问题。</p>
<h3 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h3><p>对于某一个确定的$\pi$，我们对这个策略进行策略评估，可以得到动作价值函数（状态s下采取行动a预计累计回报的期望值）和状态价值函数（状态s下预计累计回报的期望值）：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081802820.png" alt=""></p>
<p>当然也可以写成Bellman方程的形式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081803143.png" alt=""></p>
<p>基于上面的两个值函数，我们可以定义用于评估策略的优势函数：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081805118.png" alt=""></p>
<p>从上面的定义中很容易得到：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081811253.png" alt=""></p>
<p>于是我们可以得到：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081812422.png" alt=""></p>
<p>接下来我们可以定义最优值函数：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091047168.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091047842.png" alt=""></p>
<p>那么同样的，有最优的优势函数：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091048744.png" alt=""></p>
<p>我们依旧可以通过 $Q^{\ast}(s,a)$ 得到 $V^{\ast}(s)$ ：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091050987.png" alt=""></p>
<p>于是有：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091051202.png" alt=""></p>
<p>其中：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091051920.png" alt=""></p>
<p>我们对于<strong>用于评估的优势函数</strong>和<strong>用于控制的优势函数</strong>各得到的一个式子：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091056285.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091056136.png" alt=""></p>
<p>那么，针对策略评估的情形，我们将Q值的表达式改成：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091057874.png" alt=""></p>
<p>而针对最优控制情形，我们将Q值的表达式改成：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091058155.png" alt=""></p>
<p>这样就<strong>倒逼</strong> $V(s;\theta,\beta)$ 必须精确的学出Value，从而保证了 $A(s,a^{\prime};\theta,\alpha)$ 也是精确的。实验表明，用均值版本的表达式可以在两种情况都适用，因此通常只是用均值形式的。</p>
<h3 id="Noisy-DQN"><a href="#Noisy-DQN" class="headerlink" title="Noisy DQN"></a>Noisy DQN</h3><p>Noisy DQN（Noisy Deep Q-Network）提出了一种新的有效的增加网络探索性的策略，这与之前大多数DQN系列通常采用的$\varepsilon$-greedy策略有所不同。Noisy DQN在模型训练过程中，会在网络中添加随机噪声以增强最终结果的不确定性，从而达到和$\varepsilon$-greedy策略随机选择动作所类似的效果。Noisy DQN取得了比传统DQN更优越的性能。在强化学习中，探索是指代理尝试新动作以发现潜在的更好策略，而利用是指代理根据目前已知的最佳策略来采取动作。传统的DQN使用$\varepsilon$-greedy策略来平衡探索与利用，但这会受到值的影响，需要经过调整来达到最佳效果。Noisy DQN通过参数噪声的引入，以一种更自适应的方式实现了探索与利用的平衡。<br>有两种引入噪声的方式。一个是独立高斯噪声，每一个权重和偏执都有自己的噪声。一个是分解高斯噪声，以神经元为单位适用同一个噪声。Noisy DQN采用的是第二种分解高斯噪声。权重和偏置的均值从$U[-\frac{1}{\sqrt{p}},+\frac{1}{\sqrt{p}}]$中进行采样，$p$是输入的维度表。而标准差$\frac{\sigma_{0}}{p}$用进行填充式赋值，分子是提前设置的超参数，一般用0.5。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091140788.png" alt=""></p>
<h3 id="Prioritized-DQN"><a href="#Prioritized-DQN" class="headerlink" title="Prioritized DQN"></a>Prioritized DQN</h3><p>Prioritized DQN（Prioritized Deep Q-Network）是基于DQN（Deep Q-Network）的改进版本，旨在提高学习效率和性能，特别是在处理经验回放时。它引入了一种称为”优先级采样”（Prioritized Experience Replay）的机制，该机制允许算法有选择性地重放经验，更关注那些对训练有重要影响的经验。<br>传统的DQN使用均匀随机采样来选择经验回放中的样本，这意味着所有经验都有相同的机会被选择。然而，某些经验可能对学习更有价值，而其他经验则可能对学习不太重要。Prioritized DQN引入了一个优先级值，用于衡量每个经验的重要性。经验的优先级通常与其导致的TD误差（Temporal Difference Error）有关，TD误差越大，优先级越高。这样，训练时会更多地关注那些有助于降低误差的经验。<br>具体来说，以TD error为criterion，样本的priority要正比于TD loss，再根据normalized priority得到每个样本被采样的概率。每个样本被采样的概率可以表示为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091405091.png" alt=""></p>
<p>其中，$p_{i}$是样本$i$的priority，$\alpha$决定优先级化的程度，当$\alpha=0$时，退化成均匀采样。通常可以通过两种方式求得，在proportional prioritization中，我们直接根据TD error的绝对值决定概率：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091407116.png" alt=""></p>
<p>在rank-based prioritization中，我们根据rank来决定概率，其中$rank(i)$就是第个样本在全体样本中排在多少位，按照对应TD error的绝对值由大到小排列：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091408114.png" alt=""></p>
<h3 id="Distributional-DQN"><a href="#Distributional-DQN" class="headerlink" title="Distributional DQN"></a>Distributional DQN</h3><p>在DQN中，网络输出的都是状态-动作价值Q的期望预估值。这个期望值其实忽略很多信息。因为不同的的分布可以有相同的均值，假设我们只用值的期望来代表整个奖励，可能会丢失一些信息，无法对奖励的分布进行建模。<br>所以从理论上来说，从分布视角(distributional perspective)来建模深度强化学习模型，可以获得更多有用的信息，从而得到更好、更稳定的结果。Distributional DQN选择直方图来表示对于价值分布的估计，并将价值限定在$[V_{min},V_{max}]$之间。在$[V_{min},V_{max}]$选择$N$个等距的价值采样点。网络的输出便是这$N$个价值采样点的概率。<br>还是延续DQN中的双网络结构，我们会得到估计的价值分布和目标的价值分布（目标价值分布需要进行裁剪和投影），并使用交叉熵损失函数来计算两个分布之间的差距，并通过梯度下降法进行参数的更新。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091436727.png" alt=""></p>
<h3 id="Multi-Step-Bootstrapping"><a href="#Multi-Step-Bootstrapping" class="headerlink" title="Multi-Step Bootstrapping"></a>Multi-Step Bootstrapping</h3><p>多步自举就是介于MC和TD的一种方法，可以说MC和TD分别是MB方法的两种极端。TD方法的限制在于，单步必须更新，更新的频率是不能控制的，MB方法就解决了这个问题。自举在有重要，可辨识的状态变更的一段时间里，有最好的效果。（bootstrapping works best if it is over a length of time in which a significant and recognizable state change has occurred.）</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091513535.png" alt=""></p>
<p>迭代规则如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091513060.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091514291.png" alt=""></p>
<p>我们采用multi-step方法，对于N步的情况，loss公式变为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091517811.png" alt=""></p>
<h3 id="Rainbow-DQN"><a href="#Rainbow-DQN" class="headerlink" title="Rainbow DQN"></a>Rainbow DQN</h3><p>Rainbow DQN是一种改进的深度强化学习算法，它旨在结合多种强化学习的改进技术，以提高在各种任务上的性能。Rainbow DQN的名称来源于它的多种改进技术（Double DQN、Prioritized experience replay、Dueling network architecture、Multi-step bootstrap、Distributional DQN、Noisy DQN），就像”七彩虹”一样，每个颜色代表一个不同的改进方法。Rainbow DQN的具体改进如下：</p>
<ul>
<li><p>首先把TD的一步自举换成$n$步自举，目标分布变成了：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091501870.png" alt=""></p>
<p>损失函数变成如下的形式，式子中的$\Phi_{z}$表示投影：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091503001.png" alt=""></p>
</li>
<li><p>Prioritized experience replay排序的顺序也需要调整，之前依据TD-error排序，现在不再使用Q值函数，因此TD-error应该进化为KL散度：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091503352.png" alt=""></p>
</li>
<li><p>Dueling DQN的结构和Distributional DQN的结构需要结合起来，因为这两种方法都对网络结构作出了调整。</p>
<ul>
<li><p>我们从Dueling DQN的结构出发，原先是输出一个状态值函数$V(s)$和一组动作的优势函数$A(s,a)$ ，现在我们需要输出分布了。</p>
</li>
<li><p>别忘了C51用的是固定的atoms位置，可变的atoms概率表示分布。我们设卷积层的输出是$\phi$，value stream参数是$\eta$，advantage stream的参数是$\psi$，设atoms的个数为$N$。那么value stream $v_{\eta}$的输出应该是一个$N$维的向量，而advantage stream $a_{\psi}$的输出应该是一个$N\times|\mathcal{A}|$的矩阵，最后每个atoms对应的概率可以表示为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091508292.png" alt=""></p>
</li>
</ul>
</li>
<li><p>最后把所有全连接层的参数换成带有噪声的参数。</p>
</li>
</ul>
<h3 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h3><p>策略梯度（Policy Gradient）是一种强化学习方法，用于直接学习和优化策略（policy），而不是通过间接学习值函数（value function）来实现。策略表示了在给定状态下选择动作的概率分布，通过调整策略的参数，策略梯度方法旨在最大化累积奖励。<br>策略梯度的核心思想是通过计算奖励的梯度来更新策略的参数，使得策略的期望回报增加。这通常涉及到最大化一个被称为期望回报的目标函数，该目标函数表示在策略下采取一系列动作的预期累积奖励。为了最大化这个目标函数，我们使用梯度上升方法来更新策略参数。具体来说，策略梯度方法通常采用的步骤为：</p>
<ul>
<li>选择一个策略表示，通常是一个神经网络，其输出是在给定状态下采取每个动作的概率；</li>
<li>在环境中与策略交互，收集一系列轨迹（trajectories），每个轨迹包含状态、选择的动作和获得的奖励；</li>
<li>计算每个动作的梯度，以确定哪些动作对于提高期望回报是有利的。这通常涉及到计算目标函数对于策略参数的梯度，以及将奖励信号乘以梯度来更新策略；</li>
<li>使用梯度上升方法（如随机梯度上升或Adam等优化算法）来更新策略参数，以使期望回报最大化。</li>
</ul>
<p>策略梯度方法的优势在于它们可以处理离散和连续动作空间，以及高度非线性的策略。它们也适用于具有随机性的环境和部分可观测状态（如循环神经网络用于处理部分可观测问题）。</p>
<h3 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor Critic"></a>Actor Critic</h3><p>Actor-Critic算法结合了策略梯度（Actor）和值函数估计（Critic）的思想，旨在同时学习一个策略和一个值函数来实现更稳定和高效的强化学习。这种方法的主要优点是能够在学习中充分利用策略和值函数之间的互补信息。<br>其中，Actor输出每个Action的概率，有多少个Action就有多少个输出。Critic基于Actor输出的行为评判得分, Actor再根据Critic的评分修改选行为的概率。这是两个神经网络。因此策略梯度可以写成：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310091633242.png" alt=""></p>
<p>公式里的$\pi$指的就是Actor的策略，$\psi$指的就是Critic，评估Actor策略的好坏，它可以表示成很多种形式：</p>
<ul>
<li>$\sum_{t=0}^{\infty}r_{t}$：一个轨迹中的Reward相加</li>
<li>$\sum_{t=t^{\prime}}^{\infty}r_{t^{\prime}}$：动作后的Reward（从该动作往后算，可以看成前期的对后面没有影响）</li>
<li>$\sum_{t=t^{\prime}}^{\infty}r_{t^{\prime}}-b(s_{t})$：动作后的总汇报相加后的Reward减去一个baseline</li>
<li>$Q^{\pi}(s_{t},a_{t})$：用行为价值函数Q计算</li>
<li>$A^{\pi}(s_{t},a_{t})$：用优势函数A计算</li>
<li>$r_{t}+\gamma V^{\pi}(s_{t+1})-V^{\pi}(s_{t+1})$：用TD-error，计算新的状态价值减去原本的状态价值</li>
</ul>
<p>前三个都是直接应用轨迹的回报累计回报，这样计算出来的策略梯度不会存在偏差，但是因为需要累计多步的回报，所以方差会很大。后三个是利用动作价值函数，优势函数和TD偏差代替累计回报，其优点是方差小，但是三种方法中都用到了逼近方法，因此计算出来的策略梯度都存在偏差。这三种方法是以牺牲偏差来换取小的方差。</p>
<h3 id="A2C"><a href="#A2C" class="headerlink" title="A2C"></a>A2C</h3><p>A2C即Advantage Actor-Critic此算法的出现是为了应对AC算法高方差的问题，一个动作轨迹中假如所有的动作回报都为正并不代表所有动作都是好的，它可能只是个次优的动作。因此我们采用引入基线的方式来解决这个问题,基线函数的特点是能在不改变策略梯度的同时降低其方差。这里的基线baseline一般由$V(s_{t})$来代替，原来的Q值就变成：$Q=Q(s_{t},a_{t})-V(s_{t})=r_{t}+\gamma V(s_{t+1})-V(s_{t})$</p>
<h3 id="A3C"><a href="#A3C" class="headerlink" title="A3C"></a>A3C</h3><p>A3C（Asynchronous Advantage Actor-Critic）算法结合了Actor-Critic方法和异步训练的思想，旨在实现更高效的强化学习。A3C的主要贡献是引入了异步训练，使多个智能体可以并行地与环境交互，从而显著加速了训练过程。<br>A3C算法与AC算法相比，主要在如下方面进行了优化：</p>
<ul>
<li><p>异步训练框架：在A3C算法中，global network是公共的神经网络模型，包含了actor网络和critic网络两部分的功能；下面有n个worker线程，每个线程里有和公共的神经网络一样的网络结构，每个线程会独立的和环境进行交互得到经验数据，这些线程之间互不干扰，独立运行。n个worker线程和公共神经网络的更新情况：每个线程和环境交互得到一定的数据之后，就计算在自己线程里的神经网络损失函数的梯度，但是这些梯度并不更新自己线程里的神经网络，而是去更新公共的神经网络。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310101539072.png" alt=""></p>
</li>
<li><p>网络结构的优化：在AC算法中，使用了两个不同的网络即actor和critic；在A3C算法中，把两个神经网络放到了一起，即输入状态为s，输出状态为价值v和对应的策略$\pi$。</p>
</li>
<li><p>critic评估点的优化：在AC算法中，使用$Q(s,a)$了作为critic的评估点；在A3C算法中，采样更近一步，使用了步采样，以加速收敛，此时A3C算法中的优势函数为$A(s_{0},a)=Q(s_{0},a)-V(s_{0}=r_{0}+\gamma r_{1}+\cdots+\gamma^{n-1}r_{n-1}+\gamma^{n}V(s_{n})-V(s_{0})$。对于Actor和Critic的损失函数部分，和Actor-Critic基本相同。</p>
</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%80%BB%E7%BB%93/" rel="tag"># 总结</a>
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># 强化学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/08/23/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" rel="prev" title="推荐系统">
                  <i class="fa fa-chevron-left"></i> 推荐系统
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">木霈玖</span>
</div>

    </div>
  </footer>

  
  <script src="//unpkg.com/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//unpkg.com/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
