<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//unpkg.com/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//unpkg.com/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="前言开新坑啦，强化学习搞起。">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习">
<meta property="og:url" content="http://example.com/2023/10/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="木霈玖的博客">
<meta property="og:description" content="前言开新坑啦，强化学习搞起。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081106986.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241533701.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241535831.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404181459777.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404181458188.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241550023.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241551667.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310301135977.jpg">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310311129686.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310311130831.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061010578.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061026274.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061027021.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061041371.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411152225752.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061045825.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061045446.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061059726.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061103338.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061115977.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061123723.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061124112.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061151736.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061344602.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061423882.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061426853.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061432650.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061433344.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061441622.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061441784.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061456984.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061459433.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061458839.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061458666.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061546075.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061547490.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061626343.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061637403.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061736922.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081352612.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081353064.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081441872.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081442053.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081444676.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081449238.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081500026.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081501924.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081501433.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081538081.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081632154.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311091108609.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311091120042.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311131348504.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311131355382.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404171543327.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311131452578.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311131454910.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311140944102.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311140945562.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311140955539.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311140958839.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141002272.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141014067.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141020560.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141027355.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141110318.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141113629.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141116781.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141126407.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141127305.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404182146893.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141151029.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141336604.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141345612.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141350439.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141356104.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141652913.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312050900748.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312050901310.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312050910515.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051018942.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051024229.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051048897.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051121289.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051133117.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051324772.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051324074.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051328636.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051332093.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411162249796.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411162250464.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051352706.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051358467.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051417680.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051429446.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051437351.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101521630.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101530257.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101539738.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101543872.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101547561.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101552662.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101601594.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101604354.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101611233.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101617996.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101625556.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101626466.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101628249.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101630416.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101638394.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101640614.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101649780.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101649026.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404182238004.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112113051.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112155776.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112156592.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112225437.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112228101.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112238182.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112240086.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112243886.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112244924.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112250310.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404182204930.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161337398.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161356657.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161356414.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161405257.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161414768.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161417520.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161420949.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161438521.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161451592.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161456145.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281429710.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281431999.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281436975.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281438171.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281441117.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281457194.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281524563.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281526816.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281532119.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281535491.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281853834.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281859845.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281904680.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281921911.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281949961.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401282031786.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401282042062.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401282045795.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191153648.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292223186.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292224383.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292230007.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292235376.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292237297.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292239973.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292242571.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292307106.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292308216.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292312441.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292317432.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292319887.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292321160.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292324578.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292326710.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292326672.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202505080048169.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202505080103535.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202505080051698.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202505080109956.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202505080109806.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202505080110232.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191203225.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191203263.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191204290.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191205005.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191219908.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191222752.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191235939.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191237313.png">
<meta property="article:published_time" content="2023-10-08T02:57:25.000Z">
<meta property="article:modified_time" content="2025-06-05T12:52:55.492Z">
<meta property="article:author" content="木霈玖">
<meta property="article:tag" content="总结">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081106986.png">


<link rel="canonical" href="http://example.com/2023/10/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>强化学习 | 木霈玖的博客</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">木霈玖的博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%AA%E8%AE%BA"><span class="nav-number">2.</span> <span class="nav-text">绪论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0"><span class="nav-number">2.1.</span> <span class="nav-text">强化学习概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.1.1.</span> <span class="nav-text">强化学习与监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%89%B9%E5%BE%81"><span class="nav-number">2.1.2.</span> <span class="nav-text">强化学习的特征</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E5%86%B3%E7%AD%96"><span class="nav-number">2.2.</span> <span class="nav-text">序列决策</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%99%BA%E8%83%BD%E4%BD%93%E5%92%8C%E7%8E%AF%E5%A2%83"><span class="nav-number">2.2.1.</span> <span class="nav-text">智能体和环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A5%96%E5%8A%B1"><span class="nav-number">2.2.2.</span> <span class="nav-text">奖励</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E5%86%B3%E7%AD%96-1"><span class="nav-number">2.2.3.</span> <span class="nav-text">序列决策</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E4%BD%9C%E7%A9%BA%E9%97%B4"><span class="nav-number">2.3.</span> <span class="nav-text">动作空间</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%BB%84%E6%88%90%E6%88%90%E5%88%86%E5%92%8C%E7%B1%BB%E5%9E%8B"><span class="nav-number">2.4.</span> <span class="nav-text">强化学习智能体的组成成分和类型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5"><span class="nav-number">2.4.1.</span> <span class="nav-text">策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">2.4.2.</span> <span class="nav-text">价值函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.4.3.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="nav-number">2.4.4.</span> <span class="nav-text">强化学习智能体的类型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%E7%9A%84%E6%99%BA%E8%83%BD%E4%BD%93%E4%B8%8E%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E7%9A%84%E6%99%BA%E8%83%BD%E4%BD%93"><span class="nav-number">2.4.4.1.</span> <span class="nav-text">基于价值的智能体与基于策略的智能体</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%89%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%99%BA%E8%83%BD%E4%BD%93%E4%B8%8E%E5%85%8D%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%99%BA%E8%83%BD%E4%BD%93"><span class="nav-number">2.4.4.2.</span> <span class="nav-text">有模型强化学习智能体与免模型强化学习智能体</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%A7%84%E5%88%92"><span class="nav-number">2.5.</span> <span class="nav-text">学习与规划</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8"><span class="nav-number">2.6.</span> <span class="nav-text">探索和利用</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B"><span class="nav-number">3.</span> <span class="nav-text">马尔可夫决策过程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B"><span class="nav-number">3.1.</span> <span class="nav-text">马尔可夫过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8"><span class="nav-number">3.1.1.</span> <span class="nav-text">马尔可夫性质</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE"><span class="nav-number">3.1.2.</span> <span class="nav-text">马尔可夫链</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%A5%96%E5%8A%B1%E8%BF%87%E7%A8%8B"><span class="nav-number">3.2.</span> <span class="nav-text">马尔可夫奖励过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E6%8A%A5%E4%B8%8E%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">3.2.1.</span> <span class="nav-text">回报与价值函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B"><span class="nav-number">3.2.2.</span> <span class="nav-text">贝尔曼方程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%A8%E6%9C%9F%E6%9C%9B%E5%85%AC%E5%BC%8F"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">全期望公式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B%E6%8E%A8%E5%AF%BC"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">贝尔曼方程推导</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%A5%96%E5%8A%B1%E8%BF%87%E7%A8%8B%E4%BB%B7%E5%80%BC%E7%9A%84%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95"><span class="nav-number">3.2.3.</span> <span class="nav-text">计算马尔可夫奖励过程价值的迭代算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B-1"><span class="nav-number">3.3.</span> <span class="nav-text">马尔可夫决策过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%AD%96%E7%95%A5"><span class="nav-number">3.3.1.</span> <span class="nav-text">马尔可夫决策过程中的策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">3.3.2.</span> <span class="nav-text">马尔可夫决策过程中的价值函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%9F%E6%9C%9B%E6%96%B9%E7%A8%8B"><span class="nav-number">3.3.3.</span> <span class="nav-text">贝尔曼期望方程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%87%E4%BB%BD%E5%9B%BE"><span class="nav-number">3.3.4.</span> <span class="nav-text">备份图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0"><span class="nav-number">3.3.5.</span> <span class="nav-text">策略评估</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E4%B8%8E%E6%8E%A7%E5%88%B6"><span class="nav-number">3.3.6.</span> <span class="nav-text">预测与控制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92"><span class="nav-number">3.3.7.</span> <span class="nav-text">动态规划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0"><span class="nav-number">3.3.8.</span> <span class="nav-text">马尔可夫决策过程中的策略评估</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E6%8E%A7%E5%88%B6"><span class="nav-number">3.3.9.</span> <span class="nav-text">马尔可夫决策过程控制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="nav-number">3.3.10.</span> <span class="nav-text">策略迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E6%96%B9%E7%A8%8B"><span class="nav-number">3.3.10.1.</span> <span class="nav-text">贝尔曼最优方程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-number">3.3.11.</span> <span class="nav-text">价值迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E6%80%A7%E5%8E%9F%E7%90%86"><span class="nav-number">3.3.11.1.</span> <span class="nav-text">最优性原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A1%AE%E8%AE%A4%E6%80%A7%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-number">3.3.11.2.</span> <span class="nav-text">确认性价值迭代</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A1%A8%E6%A0%BC%E5%9E%8B%E6%96%B9%E6%B3%95"><span class="nav-number">4.</span> <span class="nav-text">表格型方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B-2"><span class="nav-number">4.1.</span> <span class="nav-text">马尔可夫决策过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%89%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.1.1.</span> <span class="nav-text">有模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%8D%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.1.2.</span> <span class="nav-text">免模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Q-%E8%A1%A8%E6%A0%BC"><span class="nav-number">4.2.</span> <span class="nav-text">Q 表格</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%8D%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B"><span class="nav-number">4.3.</span> <span class="nav-text">免模型预测</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0"><span class="nav-number">4.3.1.</span> <span class="nav-text">蒙特卡洛策略评估</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86"><span class="nav-number">4.3.2.</span> <span class="nav-text">时序差分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95%E3%80%81%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%96%B9%E6%B3%95%E7%9A%84%E8%87%AA%E4%B8%BE%E5%92%8C%E9%87%87%E6%A0%B7"><span class="nav-number">4.3.3.</span> <span class="nav-text">动态规划方法、蒙特卡洛方法以及时序差分方法的自举和采样</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6"><span class="nav-number">4.4.</span> <span class="nav-text">免模型控制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Sarsa%EF%BC%9A%E5%90%8C%E7%AD%96%E7%95%A5%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%8E%A7%E5%88%B6"><span class="nav-number">4.4.1.</span> <span class="nav-text">Sarsa：同策略时序差分控制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Sarsa-lambda"><span class="nav-number">4.4.1.1.</span> <span class="nav-text">Sarsa($\lambda$)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%82%E7%AD%96%E7%95%A5%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%8E%A7%E5%88%B6"><span class="nav-number">4.4.2.</span> <span class="nav-text">Q学习：异策略时序差分控制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8C%E7%AD%96%E7%95%A5%E4%B8%8E%E5%BC%82%E7%AD%96%E7%95%A5%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">4.4.3.</span> <span class="nav-text">同策略与异策略的区别</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-number">5.</span> <span class="nav-text">策略梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="nav-number">5.1.</span> <span class="nav-text">策略梯度算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%AE%9E%E7%8E%B0%E6%8A%80%E5%B7%A7"><span class="nav-number">5.2.</span> <span class="nav-text">策略梯度实现技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8A%80%E5%B7%A7-1%EF%BC%9A%E6%B7%BB%E5%8A%A0%E5%9F%BA%E7%BA%BF"><span class="nav-number">5.2.1.</span> <span class="nav-text">技巧 1：添加基线</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8A%80%E5%B7%A72%EF%BC%9A%E5%88%86%E9%85%8D%E5%90%88%E9%80%82%E7%9A%84%E5%88%86%E6%95%B0"><span class="nav-number">5.2.2.</span> <span class="nav-text">技巧2：分配合适的分数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#REINFORCE%EF%BC%9A%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-number">5.3.</span> <span class="nav-text">REINFORCE：蒙特卡洛策略梯度</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%91%E7%AB%AF%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96"><span class="nav-number">6.</span> <span class="nav-text">近端策略优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7"><span class="nav-number">6.1.</span> <span class="nav-text">重要性采样</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%91%E7%AB%AF%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96-1"><span class="nav-number">6.2.</span> <span class="nav-text">近端策略优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%91%E7%AB%AF%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%E6%83%A9%E7%BD%9A"><span class="nav-number">6.2.1.</span> <span class="nav-text">近端策略优化惩罚</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%91%E7%AB%AF%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%E8%A3%81%E5%89%AA"><span class="nav-number">6.2.2.</span> <span class="nav-text">近端策略优化裁剪</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6Q%E2%80%8B%E7%BD%91%E7%BB%9C"><span class="nav-number">7.</span> <span class="nav-text">深度Q​网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">7.1.</span> <span class="nav-text">状态价值函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">7.2.</span> <span class="nav-text">动作价值函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C"><span class="nav-number">7.3.</span> <span class="nav-text">目标网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A2%E7%B4%A2"><span class="nav-number">7.4.</span> <span class="nav-text">探索</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE"><span class="nav-number">7.5.</span> <span class="nav-text">经验回放</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9C"><span class="nav-number">7.6.</span> <span class="nav-text">深度Q网络</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9C%E8%BF%9B%E9%98%B6%E6%8A%80%E5%B7%A7"><span class="nav-number">8.</span> <span class="nav-text">深度Q网络进阶技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8C%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9C"><span class="nav-number">8.1.</span> <span class="nav-text">双深度Q网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AB%9E%E4%BA%89%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9C"><span class="nav-number">8.2.</span> <span class="nav-text">竞争深度Q网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%85%88%E7%BA%A7%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE"><span class="nav-number">8.3.</span> <span class="nav-text">优先级经验回放</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9C%A8%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95%E5%92%8C%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%96%B9%E6%B3%95%E4%B8%AD%E5%8F%96%E5%BE%97%E5%B9%B3%E8%A1%A1"><span class="nav-number">8.4.</span> <span class="nav-text">在蒙特卡洛方法和时序差分方法中取得平衡</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%99%AA%E5%A3%B0%E7%BD%91%E7%BB%9C"><span class="nav-number">8.5.</span> <span class="nav-text">噪声网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8FQ%E5%87%BD%E6%95%B0"><span class="nav-number">8.6.</span> <span class="nav-text">分布式Q函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BD%A9%E8%99%B9"><span class="nav-number">8.7.</span> <span class="nav-text">彩虹</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%92%88%E5%AF%B9%E8%BF%9E%E7%BB%AD%E5%8A%A8%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9C"><span class="nav-number">9.</span> <span class="nav-text">针对连续动作的深度Q网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%A1%88-1%EF%BC%9A%E5%AF%B9%E5%8A%A8%E4%BD%9C%E8%BF%9B%E8%A1%8C%E9%87%87%E6%A0%B7"><span class="nav-number">9.1.</span> <span class="nav-text">方案 1：对动作进行采样</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%A1%88-2%EF%BC%9A%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87"><span class="nav-number">9.2.</span> <span class="nav-text">方案 2：梯度上升</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%A1%88-3%EF%BC%9A%E8%AE%BE%E8%AE%A1%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="nav-number">9.3.</span> <span class="nav-text">方案 3：设计网络架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%A1%88-4%EF%BC%9A%E4%B8%8D%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6-Q-%E7%BD%91%E7%BB%9C"><span class="nav-number">9.4.</span> <span class="nav-text">方案 4：不使用深度 Q 网络</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%BC%94%E5%91%98-%E8%AF%84%E8%AE%BA%E5%91%98%E7%AE%97%E6%B3%95"><span class="nav-number">10.</span> <span class="nav-text">演员-评论员算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%9B%9E%E9%A1%BE"><span class="nav-number">10.1.</span> <span class="nav-text">策略梯度回顾</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9C%E5%9B%9E%E9%A1%BE"><span class="nav-number">10.2.</span> <span class="nav-text">深度Q网络回顾</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8A%BF%E6%BC%94%E5%91%98-%E8%AF%84%E8%AE%BA%E5%91%98%E7%AE%97%E6%B3%95"><span class="nav-number">10.3.</span> <span class="nav-text">优势演员-评论员算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#REINFORCE%E5%92%8CA2C%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">10.3.1.</span> <span class="nav-text">REINFORCE和A2C的区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%82%E6%AD%A5%E4%BC%98%E5%8A%BF%E6%BC%94%E5%91%98-%E8%AF%84%E8%AE%BA%E5%91%98%E7%AE%97%E6%B3%95"><span class="nav-number">10.4.</span> <span class="nav-text">异步优势演员-评论员算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B7%AF%E5%BE%84%E8%A1%8D%E7%94%9F%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-number">10.5.</span> <span class="nav-text">路径衍生策略梯度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8E%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E8%81%94%E7%B3%BB"><span class="nav-number">10.6.</span> <span class="nav-text">与生成对抗网络的联系</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A8%80%E7%96%8F%E5%A5%96%E5%8A%B1"><span class="nav-number">11.</span> <span class="nav-text">稀疏奖励</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E5%A5%96%E5%8A%B1"><span class="nav-number">11.1.</span> <span class="nav-text">设计奖励</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A5%BD%E5%A5%87%E5%BF%83"><span class="nav-number">11.2.</span> <span class="nav-text">好奇心</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0"><span class="nav-number">11.3.</span> <span class="nav-text">课程学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E5%B1%82%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">11.4.</span> <span class="nav-text">分层强化学习</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0"><span class="nav-number">12.</span> <span class="nav-text">模仿学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A1%8C%E4%B8%BA%E5%85%8B%E9%9A%86"><span class="nav-number">12.1.</span> <span class="nav-text">行为克隆</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%86%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">12.2.</span> <span class="nav-text">逆强化学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E4%BA%BA%E7%A7%B0%E8%A7%86%E8%A7%92%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0"><span class="nav-number">12.3.</span> <span class="nav-text">第三人称视角模仿学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E5%92%8C%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA"><span class="nav-number">12.4.</span> <span class="nav-text">序列生成和聊天机器人</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">12.5.</span> <span class="nav-text">模仿学习和离线强化学习的区别</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-number">13.</span> <span class="nav-text">深度确定性策略梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A6%BB%E6%95%A3%E5%8A%A8%E4%BD%9C%E4%B8%8E%E8%BF%9E%E7%BB%AD%E5%8A%A8%E4%BD%9C%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">13.1.</span> <span class="nav-text">离散动作与连续动作的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-1"><span class="nav-number">13.2.</span> <span class="nav-text">深度确定性策略梯度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8C%E5%BB%B6%E8%BF%9F%E6%B7%B1%E5%BA%A6%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-number">13.3.</span> <span class="nav-text">双延迟深度确定性策略梯度</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TRPO%E7%AE%97%E6%B3%95"><span class="nav-number">14.</span> <span class="nav-text">TRPO算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">14.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E7%9B%AE%E6%A0%87"><span class="nav-number">14.2.</span> <span class="nav-text">策略目标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%91%E4%BC%BC%E6%B1%82%E8%A7%A3"><span class="nav-number">14.3.</span> <span class="nav-text">近似求解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6"><span class="nav-number">14.4.</span> <span class="nav-text">共轭梯度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%90%9C%E7%B4%A2"><span class="nav-number">14.5.</span> <span class="nav-text">线性搜索</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%BF%E4%B9%89%E4%BC%98%E5%8A%BF%E4%BC%B0%E8%AE%A1"><span class="nav-number">14.6.</span> <span class="nav-text">广义优势估计</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SAC%E7%AE%97%E6%B3%95"><span class="nav-number">15.</span> <span class="nav-text">SAC算法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">16.</span> <span class="nav-text">多智能体强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BE%E5%AE%9A"><span class="nav-number">16.1.</span> <span class="nav-text">设定</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%AF%E8%AF%AD"><span class="nav-number">16.2.</span> <span class="nav-text">术语</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9E%B6%E6%9E%84"><span class="nav-number">16.3.</span> <span class="nav-text">架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Fully-Decentralized-Training"><span class="nav-number">16.3.1.</span> <span class="nav-text">Fully Decentralized Training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Centralized-Training"><span class="nav-number">16.3.2.</span> <span class="nav-text">Centralized Training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Centralized-Training-with-Decentralized-Execution"><span class="nav-number">16.3.3.</span> <span class="nav-text">Centralized Training with Decentralized Execution</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">木霈玖</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">26</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/10/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="木霈玖">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="木霈玖的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-08 10:57:25" itemprop="dateCreated datePublished" datetime="2023-10-08T10:57:25+08:00">2023-10-08</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2025-06-05 20:52:55" itemprop="dateModified" datetime="2025-06-05T20:52:55+08:00">2025-06-05</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>42k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>38 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>开新坑啦，强化学习搞起。</p>
<a id="more"></a>
<h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><h2 id="强化学习概述"><a href="#强化学习概述" class="headerlink" title="强化学习概述"></a>强化学习概述</h2><p>强化学习<code>(Reinforcement Learning, RL)</code>是机器学习的一种重要方法，它通过智能体<code>(agent)</code>与环境<code>(environment)</code>的交互来实现自主学习和决策。在强化学习中，智能体会采取一系列的行动，环境会根据智能体的行动给出奖励或惩罚，智能体的目标是最大化累积奖励。强化学习在许多实际应用中都有着广泛的应用，例如自动驾驶、机器人控制、金融交易、游戏等。<strong>智能体的目的就是尽可能多地从环境中获取奖励。</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310081106986.png" alt=""></p>
<h3 id="强化学习与监督学习"><a href="#强化学习与监督学习" class="headerlink" title="强化学习与监督学习"></a>强化学习与监督学习</h3><p>监督学习：</p>
<ul>
<li>输入的数据应该是没有关联的（独立同分布）</li>
<li>我们告诉学习器正确的标签是什么</li>
</ul>
<p>强化学习：</p>
<ul>
<li>观测<code>(observation)</code>不是独立同分布的</li>
<li>并没有立刻获得反馈</li>
</ul>
<h3 id="强化学习的特征"><a href="#强化学习的特征" class="headerlink" title="强化学习的特征"></a>强化学习的特征</h3><ul>
<li>强化学习会试错探索，它通过探索环境来获取对环境的理解。</li>
<li>强化学习智能体会从环境里面获得延迟的奖励。</li>
<li>在强化学习的训练过程中，时间非常重要。</li>
<li>智能体的动作会影响它随后得到的数据。</li>
</ul>
<h2 id="序列决策"><a href="#序列决策" class="headerlink" title="序列决策"></a>序列决策</h2><h3 id="智能体和环境"><a href="#智能体和环境" class="headerlink" title="智能体和环境"></a>智能体和环境</h3><p>强化学习研究的问题是智能体与环境交互的问题。智能体把它的动作输出给环境，环境取得这个动作后会进行下一步，把下一步的观测与这个动作带来的奖励返还给智能体。这样的交互会产生很多观测，智能体的目的是从这些观测之中学到能最大化奖励的策略。</p>
<h3 id="奖励"><a href="#奖励" class="headerlink" title="奖励"></a>奖励</h3><p>奖励是由环境给的一种标量的反馈信号<code>(scalar feedback signal)</code>，这种信号可显示智能体在某一步采取某个策略的表现如何。强化学习的目的就是最大化智能体可以获得的奖励，智能体在环境里面存在的目的就是最大化它的期望的累积奖励<code>(expected cumulative reward)</code>。</p>
<h3 id="序列决策-1"><a href="#序列决策-1" class="headerlink" title="序列决策"></a>序列决策</h3><p>在一个强化学习环境里面，智能体的目的就是选取一系列的动作来最大化奖励，所以这些选取的动作必须有长期的影响。但在这个过程里面，智能体的奖励其实是被延迟了的，就是我们现在选取的某一步动作，可能要等到很久后才知道这一步到底产生了什么样的影响。</p>
<p><strong>状态</strong>是对世界的完整描述，不会隐藏世界的信息。 <strong>观测</strong>是对状态的部分描述，可能会遗漏一些信息。</p>
<p>当智能体的状态与环境的状态等价的时候，即当智能体能够观察到环境的所有状态时，我们称这个环境是<strong>完全可观测的<code>(fully observed)</code></strong>。在这种情况下面，强化学习通常被建模成一个<strong>马尔可夫决策过程<code>(Markov decision process, MDP)</code></strong>的问题。</p>
<p>当智能体只能看到部分的观测，我们就称这个环境是<strong>部分可观测的<code>(partially observed)</code></strong>。在这种情况下，强化学习通常被建模成<strong>部分可观测马尔可夫决策过程<code>(partially observable Markov decision process, POMDP)</code></strong>的问题。</p>
<h2 id="动作空间"><a href="#动作空间" class="headerlink" title="动作空间"></a>动作空间</h2><p>在给定的环境中，有效动作的集合经常被称为<strong>动作空间<code>(action space)</code></strong>。像雅达利游戏和围棋这样的环境有<strong>离散动作空间<code>(discrete action space)</code></strong>，在这个动作空间里，智能体的动作数量是有限的。在其他环境，比如在物理世界中控制一个智能体，在这个环境中就有<strong>连续动作空间<code>(continuous action space)</code></strong>。在连续动作空间中，动作是实值的向量。</p>
<h2 id="强化学习智能体的组成成分和类型"><a href="#强化学习智能体的组成成分和类型" class="headerlink" title="强化学习智能体的组成成分和类型"></a>强化学习智能体的组成成分和类型</h2><ul>
<li>策略<code>(policy)</code>：智能体会用策略来选取下一步的动作。</li>
<li>价值函数<code>(value function)</code>：我们用价值函数来对当前状态进行评估。价值函数用于评估智能体进入某个状态后，可以对后面的奖励带来多大的影响。价值函数值越大，说明智能体进入这个状态越有利。</li>
<li>模型<code>(model)</code>：模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式。</li>
</ul>
<h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><p>策略是智能体的动作模型，它决定了智能体的动作。它其实是一个函数，用于把输入的状态变成动作。策略可分为两种：随机性策略和确定性策略。<br><strong>随机性策略<code>(stochastic policy)</code></strong>就是 $\pi$ 函数，即 $\pi(a|s) = p (a_{t} = a|s_{t} = s)$ 。输入一个状态 $s$ ，输出一个概率。这个概率是智能体所有动作的概率，然后对这个概率分布进行采样，可得到智能体将采取的动作。比如可能是有 $0.7$ 的概率往左， $0.3$ 的概率往右，那么通过采样就可以得到智能体将采取的动作。<br><strong>确定性策略<code>(deterministic policy)</code></strong>就是智能体直接采取最有可能的动作，即 $a^{\ast} = \mathop{\mathrm{argmax}}\limits_{a}{\pi(a | s)}$ 。  </p>
<p>通常情况下，强化学习一般使用随机性策略，随机性策略有很多优点。</p>
<ul>
<li>在学习时可以通过引入一定的随机性来更好地探索环境；</li>
<li>随机性策略的动作具有多样性，这一点在多个智能体博弈时非常重要；</li>
<li>采用确定性策略的智能体总是对同样的状态采取相同的动作，这会导致它的策略很容易被对手预测。</li>
</ul>
<h3 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h3><p>价值函数的值是对未来奖励的预测，我们用它来评估状态的好坏。价值函数里面有一个折扣因子<code>(discount factor)</code>，我们希望在尽可能短的时间里面得到尽可能多的奖励。</p>
<p>因此，我们可以把折扣因子放到价值函数的定义里面，价值函数的定义为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241533701.png" alt=""></p>
<p>我们还有一种价值函数： $Q$ 函数。 $Q$ 函数里面包含两个变量：状态和动作。其定义为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241535831.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404181459777.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404181458188.png" alt=""></p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>模型决定了下一步的状态。下一步的状态取决于当前的状态以及当前采取的动作。它由状态转移概率和奖励函数两个部分组成。状态转移概率即：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241550023.png" alt=""></p>
<p>奖励函数是指我们在当前状态采取了某个动作，可以得到多大的奖励，即：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310241551667.png" alt=""></p>
<h3 id="强化学习智能体的类型"><a href="#强化学习智能体的类型" class="headerlink" title="强化学习智能体的类型"></a>强化学习智能体的类型</h3><h4 id="基于价值的智能体与基于策略的智能体"><a href="#基于价值的智能体与基于策略的智能体" class="headerlink" title="基于价值的智能体与基于策略的智能体"></a>基于价值的智能体与基于策略的智能体</h4><p><strong>基于价值的智能体<code>(value-based agent)</code></strong>显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的。<strong>基于策略的智能体<code>(policy-based agent)</code></strong> 直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。把基于价值的智能体和基于策略的智能体结合起来就有了<strong>演员-评论员智能体<code>(actor-critic agent)</code></strong>。这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作。</p>
<h4 id="有模型强化学习智能体与免模型强化学习智能体"><a href="#有模型强化学习智能体与免模型强化学习智能体" class="headerlink" title="有模型强化学习智能体与免模型强化学习智能体"></a>有模型强化学习智能体与免模型强化学习智能体</h4><p><strong>有模型<code>(model-based)</code>强化学习智能体</strong>通过学习状态的转移来采取动作。<strong>免模型<code>(model-free)</code>强化学习智能体</strong>没有去直接估计状态的转移，也没有得到环境的具体转移变量，它通过学习价值函数和策略函数进行决策。免模型强化学习智能体的模型里面没有环境转移的模型。</p>
<h2 id="学习与规划"><a href="#学习与规划" class="headerlink" title="学习与规划"></a>学习与规划</h2><p>在规划中，环境是已知的，智能体被告知了整个环境的运作规则的详细信息。</p>
<h2 id="探索和利用"><a href="#探索和利用" class="headerlink" title="探索和利用"></a>探索和利用</h2><p><strong>探索</strong>即我们去探索环境，通过尝试不同的动作来得到最佳的策略（带来最大奖励的策略）。<strong>利用</strong>即我们不去尝试新的动作，而是采取已知的可以带来很大奖励的动作。</p>
<h1 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h1><h2 id="马尔可夫过程"><a href="#马尔可夫过程" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h2><h3 id="马尔可夫性质"><a href="#马尔可夫性质" class="headerlink" title="马尔可夫性质"></a>马尔可夫性质</h3><p>在随机过程中， 马尔可夫性质<code>(Markov property)</code>是指一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310301135977.jpg" alt=""></p>
<h3 id="马尔可夫链"><a href="#马尔可夫链" class="headerlink" title="马尔可夫链"></a>马尔可夫链</h3><p>离散时间的马尔可夫过程也称为马尔可夫链<code>(Markov chain)</code>。  </p>
<h2 id="马尔可夫奖励过程"><a href="#马尔可夫奖励过程" class="headerlink" title="马尔可夫奖励过程"></a>马尔可夫奖励过程</h2><p>马尔可夫奖励过程<code>(Markov reward process, MRP)</code>是马尔可夫链加上奖励函数。</p>
<h3 id="回报与价值函数"><a href="#回报与价值函数" class="headerlink" title="回报与价值函数"></a>回报与价值函数</h3><p>范围<code>(horizon)</code>是指一个回合的长度（每个回合最大的时间步数），它是由有限个步数决定的。</p>
<p>回报<code>(return)</code> 可以定义为奖励的逐步叠加，假设时刻 $t$ 后的奖励序列为 $r_{t+1}, r_{t+2}, r_{t+3}, \cdots ，$ 则回报为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310311129686.png" alt=""></p>
<p>对于马尔可夫奖励过程，状态价值函数被定义成回报的期望：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202310311130831.png" alt=""></p>
<h3 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h3><p>贝尔曼方程定义了当前状态与未来状态之间的关系。未来奖励的折扣总和加上即时奖励，就组成了贝尔曼方程。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061010578.png" alt=""></p>
<h4 id="全期望公式"><a href="#全期望公式" class="headerlink" title="全期望公式"></a>全期望公式</h4><p>在推导贝尔曼方程之前，首先需要仿照全期望公式<code>(law of total expectation)</code> 的证明过程来证明：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061026274.png" alt=""></p>
<p>为了表达方便，我们令 $s = s_{t}$ ，$ g^{\prime} = G_{t+1}$ ， $s^{\prime} = s_{t+1}$ ，那么我们可以得到如下推导，其中第二步根据全概率公式和期望的线性性，可以对外部的期望$\mathbb{E}[\cdot | s]$展开未对中间变量$s^{\prime}$的求和（或积分）：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061027021.png" alt=""></p>
<h4 id="贝尔曼方程推导"><a href="#贝尔曼方程推导" class="headerlink" title="贝尔曼方程推导"></a>贝尔曼方程推导</h4><p>贝尔曼方程的推导过程如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061041371.png" alt=""></p>
<p>上式的进一步推导如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411152225752.png" alt=""></p>
<p>我们可以把贝尔曼方程写成矩阵的形式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061045825.png" alt=""></p>
<p>当我们把贝尔曼方程写成矩阵形式后，可以直接求解：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061045446.png" alt=""></p>
<h3 id="计算马尔可夫奖励过程价值的迭代算法"><a href="#计算马尔可夫奖励过程价值的迭代算法" class="headerlink" title="计算马尔可夫奖励过程价值的迭代算法"></a>计算马尔可夫奖励过程价值的迭代算法</h3><p>我们可以将迭代的方法应用于状态非常多的马尔可夫奖励过程<code>(large MRP)</code>，比如：动态规划的方法，蒙特卡洛的方法（通过采样的办法计算它），时序差分学习<code>(temporal-difference learning， TD learning)</code>的方法（时序差分学习是动态规划和蒙特卡洛方法的一个结合）。</p>
<p>蒙特卡洛方法就是当得到一个马尔可夫奖励过程后，我们可以从某个状态开始，把小船放到状态转移矩阵里面，让它“随波逐流”，这样就会产生一个轨迹。产生一个轨迹之后，就会得到一个奖励，那么直接把折扣的奖励即回报 $g$ 算出来。算出来之后将它积累起来，得到回报 $G_{t}$。当积累了一定数量的轨迹之后，我们直接用 $G_{t}$ 除以轨迹数量，就会得到某个状态的价值。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061059726.png" alt=""></p>
<p>动态规划方法就是一直迭代贝尔曼方程，直到价值函数收敛，我们就可以得到某个状态的价值。我们通过自举<code>(bootstrapping)</code>的方法不停地迭代贝尔曼方程，当最后更新的状态与我们上一个状态的区别并不大的时候，更新就可以停止，我们就可以输出最新的 $V^{\prime}(s)$ 作为它当前的状态的价值。这里就是把贝尔曼方程变成一个贝尔曼更新<code>(Bellman update)</code>，这样就可以得到状态的价值。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061103338.png" alt=""></p>
<h2 id="马尔可夫决策过程-1"><a href="#马尔可夫决策过程-1" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2><p>相对于马尔可夫奖励过程，马尔可夫决策过程多了决策（决策是指动作），其他的定义与马尔可夫奖励过程的是类似的。此外，状态转移也多了一个条件，变成了 $p (s_{t+1}=s^{\prime}|s_{t}=s, a_{t}=a)$。未来的状态不仅依赖于当前的状态，也依赖于在当前状态智能体采取的动作。</p>
<h3 id="马尔可夫决策过程中的策略"><a href="#马尔可夫决策过程中的策略" class="headerlink" title="马尔可夫决策过程中的策略"></a>马尔可夫决策过程中的策略</h3><p>策略定义了在某一个状态应该采取什么样的动作。知道当前状态后，我们可以把当前状态代入策略函数来得到一个概率，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061115977.png" alt=""></p>
<p>已知马尔可夫决策过程和策略 $\pi$，我们可以把马尔可夫决策过程转换成马尔可夫奖励过程。在马尔可夫决策过程里面，状态转移函数 $P(s^{\prime}|s, a)$ 基于它当前的状态以及它当前的动作。因为我们现在已知策略函数，也就是已知在每一个状态下，可能采取的动作的概率，所以我们就可以直接把动作进行加和，去掉 $a$，这样我们就可以得到对于马尔可夫奖励过程的转移，这里就没有动作，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061123723.png" alt=""></p>
<p>对于奖励函数，我们也可以把动作去掉，这样就会得到类似于马尔可夫奖励过程的奖励函数，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061124112.png" alt=""></p>
<h3 id="马尔可夫决策过程中的价值函数"><a href="#马尔可夫决策过程中的价值函数" class="headerlink" title="马尔可夫决策过程中的价值函数"></a>马尔可夫决策过程中的价值函数</h3><p>马尔可夫决策过程中的价值函数的定义如下，其中，期望基于我们采取的策略。当策略决定后，我们通过对策略进行采样来得到一个期望，计算出它的价值函数。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061151736.png" alt=""></p>
<p>这里另外引入了一个 $Q$ 函数。 $Q$ 函数也被称为动作价值函数<code>(action-value function)</code>。 $Q$ 函数定义的是在某一个状态采取某一个动作，它有可能得到的回报的一个期望，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061344602.png" alt=""></p>
<p>状态价值函数可以理解为在状态 $s$ 的情况下，未采取动作之前期望的回报值，即所有可能动作的奖励之和。那么我们可以得到如下转化关系公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061423882.png" alt=""></p>
<p>动作价值函数可以理解为在状态 $s$ 下，采取动作 $a$ 之后，转变到下一个状态 $s^{\prime}$ 产生的奖励与下一个状态的期望奖励 $v_{\pi}(s^{\prime})$ 之和。那么我们可以得到如下转化关系公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061426853.png" alt=""></p>
<h3 id="贝尔曼期望方程"><a href="#贝尔曼期望方程" class="headerlink" title="贝尔曼期望方程"></a>贝尔曼期望方程</h3><p>我们可以把状态价值函数和 $Q$ 函数拆解成两个部分：即时奖励和后续状态的折扣价值<code>(discounted value of successor state)</code>。通过对状态价值函数进行分解，我们就可以得到一个类似于之前马尔可夫奖励过程的贝尔曼方程——贝尔曼期望方程<code>(Bellman expectation equation)</code>：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061432650.png" alt=""></p>
<p>对于 $Q$ 函数，我们也可以做类似的分解，得到 $Q$ 函数的贝尔曼期望方程：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061433344.png" alt=""></p>
<p>同时根据上一节得到的状态价值函数和动作价值函数的表达式，我们可以得到贝尔曼期望方程的另一种形式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061441622.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061441784.png" alt=""></p>
<h3 id="备份图"><a href="#备份图" class="headerlink" title="备份图"></a>备份图</h3><p>备份图用来定义未来下一时刻的状态价值函数（动作价值函数）与上一时刻的状态价值函数（动作价值函数）之间的关联。</p>
<p>状态价值函数 $V_{\pi}$ 的备份图和计算分解：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061456984.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061459433.png" alt=""></p>
<p>  动作价值函数 $Q_{\pi}$ 的备份图和计算分解：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061458839.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061458666.png" alt=""></p>
<h3 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h3><p>已知马尔可夫决策过程以及要采取的策略 $\pi$ ，计算价值函数 $V_{\pi}(s)$ 的过程就是策略评估。</p>
<h3 id="预测与控制"><a href="#预测与控制" class="headerlink" title="预测与控制"></a>预测与控制</h3><p>预测<code>(prediction)</code>和控制<code>(control)</code>是马尔可夫决策过程里面的核心问题。<br>预测（评估一个给定的策略）的输入是马尔可夫决策过程 $&lt; S, A, P, R, \gamma &gt;$ 和策略 $\pi$，输出是价值函数 $V_{\pi}$。预测是指给定一个马尔可夫决策过程以及一个策略 $\pi$ ，计算它的价值函数，也就是计算每个状态的价值。<br>控制（搜索最佳策略）的输入是马尔可夫决策过程 $&lt; S, A, P, R, \gamma &gt;$，输出是最佳价值函数<code>(optimal value function)</code> $V^{\ast}$ 和最佳策略<code>(optimal policy)</code> $\pi^{\ast}$。控制就是我们去寻找一个最佳的策略，然后同时输出它的最佳价值函数以及最佳策略。</p>
<h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>动态规划<code>(dynamic programming, DP)</code>适合解决满足最优子结构<code>(optimal substructure)</code>和重叠子问题<code>(overlapping subproblem)</code>两个性质的问题。</p>
<h3 id="马尔可夫决策过程中的策略评估"><a href="#马尔可夫决策过程中的策略评估" class="headerlink" title="马尔可夫决策过程中的策略评估"></a>马尔可夫决策过程中的策略评估</h3><p>  策略评估就是给定马尔可夫决策过程和策略，评估我们可以获得多少价值，即对于当前策略，我们可以得到多大的价值。</p>
<h3 id="马尔可夫决策过程控制"><a href="#马尔可夫决策过程控制" class="headerlink" title="马尔可夫决策过程控制"></a>马尔可夫决策过程控制</h3><p>策略评估是指给定马尔可夫决策过程和策略，我们可以估算出价值函数的值。如果我们只有马尔可夫决策过程，那么应该如何寻找最佳的策略，从而得到最佳价值函数<code>(optimal value function)</code>呢？  </p>
<p>最佳价值函数的定义为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061546075.png" alt=""></p>
<p>最佳价值函数是指，我们搜索一种策略 $\pi$ 让每个状态的价值最大。 $V^{\ast}$ 就是到达每一个状态，它的值的最大化情况。在这种最大化情况中，我们得到的策略就是最佳策略，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061547490.png" alt=""></p>
<p>最佳策略使得每个状态的价值函数都取得最大值。我们可以通过策略迭代和价值迭代来解决马尔可夫决策过程的控制问题。</p>
<h3 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h3><p>历程：迭代执行 $\pi \rightarrow V_{\pi} \rightarrow Q_{\pi} \rightarrow \pi^{\prime}$ ，最终得到 $V^{\ast}$</p>
<p>策略迭代由两个步骤组成：策略评估和策略改进。第一个步骤是策略评估，当前我们在优化策略 $\pi$，在优化过程中得到一个最新的策略。我们先保证这个策略不变，然后估计它的价值，即给定当前的策略函数来估计状态价值函数。第二个步骤是策略改进，得到状态价值函数后，我们可以进一步推算出它的 $Q$ 函数。得到 $Q$ 函数后，我们直接对 $Q$ 函数进行最大化，通过在 $Q$ 函数做一个贪心的搜索来进一步改进策略。这两个步骤一直在迭代进行。</p>
<h4 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h4><p>当我们一直采取 $\arg \max$ 操作的时候，我们会得到一个单调的递增。通过采取这种贪心操作，我们就会得到更好的或者不变的策略，而不会使价值函数变差。所以当改进停止后，我们就会得到一个最佳策略。当改进停止后，我们取让 $Q$ 函数值最大化的动作， $Q$ 函数就会直接变成价值函数，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061626343.png" alt=""></p>
<p>我们也就可以得到贝尔曼最优方程<code>(Bellman optimality equation)</code>。贝尔曼最优方程表明：最佳策略下的一个状态的价值必须等于在这个状态下采取最好动作得到的回报的期望。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061637403.png" alt=""></p>
<h3 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h3><p>历程：迭代执行 $Q_{\pi} \rightarrow V_{\pi}$ ，最终得到 $\pi^{\ast}$</p>
<h4 id="最优性原理"><a href="#最优性原理" class="headerlink" title="最优性原理"></a>最优性原理</h4><p>最优性原理定理<code>(principle of optimality theorem)</code>：一个策略 $\pi(a|s)$ 在状态 $s$ 达到了最优价值，也就是 $V_{\pi}(s) = V^{\ast}(s)$ 成立，当且仅当对于任何能够从 $s$ 到达的 $s^{\prime}$ ，都已经达到了最优价值。也就是对于所有的 $s^{\prime}$，$ V_{\pi}(s^{\prime}) = V^{\ast}(s^{\prime})$ 恒成立。</p>
<h4 id="确认性价值迭代"><a href="#确认性价值迭代" class="headerlink" title="确认性价值迭代"></a>确认性价值迭代</h4><p>如果我们知道子问题 $V^{\ast}(s^{\prime})$ 的最优解，就可以通过价值迭代来得到最优的 $V^{\ast}(s)$ 的解。价值迭代就是把贝尔曼最优方程当成一个更新规则来进行，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311061736922.png" alt=""></p>
<p>上式只有当整个马尔可夫决策过程已经达到最佳的状态时才满足。我们可以把它转换成一个备份的等式。备份的等式就是一个迭代的等式。我们不停地迭代贝尔曼最优方程，价值函数就能逐渐趋向于最佳的价值函数，这是价值迭代算法的精髓。</p>
<h1 id="表格型方法"><a href="#表格型方法" class="headerlink" title="表格型方法"></a>表格型方法</h1><h2 id="马尔可夫决策过程-2"><a href="#马尔可夫决策过程-2" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2><p>状态转移概率是具有马尔可夫性质的（系统下一时刻的状态仅由当前时刻的状态决定，不依赖于以往任何状态）。状态、动作、状态转移概率和奖励，这 $4$ 个合集就构成了强化学习马尔可夫决策过程的四元组，后面也可能会再加上折扣因子构成五元组。</p>
<h3 id="有模型"><a href="#有模型" class="headerlink" title="有模型"></a>有模型</h3><p>我们与环境交互时，只能走一条完整的通路，这里面产生了一系列决策的过程，我们与环境交互产生了经验。我们会使用概率函数 $P [s_{t+1}, r_{t} | s_{t}, a_{t}]$ 和奖励函数 $R [s_{t}, a_{t}]$ 来描述环境。概率函数就是状态转移的概率，它反映的是环境的随机性。</p>
<h3 id="免模型"><a href="#免模型" class="headerlink" title="免模型"></a>免模型</h3><p>很多强化学习的经典算法都是免模型的，也就是环境是未知的。我们处在未知的环境里，也就是这一系列的决策的概率函数和奖励函数是未知的，这就是有模型与免模型的最大的区别。</p>
<h2 id="Q-表格"><a href="#Q-表格" class="headerlink" title="Q 表格"></a>Q 表格</h2><p>$Q$ 表格里面 $Q$ 函数的意义就是我们选择了某个动作后，最后能不能成功，就需要我们去计算在某个状态下选择某个动作，后续能够获得多少总奖励。如果可以预估未来的总奖励的大小，我们就知道在当前的状态下选择哪个动作价值更高。我们选择某个动作是因为这样未来可以获得的价值会更高。所以强化学习的目标导向性很强，环境给出的奖励是非常重要的反馈，它根据环境的奖励来做选择。</p>
<p>$Q$ 表格的更新是接下来要引入的强化概念。强化是指我们可以用下一个状态的价值来更新当前状态的价值，其实就是强化学习里面自举的概念。在强化学习里面，我们可以每走一步更新一次 $Q$ 表格，用下一个状态的 $Q$ 值来更新当前状态的 $Q$ 值，这种单步更新的方法被称为时序差分方法。</p>
<h2 id="免模型预测"><a href="#免模型预测" class="headerlink" title="免模型预测"></a>免模型预测</h2><h3 id="蒙特卡洛策略评估"><a href="#蒙特卡洛策略评估" class="headerlink" title="蒙特卡洛策略评估"></a>蒙特卡洛策略评估</h3><p>蒙特卡洛方法是基于采样的方法，给定策略 $\pi$ ，我们让智能体与环境进行交互，可以得到很多轨迹。每个轨迹都有对应的回报：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081352612.png" alt=""></p>
<p>我们求出所有轨迹的回报的平均值，就可以知道某一个策略对应状态的价值，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081353064.png" alt=""></p>
<p>假设现在有样本 $x_{1}, x_{2}, \cdots, x_{t}$，我们可以把经验均值转换成增量均值的形式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081441872.png" alt=""></p>
<p>通过这种转换，我们就可以把上一时刻的平均值与现在时刻的平均值建立联系，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081442053.png" alt=""></p>
<p>基于此，我们可以把蒙特卡洛方法更新的方法写成增量式蒙特卡洛方法。我们采集数据，得到一个新的轨迹 $(s_{1}, a_{1}, r_{1}, \cdots, s_{t})$。对于这个轨迹，我们采用增量的方法进行更新：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081444676.png" alt=""></p>
<p><strong>动态规划方法和蒙特卡洛方法的差异</strong></p>
<p>在动态规划方法里面，我们使用了自举的思想。自举就是我们基于之前估计的量来估计一个量。此外，动态规划方法使用贝尔曼期望备份，通过上一时刻的值 $V_{i−1}(s^{\prime})$ 来更新当前时刻的值 $V_{i}(s)$ ，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081449238.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081500026.png" alt=""></p>
<p>蒙特卡洛方法通过一个回合的经验平均回报（实际得到的奖励）来进行更新，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081501924.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081501433.png" alt=""></p>
<h3 id="时序差分"><a href="#时序差分" class="headerlink" title="时序差分"></a>时序差分</h3><p>时序差分方法的目的是对于某个给定的策略 $\pi$，在线<code>(online)</code>地算出它的价值函数 $V_{\pi}$，即一步一步地<code>(step-by-step)</code>算。最简单的算法是一步时序差分<code>(one-step TD)</code>，即 <strong><code>TD(0)</code></strong>。每往前走一步，就做一步自举，用得到的估计回报 $r_{t+1}+ \gamma V (s_{t+1})$ 来更新上一时刻的值 $V (s_{t})$：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081538081.png" alt=""></p>
<p><strong>蒙特卡洛方法和时间差分方法的对比</strong></p>
<ul>
<li>时序差分方法可以在线学习，每走一步就可以更新，效率高。蒙特卡洛方法必须等游戏结束时才可以学习。</li>
<li>时序差分方法可以从不完整序列上进行学习。蒙特卡洛方法只能从完整的序列上进行学习。</li>
<li>时序差分方法可以在连续的环境下（没有终止）进行学习。蒙特卡洛方法只能在有终止的情况下学习。</li>
<li>时序差分方法利用了马尔可夫性质，在马尔可夫环境下有更高的学习效率。蒙特卡洛方法没有假设环境具有马尔可夫性质，利用采样的价值来估计某个状态的价值，在不是马尔可夫的环境下更加有效。</li>
</ul>
<h3 id="动态规划方法、蒙特卡洛方法以及时序差分方法的自举和采样"><a href="#动态规划方法、蒙特卡洛方法以及时序差分方法的自举和采样" class="headerlink" title="动态规划方法、蒙特卡洛方法以及时序差分方法的自举和采样"></a>动态规划方法、蒙特卡洛方法以及时序差分方法的自举和采样</h3><p>自举是指更新时使用了估计。蒙特卡洛方法没有使用自举，因为它根据实际的回报进行更新。动态规划方法和时序差分方法使用了自举。</p>
<p>采样是指更新时通过采样得到一个期望。蒙特卡洛方法是纯采样的方法。动态规划方法没有使用采样，它是直接用贝尔曼期望方程来更新状态价值的。时序差分方法使用了采样。时序差分目标由两部分组成，一部分是采样，一部分是自举。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311081632154.png" alt=""></p>
<h2 id="免模型控制"><a href="#免模型控制" class="headerlink" title="免模型控制"></a>免模型控制</h2><p>在我们不知道马尔可夫决策过程模型的情况下，如何优化价值函数，得到最佳的策略呢？我们可以把策略迭代进行广义的推广，使它能够兼容蒙特卡洛和时序差分的方法，即带有蒙特卡洛方法和时序差分方法的广义策略迭代<code>(generalized policy iteration，GPI)</code>。</p>
<p><strong>当我们不知道奖励函数和状态转移时，如何进行策略的优化？</strong></p>
<p>针对上述情况，我们引入了广义的策略迭代的方法。我们对策略评估部分进行修改，使用蒙特卡洛的方法代替动态规划的方法估计 $Q$ 函数。我们首先进行策略评估，使用蒙特卡洛方法来估计策略 $Q = Q_{\pi}$，然后进行策略更新，即得到 $Q$ 函数后，我们就可以通过贪心的方法去改进它。</p>
<p>一个保证策略迭代收敛的假设是回合有探索性开始<code>(exploring start)</code>。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311091108609.png" alt=""></p>
<p>为了确保蒙特卡洛方法能够有足够的探索，我们使用了 $\varepsilon$-贪心探索。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311091120042.png" alt=""></p>
<h3 id="Sarsa：同策略时序差分控制"><a href="#Sarsa：同策略时序差分控制" class="headerlink" title="Sarsa：同策略时序差分控制"></a>Sarsa：同策略时序差分控制</h3><p><code>Sarsa</code>所做出的改变很简单，它将原本时序差分方法更新 $V$ 的过程，变成了更新 $Q$，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311131348504.png" alt=""></p>
<p>算法由于每次更新值函数时需要知道当前的状态<code>(state)</code>、当前的动作<code>(action)</code>、奖励<code>(reward)</code>、下一步的状态<code>(state)</code>、下一步的动作<code>(action)</code>，即 $(s_{t}, a_{t}, r_{t+1}, s_{t+1}, a_{t+1})$ 这几个值，因此得名<code>Sarsa</code>算法。它走了一步之后，获取了 $(s_{t}, a_{t}, r_{t+1}, s_{t+1}, a_{t+1})$ 之后，就可以做一次更新。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311131355382.png" alt=""></p>
<h4 id="Sarsa-lambda"><a href="#Sarsa-lambda" class="headerlink" title="Sarsa($\lambda$)"></a>Sarsa($\lambda$)</h4><p>$\lambda$ 是脚步衰变值，取值为 $[0, 1]$。通过设定 $\lambda$ 可以让离奖励较近的选择获得较大的奖赏，离奖励较远的选择获得较小的奖赏。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404171543327.png" alt=""></p>
<h3 id="Q学习：异策略时序差分控制"><a href="#Q学习：异策略时序差分控制" class="headerlink" title="Q学习：异策略时序差分控制"></a>Q学习：异策略时序差分控制</h3><p><code>Sarsa</code>是一种<strong>同策略（on-policy）</strong>算法，它优化的是它实际执行的策略，它直接用下一步会执行的动作去优化$Q$表格。同策略在学习的过程中，只存在一种策略，它用一种策略去做动作的选取，也用一种策略去做优化。Q 学习是一种<strong>异策略（off-policy）</strong>算法。异策略在学习的过程中，有两种不同的策略：<strong>目标策略（target policy）</strong>和<strong>行为策略（behavior policy）</strong>。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311131452578.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311131454910.png" alt=""></p>
<h3 id="同策略与异策略的区别"><a href="#同策略与异策略的区别" class="headerlink" title="同策略与异策略的区别"></a>同策略与异策略的区别</h3><p>$Q$学习是一个非常激进的方法，它希望每一步都获得最大的利益；Sarsa则相对较为保守，它会选择一条相对安全的迭代路线。</p>
<h1 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h1><h2 id="策略梯度算法"><a href="#策略梯度算法" class="headerlink" title="策略梯度算法"></a>策略梯度算法</h2><p>策略是一个网络；输入是游戏的画面，它通常是由像素组成的；输出是我们可以执行的动作，有几个动作，输出层就有几个神经元。假设我们现在可以执行的动作有 3 个，输出层就有 3 个神经元，每个神经元对应一个可以采取的动作。</p>
<p>环境是一个函数，我们可以把游戏的主机看成一个函数，虽然它不一定是神经网络，可能是基于规则的（rule-based）模型，但我们可以把它看作一个函数。这个函数一开始先“吐”出一个状态（游戏画面 $s_{1}$ ），接下来演员看到游戏画面 $s_{1}$ 以后，它“吐”出动作 $a_{1}$ 。环境把动作 $a_{1}$ 当作它的输入，再“吐”出新的游戏画面 $s_{2}$。演员看到新的游戏画面 $s_{2}$，再采取新的动作 $a_{2}$。环境看到 $a_{2}$，再“吐”出 $s_{3}$ …… 这个过程会一直持续下去，直到环境觉得应该要停止为止。在一场游戏里面，我们把环境输出的 $s$ 与演员输出的动作 $a$ 全部组合起来，就是一个轨迹，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311140944102.png" alt=""></p>
<p>给定演员的参数 $\theta$，我们可以计算某个轨迹 $\tau$ 发生的概率为</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311140945562.png" alt=""></p>
<p>在某一场游戏的某一个回合里面，我们会得到 $R(\tau)$。我们要做的就是调整演员内部的参数 $\theta$，使得 $R(\tau)$ 的值越大越好。但实际上 $R(\tau)$ 并不只是一个标量，它是一个随机变量，因为演员在给定同样的状态下会采取什么样的动作，这是有随机性的。环境在给定同样的观测时要采取什么样的动作，要产生什么样的观测，本身也是有随机性的，所以 $R(\tau)$ 是一个随机变量。我们能够计算的是 $R(\tau)$ 的期望值。给定某一组参数 $\theta$，我们可计算 $r_{\theta}$ 的期望值为</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311140955539.png" alt=""></p>
<p>总奖励使用 $\tau$ 出现的概率进行加权，对所有的 $\tau$ 进行求和，就是期望值。给定一个参数，我们可以计算期望值为</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311140958839.png" alt=""></p>
<p>因为我们要让奖励越大越好，所以可以使用梯度上升来最大化期望奖励。要进行梯度上升，我们先要计算期望奖励 $\bar{R}_{\theta}$ 的梯度。我们对 $\bar{R}_{\theta}$ 做梯度运算</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141002272.png" alt=""></p>
<p>基于等式 $\nabla f(x)=f(x)\nabla \log f(x)$ ，我们可以得到：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141014067.png" alt=""></p>
<p>实际上期望值 $E_{\tau \sim p_{\theta}(\tau)} [R(\tau)\nabla \log p_{\theta}(\tau)]$ 无法计算，所以我们用采样的方式采样 $N$ 个 $\tau$ 并计算每一个的值，把每一个的值加起来，就可以得到梯度，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141020560.png" alt=""></p>
<p>策略梯度的具体执行示意图：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141027355.png" alt=""></p>
<p>更新完模型以后，我们要重新采样数据再更新模型。注意，一般策略梯度采样的数据只会用一次。我们采样这些数据，然后用这些数据更新参数，再丢掉这些数据。接着重新采样数据，才能去更新参数。</p>
<h2 id="策略梯度实现技巧"><a href="#策略梯度实现技巧" class="headerlink" title="策略梯度实现技巧"></a>策略梯度实现技巧</h2><h3 id="技巧-1：添加基线"><a href="#技巧-1：添加基线" class="headerlink" title="技巧 1：添加基线"></a>技巧 1：添加基线</h3><p>假设我们在某一个状态有 3 个动作 a、 b、 c 可以执行，要把这3 个动作的概率，对数概率都提高。但是它们前面的权重 $R(\tau)$ 是不一样的。权重是有大有小的，权重小的，该动作的概率提高的就少；权重大的，该动作的概率提高的就多。因为对数概率是一个概率，所以动作 a、 b、 c 的对数概率的和是 $0$。所以提高少的，在做完归一化以后，动作 b 的概率就是下降的；提高多的，该动作的概率才会上升。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141110318.png" alt=""></p>
<p>这是一个理想的情况，但是实际上，我们是在做采样本来这边应该是一个期望，对所有可能的 $s$ 与 $a$ 的对进行求和。但我们真正在学习的时候，只是采样了少量的 $s$ 与 $a$ 的对。因为我们做的是采样，所以有一些动作可能从来都没有被采样到。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141113629.png" alt=""></p>
<p>为了解决奖励总是正的的问题，我们可以把奖励减 $b$，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141116781.png" alt=""></p>
<p>通过这种方法，我们就可以让 $R(\tau) - b$ 这一项有正有负。如果我们得到的总奖励 $R(\tau) &gt; b$，就让 $(s, a)$ 的概率上升。如果 $R(\tau) &lt; b$，就算 $R(\tau)$ 是正的，值很小也是不好的，我们就让 $(s, a)$ 的概率下降，让这个状态采取这个动作的分数下降。 $b$ 怎么设置呢？我们可以对 $\tau$ 的值取期望，计算 $\tau$ 的平均值，令 $b \approx E[R(\tau)]$。所以在训练的时候，我们会不断地把 $R(\tau)$ 的值记录下来，会不断地计算 $R(\tau)$ 的平均值，把这个平均值当作 $b$ 来使用。这样就可以让我们在训练的时候， $R(\tau) - b$ 是有正有负的，这是第一个技巧。</p>
<h3 id="技巧2：分配合适的分数"><a href="#技巧2：分配合适的分数" class="headerlink" title="技巧2：分配合适的分数"></a>技巧2：分配合适的分数</h3><p>第二个技巧：给每一个动作分配合适的分数。只要在同一个回合里面，在同一场游戏里面，所有的状态-动作对就使用同样的奖励项进行加权，这显然是不公平的。<strong>一个做法是计算某个状态-动作对的奖励的时候，不把整场游戏得到的奖励全部加起来，只计算从这个动作执行以后得到的奖励。</strong></p>
<p>分配合适的分数这一技巧可以表达为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141126407.png" alt=""></p>
<p>更进一步，我们把未来的奖励做一个折扣，因为虽然在某一时刻，执行某一个动作，会影响接下来所有的结果，但在一般的情况下，时间拖得越长，该动作的影响力就越小。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141127305.png" alt=""></p>
<p>实际上就是这么实现的。 $b$ 可以是依赖状态的，事实上 $b$ 通常是一个网络估计出来的，它是一个网络的输出。我们把 $R − b$ 这一项称为优势函数，用 $A^{\theta}(s_{t}, a_{t})$ 来表示优势函数。优势函数的意义是，假设我们在某一个状态 $s_{t}$ 执行某一个动作 $a_{t}$，相较于其他可能的动作， $a_{t}$ 有多好。优势函数在意的不是绝对的好，而是相对的好，即相对优势。因为在优势函数中，我们会减去一个基线 $b$，所以这个动作是相对的好，不是绝对的好。 $A^{\theta}(s_{t}, a_{t})$ 通常可以由一个网络估计出来，这个网络称为评论员。</p>
<h2 id="REINFORCE：蒙特卡洛策略梯度"><a href="#REINFORCE：蒙特卡洛策略梯度" class="headerlink" title="REINFORCE：蒙特卡洛策略梯度"></a>REINFORCE：蒙特卡洛策略梯度</h2><p>蒙特卡洛方法可以理解为算法完成一个回合之后，再利用这个回合的数据去学习，做一次更新。因为我们已经获得了整个回合的数据，所以也能够获得每一个步骤的奖励，我们可以很方便地计算每个步骤的未来总奖励，即回报 $G_{t}$ 。 $G_{t}$ 是未来总奖励，代表从这个步骤开始，我们能获得的奖励之和。 $G_{1}$ 代表我们从第一步开始，往后能够获得的总奖励。 $G_{2}$ 代表从第二步开始，往后能够获得的总奖励。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404182146893.png" alt=""></p>
<p>相比蒙特卡洛方法一个回合更新一次，时序差分方法是每个步骤更新一次，即每走一步，更新一次，时序差分方法的更新频率更高。时序差分方法使用 $Q$ 函数来近似地表示未来总奖励 $G_{t}$。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141151029.png" alt=""></p>
<p>REINFORCE 用的是回合更新的方式，它在代码上的处理上是先获取每个步骤的奖励，然后计算每个步骤的未来总奖励 $G_{t}$，将每个 $G_{t}$ 代入下式优化每一个动作的输出。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141336604.png" alt=""></p>
<p>未来总奖励可写成一个递推式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141345612.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141350439.png" alt=""></p>
<h1 id="近端策略优化"><a href="#近端策略优化" class="headerlink" title="近端策略优化"></a>近端策略优化</h1><h2 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h2><p>策略梯度是同策略的算法，因为在策略梯度中，我们需要一个智能体、一个策略和一个演员。演员去与环境交互搜集数据，搜集很多的轨迹 $\tau$，根据搜集到的数据按照策略梯度的公式更新策略的参数，所以策略梯度是一个同策略的算法。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141356104.png" alt=""></p>
<p>$E_{\tau \sim p_{\theta}}(\tau)$ 是对策略 $\pi_{\theta}$ 采样的轨迹 $\tau$ 求期望。一旦更新了参数，从 $\theta$ 变成 $\theta^{\prime}$ ，概率 $p_{\theta}(\tau)$ 就不对了，之前采样的数据也不能用了。所以策略梯度是一个会花很多时间来采样数据的算法，其大多数时间都在采样数据。智能体与环境交互以后，接下来就要更新参数。我们只能更新参数一次，然后就要重新采样数据，才能再次更新参数。  </p>
<p>假设我们有一个函数 $f(x)$ ，要计算从分布 $p$ 采样 $x$ ，再把 $x$ 代入 $f$ ，得到 $f(x)$ 。我们该怎么计算 $f(x)$ 的期望值呢？假设我们不能对分布 $p$ 做积分，但可以从分布 $p$ 采样一些数据 $x^{i}$ 。把 $x^{i}$ 代入 $f(x)$ ，取它的平均值，就可以近似 $f(x)$ 的期望值。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202311141652913.png" alt=""></p>
<p>现在有另外一个问题，假设我们不能从分布 $p$ 采样数据，只能从另外一个分布 $q$ 采样数据 $x$ ，其中 $q$ 可以是任何分布。如果我们从 $q$ 采样 $x^{i}$，就不能使用上式计算期望，因为上式是假设 $x$ 都是从 $p$ 采样出来的。我们做一个修正，期望值 $\mathbb{E}_{x\sim p}[f(x)]$ 就是 $\int f(x)p(x)dx$，我们对其做如下的变换：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312050900748.png" alt=""></p>
<p>就可得：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312050901310.png" alt=""></p>
<p>我们就可以写成对 $q$ 里面所采样出来的 $x$ 取期望值。我们从 $q$ 里面采样 $x$，再计算 $f(x)\frac{p(x)}{q(x)}$ ，再取期望值。所以就算我们不能从 $p$ 里面采样数据，但只要能从 $q$ 里面采样数据，就可以计算从 $p$ 采样 $x$ 代入 $f$ 以后的期望值。</p>
<p>$q(x)$ 可以是任何分布，唯一的限制就是 $q(x)$ 的概率是 $0$ 的时候， $p(x)$ 的概率不为 $0$，不然会没有定义。重要性采样有一些问题。虽然我们可以把 $p$ 换成任何的 $q$。但是在实现上， $p$ 和 $q$ 的差距不能太大。差距太大，会有一些问题（方差）。我们可以将 $f(x)$ 和 $f(x)\frac{p(x)}{q(x)}$ 代入方差的公式 $Var[X] = E [X^{2}] - (E[X])^{2}$，可得</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312050910515.png" alt=""></p>
<p>证明：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051018942.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051024229.png" alt=""></p>
<p>现在要做的就是把重要性采样用在异策略的情况中，把同策略训练的算法改成异策略训练的算法。之前我们用策略 $\pi_{\theta}$ 与环境交互，采样出轨迹 $\tau$，计算 $R(\tau)\nabla \log p_{\theta}(\tau)$。现在我们不用 $\theta$ 与环境交互，假设有另外一个策略 $\pi_{\theta}^{\prime}$ ，它就是另外一个演员，它的工作是做示范。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051048897.png" alt=""></p>
<p>$\theta^{\prime}$ 的工作是为 $\theta$ 做示范。它与环境交互，告诉 $\theta$ 它与环境交互会发生什么事，借此来训练 $\theta$。我们要训练的是 $\theta$ ， $\theta^{\prime}$ 只负责做示范，负责与环境交互。我们现在的 $\tau$ 是从 $\theta^{\prime}$ 采样出来的，是用 $\theta^{\prime}$ 与环境交互。所以采样出来的 $\tau$ 是从 $\theta^{\prime}$ 采样出来的，这两个分布不一样。但没有关系，假设我们本来是从 $p$ 采样，但发现不能从 $p$ 采样，所以我们不用 $\theta$ 与环境交互，可以把 $p$ 换成 $q$，在后面补上一个重要性权重。同理，我们把 $\theta$ 换成 $\theta^{\prime}$ 后，要补上一个重要性权重 $\frac{p_{\theta}(\tau)}{p_{\theta}^{\prime}(\tau)}$ 。这个重要性权重就是某一个轨迹 $\tau$ 用 $\theta$ 算出来的概率除以这个轨迹 $\tau$ 用 $\theta^{\prime}$ 算出来的概率。这一项是很重要的，因为我们要学习的是演员 $\theta$，而 $\theta$ 和 $\theta^{\prime}$ 是不太一样的， $\theta^{\prime}$ 见到的情形与 $\theta$ 见到的情形可能不是一样的，所以中间要有一个修正的项。</p>
<p>实际在做策略梯度的时候，我们并不是给整个轨迹 $\tau$ 一样的分数，而是将每一个状态-动作对分开计算。实际更新梯度的过程可写为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051121289.png" alt=""></p>
<p>我们可以通过重要性采样把同策略变成异策略，从 $\theta$ 变成 $\theta^{\prime}$。所以现在 $s_{t}$、 $a_{t}$ 是 $\theta^{\prime}$ 与环境交互以后所采样到的数据。但是训练时，要调整的参数是模型 $\theta$。因为 $\theta^{\prime}$ 与 $\theta$ 是不同的模型，所以我们要有一个修正的项。这个修正的项，就是用重要性采样的技术，把 $s_{t}$、 $a_{t}$ 用 $\theta$ 采样出来的概率除以 $s_{t}$、 $a_{t}$ 用 $\theta^{\prime}$ 采样出来的概率。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051133117.png" alt=""></p>
<p>接下来，我们可以拆解 $p_{\theta}(s_{t}, a_{t})$ 和 $p_{\theta^{\prime}} (s_{t}, a_{t})$，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051324772.png" alt=""></p>
<p>于是我们可得</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051324074.png" alt=""></p>
<p>这里需要做的一件事情是，假设模型是 $\theta$ 的时候，我们看到 $s_{t}$ 的概率，与模型是 $\theta^{\prime}$ 的时候，我们看到 $s_{t}$ 的概率是一样的，即 $p_{\theta}(s_{t}) = p_{\theta^{\prime}}(s_{t})$。因为 $p_{\theta}(s_{t})$ 和 $p_{\theta^{\prime}}(s_{t})$ 是一样的，所以我们可得</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051328636.png" alt=""></p>
<p>实际上，当我们使用重要性采样的时候，要去优化的目标函数为</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051332093.png" alt=""></p>
<p>我们将其记为 $J^{\theta^{\prime}}(\theta)$，因为 $J^{\theta^{\prime}}(\theta)$ 括号里面的 $\theta$ 代表我们要去优化的参数。</p>
<p>根据PPO的原始论文建议，我们一般使用GAE来计算优势函数：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411162249796.png" alt=""></p>
<p>其中，$\delta_{t}^{V}$的表达式为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411162250464.png" alt=""></p>
<h2 id="近端策略优化-1"><a href="#近端策略优化-1" class="headerlink" title="近端策略优化"></a>近端策略优化</h2><p>我们可以通过重要性采样把同策略换成异策略，但重要性采样有一个问题：如果 $p_{\theta}(a_{t}|s_{t})$ 与 $p_{\theta^{\prime}}(s_{t}|a_{t})$相差太多，即这两个分布相差太多，重要性采样的结果就会不好。怎么避免它们相差太多呢？这就是PPO要做的事情。</p>
<p>PPO 需要优化目标函数 $J_{\theta^{\prime}}(\theta)$ ，这个目标函数牵涉到重要性采样。在做重要性采样的时候， $p_{\theta}(a_{t}|s_{t})$ 不能与 $p_{\theta^{\prime}}(s_{t}|a_{t})$ 相差太多。做示范的模型不能与真正的模型相差太多，相差太多，重要性采样的结果就会不好。我们在训练的时候，应多加一个约束。这个约束是 $\theta$ 与 $\theta^{\prime}$ 输出的动作的 KL 散度，这一项用于衡量 $\theta$ 与 $\theta^{\prime}$ 的相似程度。<strong>注意，虽然 PPO 的优化目标涉及到了重要性采样，但其只用到了上一轮策略 $\theta^{\prime}$ 的数据。 PPO 目标函数中加入了 KL 散度的约束，行为策略 $\theta^{\prime}$ 和目标策略 $\theta$ 非常接近， PPO 的行为策略和目标策略可认为是同一个策略，因此 PPO 是同策略算法。</strong></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051352706.png" alt=""></p>
<p>PPO 有一个前身： 信任区域策略优化（trust region policy optimization， TRPO）。TRPO可表示为</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051358467.png" alt=""></p>
<p>TRPO 与 PPO 不一样的地方是约束所在的位置不一样， PPO 直接把约束放到要优化的式子里面，我们就可以用梯度上升的方法去最大化 $J_{PPO}^{\theta^{\prime}}(\theta)$。但 TRPO 是把 KL 散度当作约束，它希望 $\theta$ 与 $\theta^{\prime}$ 的 KL散度小于 $\delta$。如果我们使用的是基于梯度的优化，有约束是很难处理的。</p>
<h3 id="近端策略优化惩罚"><a href="#近端策略优化惩罚" class="headerlink" title="近端策略优化惩罚"></a>近端策略优化惩罚</h3><p>PPO 算法有两个主要的变种：<strong>近端策略优化惩罚（PPO-penalty）</strong>和<strong>近端策略优化裁剪（PPO-clip）</strong>。</p>
<p>先初始化一个策略的参数 $\theta^{0}$。在每一个迭代里面，我们用前一个训练的迭代得到的演员的参数 $\theta^{k}$ 与环境交互，采样到大量状态-动作对。根据 $\theta^{k}$ 交互的结果，我们估测 $A^{\theta^{k}}(s_{t},a_{t})$。我们使用 PPO 的优化公式。但与原来的策略梯度不一样，原来的策略梯度只能更新一次参数，更新完以后，我们就要重新采样数据。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051417680.png" alt=""></p>
<p>在 PPO 的论文里面还有一个自适应 KL 散度 。这里会遇到一个问题就，即 $\beta$ 要设置为多少。我们有一个动态调整 $\beta$ 的方法。在这个方法里面，我们先设一个可以接受的 KL 散度的最大值。假设优化完上式以后， KL 散度的值太大，这就代表后面惩罚的项 $\beta KL(\theta, \theta^{k})$ 没有发挥作用，我们就把 $\beta$ 增大。另外，我们设一个 KL 散度的最小值。如果优化完上式以后， KL 散度比最小值还要小，就代表后面这一项的效果太强了，我们怕他只优化后一项，使 $\theta$ 与 $\theta^{k}$ 一样，这不是我们想要的，所以我们要减小 $\beta$。 $\beta$ 是可以动态调整的，因此我们称之为自适应 KL 惩罚。</p>
<p>近端策略优化惩罚可表示为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051429446.png" alt=""></p>
<h3 id="近端策略优化裁剪"><a href="#近端策略优化裁剪" class="headerlink" title="近端策略优化裁剪"></a>近端策略优化裁剪</h3><p>如果我们觉得计算 KL 散度很复杂，那么还有一个 PPO2 算法， PPO2 即近端策略优化裁剪。近端策略优化裁剪的目标函数里面没有 KL 散度，其要最大化的目标函数为，其中裁剪函数是指，在括号里面有 3 项，如果第一项小于第二项，那就输出 $1-\varepsilon$；第一项如果大于第三项，那就输出 $1+\varepsilon$：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312051437351.png" alt=""></p>
<h1 id="深度Q​网络"><a href="#深度Q​网络" class="headerlink" title="深度Q​网络"></a>深度Q​网络</h1><p>传统的强化学习算法会使用表格的形式存储状态价值函数 $V (s)$ 或动作价值函数 $Q(s, a)$ ，但是这样的方法存在很大的局限性。例如，现实中的强化学习任务所面临的状态空间往往是连续的，存在无穷多个状态，在这种情况下，就不能再使用表格对价值函数进行存储。价值函数近似利用函数直接拟合状态价值函数或动作价值函数，降低了对存储空间的要求，有效地解决了这个问题。</p>
<p>深度 $Q$ 网络（deep Q-network， DQN）是指基于深度学习的 $Q$ 学习算法，主要结合了价值函数近似与神经网络技术，并采用目标网络和经历回放的方法进行网络的训练。</p>
<h2 id="状态价值函数"><a href="#状态价值函数" class="headerlink" title="状态价值函数"></a>状态价值函数</h2><p>深度 $Q$ 网络是基于价值的算法，在基于价值的算法里面，我们学习的不是策略，而是评论员（critic）。评论员的任务是评价现在的动作有多好或有多不好。假设有一个演员，其要学习一个策略来得到尽量高的回报。评论员就是评价演员的策略 $\pi$ 好还是不好，即策略评估。例如，有一种评论员称为状态价值函数 $V_{\pi}$。状态价值函数是指，假设演员的策略是 $\pi$，用 $\pi$ 与环境交互，假设 $\pi$ 看到了某一个状态 $s$，例如在玩雅达利游戏，状态 $s$ 是某一个画面， $\pi$ 看到某一个画面，接下来一直到游戏结束，期望的累积奖励有多大。</p>
<p>怎么衡量状态价值函数 $V_{\pi}(s)$ 呢？有两种不同的方法：基于蒙特卡洛的方法和基于时序差分的方法。基于蒙特卡洛的方法就是让演员与环境交互，我们要看演员好不好，就让演员与环境交互，让评论员评价。基于时序差分的方法不需要玩到游戏结束，只需要在游戏的某一个状态 $s_{t}$ 的时候，采取动作 $a_{t}$ 得到奖励 $r_{t}$ ，接下来进入状态 $s_{t+1}$，就可以使用时序差分的方法。我们是这样训练的，我们把 $s_{t}$ 输入网络，因为把 $s_{t}$ 输入网络会得到 $V_{\pi}(s_{t})$，把 $s_{t+1}$ 输入网络会得到 $V_{\pi}(s_{t+1})$， $V_{\pi}(s_{t})$ 减 $V_{\pi}(s_{t+1})$ 的值应该是 $r_{t}$。我们希望它们相减的损失与 $r_{t}$ 接近，训练下去，更新 $V_{\pi}$ 的参数，我们就可以把 $V_{\pi}$ 函数学习出来。</p>
<p>蒙特卡洛方法与时序差分方法有什么差别呢？蒙特卡洛方法最大的问题就是方差很大。<strong>（时序差分就是将自举估计的回报代替蒙特卡洛更新式中的回报，这样我们不需要经过一个完整的episode就可以估算出回报）</strong></p>
<h2 id="动作价值函数"><a href="#动作价值函数" class="headerlink" title="动作价值函数"></a>动作价值函数</h2><p>状态价值函数的输入是一个状态，它根据状态计算出这个状态以后的期望的累积奖励是多少。动作价值函数的输入是一个状态-动作对，其指在某一个状态采取某一个动作，假设我们都使用策略 $\pi$ ，得到的累积奖励的期望值有多大。</p>
<p>$Q$ 函数有两种写法：</p>
<ul>
<li>输入是状态与动作，输出就是一个标量。这种 $Q$ 函数既适用于连续动作（动作是无法穷举的），又适用于离散动作。</li>
<li>输入是一个状态，输出就是多个值。这种 $Q$ 函数只适用于离散动作。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101521630.png" alt=""></p>
<p>有了 $Q$ 函数以后，我们把根据下式决定动作的策略称为 $\pi^{\prime}$，$\pi^{\prime}$ 一定比 $\pi$ 好。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101530257.png" alt=""></p>
<h2 id="目标网络"><a href="#目标网络" class="headerlink" title="目标网络"></a>目标网络</h2><p>所以我们会把其中一个 $Q$ 网络，通常是把下图右边的 $Q$ 网络固定住。在训练的时候，我们只更新左边的 $Q$ 网络的参数，而右边的 $Q$ 网络的参数会被固定。因为右边的 $Q$ 网络负责产生目标，所以被称为目标网络。因为目标网络是固定的，所以现在得到的目标 $r_{t} + Q_{\pi} (s_{t+1}, \pi (s_{t+1}))$ 的值也是固定的。我们只调整左边 $Q$ 网络的参数，它就变成一个回归问题。我们希望模型输出的值与目标越接近越好，这样会最小化它的均方误差（mean square error）</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101539738.png" alt=""></p>
<h2 id="探索"><a href="#探索" class="headerlink" title="探索"></a>探索</h2><p>像 $Q$ 函数中，如果我们采取的动作总是固定的，会遇到的问题就是这不是一个好的收集数据的方式。这个问题就是探索-利用窘境（exploration-exploitation dilemma） 问题，有两个方法可以解决这个问题： $\varepsilon$-贪心和玻尔兹曼探索（Boltzmann exploration）。</p>
<p>$\varepsilon$-贪心是指我们有 $1 − \varepsilon$ 的概率会按照 $Q$ 函数来决定动作，可写为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101543872.png" alt=""></p>
<p>还有一个方法称为玻尔兹曼探索。在玻尔兹曼探索中，我们假设对于任意的 $s$、 $a$， $Q(s, a) \geq 0$，因此 $a$ 被选中的概率与 $e^{Q(s,a)/T}$ 呈正比，其中， $T \gt 0$ 称为温度系数。如果 $T$ 很大，所有动作几乎以等概率选择（探索）；如果 $T$ 很小， $Q$ 值大的动作更容易被选中（利用）；如果 $T$ 趋于 $0$，我们就只选择最优动作。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101547561.png" alt=""></p>
<h2 id="经验回放"><a href="#经验回放" class="headerlink" title="经验回放"></a>经验回放</h2><p>经验回放会构建一个回放缓冲区（replay buffer），回放缓冲区又被称为回放内存（replay memory）。回放缓冲区是指现在有某一个策略 $\pi$ 与环境交互，它会去收集数据，我们把所有的数据放到一个数据缓冲区（buffer）里面，数据缓冲区里面存储了很多数据。</p>
<p>这么做有两个好处。第一个好处是，在进行强化学习的时候，往往最花时间的步骤是与环境交互，训练网络反而是比较快的。因为我们用 GPU 训练其实很快，真正花时间的往往是与环境交互。用回放缓冲区可以减少与环境交互的次数，因为在做训练的时候，经验不需要通通来自于某一个策略。一些过去的策略所得到的经验可以放在回放缓冲区里面被使用很多次，被反复的再利用，这样可以比较高效地采样经验。第二个好处是，在训练网络的时候，其实我们希望一个批量里面的数据越多样（diverse）越好。如果批量里面的数据都是同样性质的，我们训练下去，训练结果是容易不好的。如果批量里面都是一样的数据，训练的时候，性能会比较差。我们希望批量里的数据越多样越好。如果回放缓冲区里面的经验通通来自于不同的策略，我们采样到的一个批量里面的数据会是比较多样的。</p>
<h2 id="深度Q网络"><a href="#深度Q网络" class="headerlink" title="深度Q网络"></a>深度Q网络</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101552662.png" alt=""></p>
<h1 id="深度Q网络进阶技巧"><a href="#深度Q网络进阶技巧" class="headerlink" title="深度Q网络进阶技巧"></a>深度Q网络进阶技巧</h1><h2 id="双深度Q网络"><a href="#双深度Q网络" class="headerlink" title="双深度Q网络"></a>双深度Q网络</h2><p>在实现上， $Q$ 值往往是被高估的。</p>
<p>为什么 $Q$ 值总是被高估了？因为实际在训练的时候，如下式所示，我们要让左式与右式（目标）越接近越好。但目标的值很容易被设得太高，因为在计算目标的时候，我们实际上在做的，是看哪一个 $a$ 可以得到最大的 $Q$ 值，就把它加上去变成目标。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101601594.png" alt=""></p>
<p>例如，假设我们现在有 $4$ 个动作，本来它们得到的 $Q$ 值都是差不多的，它们得到的奖励也是差不多的。但是在估计的时候，网络是有误差的。如图（a）所示，假设是第一个动作被高估了，绿色代表是被高估的量，智能体就会选这个动作，就会选这个高估的 $Q$ 值来加上 $r_{t}$ 来当作目标。如图（b）所示，如果第四个动作被高估了，智能体就会选第四个动作来加上 $r_{t}$ 当作目标。所以智能体总是会选那个 $Q$ 值被高估的动作，总是会选奖励被高估的动作的 $Q$ 值当作最大的结果去加上 $r_{t}$ 当作目标，所以目标值总是太大。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101604354.png" alt=""></p>
<p>怎么解决目标值总是太大的问题呢？在DDQN里面，选动作的 $Q$ 函数与计算值的 $Q$ 函数不是同一个。在原来的深度 $Q$ 网络里面，我们穷举所有的 $a$，把每一个 $a$ 都代入 $Q$ 函数，看哪一个 $a$ 可以得到的 $Q$ 值最高，就把那个 $Q$ 值加上 $r_{t}$。但是在 DDQN 里面有两个 $Q$ 网络，第一个 $Q$ 网络 $Q$ 决定哪一个动作的 $Q$ 值最大（我们把所有的 $a$ 代入 $Q$ 函数中，看看哪一个 $a$ 的 $Q$ 值最大）。我们决定动作以后， $Q$ 值是用 $Q^{\prime}$ 算出来的。</p>
<p>DDQN 相较于原来的深度 $Q$ 网络的更改是最少的，它几乎没有增加任何的运算量，也不需要新的网络，因为原来就有两个网络。我们只需要做一件事：本来是用目标网络 $Q^{\prime}$ 找使 $Q$ 值最大的 $a$，现在改成用另外一个会更新的 $Q$ 网络来找使 $Q$ 值最大的 $a$。如果只选一个技巧，我们一般都会选 DDQN，因为其很容易实现。</p>
<h2 id="竞争深度Q网络"><a href="#竞争深度Q网络" class="headerlink" title="竞争深度Q网络"></a>竞争深度Q网络</h2><p>第二个技巧是竞争深度 $Q$ 网络（dueling DQN） ，相较于原来的深度 $Q$ 网络，它唯一的差别是改变了网络的架构。 $Q$ 网络输入状态，输出的是每一个动作的 $Q$ 值。原来的深度 $Q$ 网络直接输出 $Q$ 值，竞争深度 $Q$ 网络不直接输出 $Q$ 值，而是分成两条路径运算。第一条路径会输出一个标量 $V(s)$，因为它与输入 $s$ 是有关系的，所以称为 $V(s)$。第二条路径会输出一个向量 $A(s, a)$，它的每一个动作都有一个值。我们再把 $V(s)$ 和 $A(s, a)$ 加起来就可以得到 $Q$ 值 $Q(s, a)$。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101611233.png" alt=""></p>
<p>根据网络的参数， $V (s)$ 与 $A(s, a)$ 的值输出以后，就直接把它们加起来，所以其实不是修改 $Q$ 值。在学习网络的时候，假设我们希望 $Q$ 表格中的 $3$ 增加 $1$ 变成 $4$、 $−1$ 增加 $1$ 变成 $0$。最后我们在训练网络的时候，我们可能就不用修改 $A(s, a)$ 的值，就修改 $V (s)$ 的值，把 $V (s)$ 的值从 $0$ 变成 $1$。从 $0$ 变成 $1$ 有什么好处呢？本来只想修改两个值，但 $Q$ 表格中的第三个值也被修改了： $−2$ 变成了 $−1$。所以有可能我们在某一个状态下，只采样到这两个动作，没采样到第三个动作，但也可以更改第三个动作的 $Q$ 值。这样的好处就是我们不需要把所有的状态-动作对都采样，可以用比较高效的方式去估计 $Q$ 值。因为有时候我们更新的时候，不一定是更新 $Q$ 表格，而是只更新了 $V(s)$，但更新 $V(s)$ 的时候，只要修改 $V(s)$ 的值， $Q$ 表格的值也会被修改。竞争深度 $Q$ 网络是一个使用数据比较有效率的方法。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101617996.png" alt=""></p>
<p>可能会有人认为使用竞争深度 $Q$ 网络会有一个问题，竞争深度 $Q$ 网络最后学习的结果可能是这样的：智能体就学到 $V(s)$ 等于 $0$， $A(s, a)$ 等于 $Q$，使用任何竞争深度 $Q$ 网络就没有任何好处，就和原来的深度 $Q$ 网络一样。为了避免这个问题出现，实际上我们要给 $A(s, a)$ 一些约束，让 $A(s, a)$ 的更新比较麻烦，让网络倾向于使用 $V (s)$ 来解决问题。<br>例如，我们有不同的约束，一个最直觉的约束是必须要让 $A(s, a)$ 的每一列的和都是 $0$，所以这边举的例子，列的和都是 $0$。如果这边列的和都是 $0$，我们就可以把 $V (s)$ 的值想成是上面 $Q$ 的每一列的平均值。这个平均值，加上 $A(s, a)$ 的值才会变成是 $Q$ 的值。所以假设在更新参数的时候，要让整个列一起被更新，更新 $A(s, a)$ 的某一列比较麻烦，所以我们就不会想要更新 $A(s, a)$ 的某一列。因为 $A(s, a)$ 的每一列的和都要是 $0$，所以我们无法让 $A(s, a)$ 的某列的值都加 $1$，这是做不到的，因为它的约束就是和永远都是 $0$，所以不可以都加 $1$，这时候就会强迫网络去更新 $V(s)$ 的值，让我们可以用比较有效率的方法去使用数据。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101625556.png" alt=""></p>
<h2 id="优先级经验回放"><a href="#优先级经验回放" class="headerlink" title="优先级经验回放"></a>优先级经验回放</h2><p>第三个技巧称为优先级经验回放（prioritized experience replay， PER）。假设有一些数据，我们之前采样过，发现这些数据的时序差分误差特别大（时序差分误差就是网络的输出与目标之间的差距），这代表我们在训练网络的时候，这些数据是比较不好训练的。既然比较不好训练，就应该给它们比较大的概率被采样到，即给它优先权（priority）。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101626466.png" alt=""></p>
<h2 id="在蒙特卡洛方法和时序差分方法中取得平衡"><a href="#在蒙特卡洛方法和时序差分方法中取得平衡" class="headerlink" title="在蒙特卡洛方法和时序差分方法中取得平衡"></a>在蒙特卡洛方法和时序差分方法中取得平衡</h2><p>蒙特卡洛方法与时序差分方法各有优劣，因此我们可以在蒙特卡洛方法和时序差分方法中取得平衡，这个方法也被称为多步方法。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101628249.png" alt=""></p>
<h2 id="噪声网络"><a href="#噪声网络" class="headerlink" title="噪声网络"></a>噪声网络</h2><p>噪声网络是指，每一次在一个回合开始的时候，在智能体要与环境交互的时候，智能体使用 $Q$ 函数来采取动作， $Q$ 函数里面就是一个网络，我们在网络的每一个参数上加上一个高斯噪声（Gaussian noise），就把原来的 $Q$ 函数变成 $\widetilde{Q}$ 。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101630416.png" alt=""></p>
<p>这里要注意，在每个回合开始的时候，与环境交互之前，我们就采样噪声。接下来我们用固定的噪声网络玩游戏，直到游戏结束，才重新采样新的噪声，噪声在一个回合中是不能被改变的。 </p>
<h2 id="分布式Q函数"><a href="#分布式Q函数" class="headerlink" title="分布式Q函数"></a>分布式Q函数</h2><p>分布式 $Q$ 函数是对分布（distribution）建模，怎么做呢？如图a所示，在原来的 $Q$ 函数里面，假设我们只能采取 $a_{1}$、 $a_{2}$、 $a_{3}$ 这 $3$ 个动作，我们输入一个状态，输出 $3$ 个值。这 $3$ 个值分别代表 $3$ 个动作的 $Q$ 值，但是这些 $Q$ 值是一个分布的期望值。所以分布式 $Q$ 函数就是直接输出分布。实际上的做法如图b所示，假设分布的值就分布在某一个范围里面，比如 $−10 \sim 10$，把 $−10 \sim 10$ 拆成一个一个的长条。例如，每一个动作的奖励空间拆成 $5$ 个长条。假设奖励空间可以拆成 $5$ 个长条， $Q$ 函数的输出就是要预测我们在某一个状态采取某一个动作得到的奖励，其落在某一个长条里面的概率。所以绿色长条概率的和应该是 $1$，其高度代表在某一个状态采取某一个动作的时候，它落在某一个长条内的概率。绿色的代表动作 $a_{1}$，红色的代表动作 $a_{2}$，蓝色的代表动作 $a_{3}$。所以我们就可以用 $Q$ 函数去估计 $a_{1}$ 的分布、 $a_{2}$ 的分布、 $a_{3}$ 的分布。实际上在做测试的时候，我们选平均值最大的动作执行。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101638394.png" alt=""></p>
<h2 id="彩虹"><a href="#彩虹" class="headerlink" title="彩虹"></a>彩虹</h2><p>最后一个技巧称为彩虹（rainbow），如图所示，假设每个方法有一种自己的颜色（如果每一个单一颜色的线代表只用某一个方法），把所有的颜色组合起来，就变成“彩虹”，我们把原来的深度 $Q$ 网络也算作一种方法，故有 $7$ 种颜色。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101640614.png" alt=""></p>
<h1 id="针对连续动作的深度Q网络"><a href="#针对连续动作的深度Q网络" class="headerlink" title="针对连续动作的深度Q网络"></a>针对连续动作的深度Q网络</h1><p>深度 $Q$ 网络其实存在一些问题，最大的问题是它很难处理连续动作。 </p>
<h2 id="方案-1：对动作进行采样"><a href="#方案-1：对动作进行采样" class="headerlink" title="方案 1：对动作进行采样"></a>方案 1：对动作进行采样</h2><p>我们可以采样出 $N$ 个可能的 $a： \{a_{1}, a_{2}, · · · , a_{N}\}$ ，把它们一个一个地代入 $Q$ 函数，看谁的 $Q$ 值最大。这个方案不会太低效，因为我们在运算的时候会使用 GPU，一次把 $N$ 个连续动作都代入 $Q$ 函数，一次得到 $N$ 个 $Q$ 值，看谁最大。当然这不是一个非常精确的方案，因为我们没有办法进行太多的采样，所以估计出来的 $Q$ 值、最后决定的动作可能不是非常精确。</p>
<h2 id="方案-2：梯度上升"><a href="#方案-2：梯度上升" class="headerlink" title="方案 2：梯度上升"></a>方案 2：梯度上升</h2><p>既然要解决的是一个优化问题（optimization problem），我们就要最大化目标函数（objective function）。要最大化目标函数，我们就可以用梯度上升。我们把 $a$ 当作参数，要找一组 $a$ 去最大化 $Q$ 函数，就用梯度上升去更新 $a$ 的值，最后看看能不能找到一个 $a$ 最大化 $Q$ 函数（目标函数）。但我们会遇到全局最大值（global maximum）的问题，不一定能够找到最优的结果，而且运算量显然很大，因为要迭代地更新 $a$，训练一个网络就很花时间了。如果我们使用梯度上升的方案来处理连续的问题，每次决定采取哪一个动作的时候，还要训练一次网络，显然运算量是很大的。</p>
<h2 id="方案-3：设计网络架构"><a href="#方案-3：设计网络架构" class="headerlink" title="方案 3：设计网络架构"></a>方案 3：设计网络架构</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101649780.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312101649026.png" alt=""></p>
<p>我们要怎么找到一个 $a$ 来最大化 $Q$ 值呢？因为 $(a −\mu(s))^{T}\Sigma(s)(a −\mu(s))$ 一定是正的，它前面有一个负号，假设我们不看负号，所以第一项 $(a − \mu(s))^{T}\Sigma(s)(a − \mu(s))$ 的值越小，最终的 $Q$ 值就越大。因为我们是把 $V(s)$ 减掉第一项，所以第一项的值越小，最后的 $Q$ 值就越大。怎么让第一项的值最小呢？我们直接令 $\mu(s)$ 等于 $a$，让第一项变成 $0$，就可以让第一项的值最小。因此，令 $\mu(s)$ 等于 $a$，我们就可以得到最大值，解决 $\arg \max$ 操作的问题就变得非常容易。</p>
<h2 id="方案-4：不使用深度-Q-网络"><a href="#方案-4：不使用深度-Q-网络" class="headerlink" title="方案 4：不使用深度 Q 网络"></a>方案 4：不使用深度 Q 网络</h2><p>第 4 个方案就是不使用深度 $Q$ 网络，用深度 $Q$ 网络处理连续动作是比较麻烦的。我们将基于策略的方法——PPO 和基于价值的方法——深度 $Q$ 网络结合在一起，就可以得到演员-评论员的方法。</p>
<h1 id="演员-评论员算法"><a href="#演员-评论员算法" class="headerlink" title="演员-评论员算法"></a>演员-评论员算法</h1><p>演员-评论员算法是一种结合策略梯度和时序差分学习的强化学习方法，其中，演员是指策略函数 $\pi_{\theta}(a|s)$，即学习一个策略以得到尽可能高的回报。评论员是指价值函数 $V_{\pi}(s)$，对当前策略的值函数进行估计，即评估演员的好坏。借助于价值函数，演员-评论员算法可以进行单步参数更新，不需要等到回合结束才进行更新。在演员-评论员算法里面，最知名的算法就是异步优势演员-评论员算法。如果我们去掉异步，则为优势演员-评论员（advantage actor-critic，A2C）算法。 A2C 算法又被译作优势演员-评论员算法。如果我们加了异步，变成异步优势演员-评论员算法。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404182238004.png" alt=""></p>
<h2 id="策略梯度回顾"><a href="#策略梯度回顾" class="headerlink" title="策略梯度回顾"></a>策略梯度回顾</h2><p>在更新策略参数 $\theta$ 的时候，我们可以通过下式来计算梯度：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112113051.png" alt=""></p>
<p>上式表示我们首先通过智能体与环境的交互，可以计算出在某一个状态 $s$ 采取某一个动作 $a$ 的概率 $p_{\theta}(a_{t}|s_{t})$。接下来，我们计算在某一个状态 $s$ 采取某一个动作 $a$ 之后直到游戏结束的累积奖励。</p>
<p>我们使用 $G$ 表示累积奖励， $G$ 是非常不稳定的。因为交互的过程本身具有随机性，所以在某一个状态 $s$ 采取某一个动作 $a$ 时计算得到的累积奖励，每次结果都是不同的，因此 $G$ 是一个随机变量。</p>
<h2 id="深度Q网络回顾"><a href="#深度Q网络回顾" class="headerlink" title="深度Q网络回顾"></a>深度Q网络回顾</h2><p>我们能不能让整个训练过程变得稳定，能不能直接估测随机变量 $G$ 的期望值？ 我们直接用一个网络去估测在状态 $s$ 采取动作 $a$ 时 $G$ 的期望值。如果这样是可行的，那么在随后的训练中我们就用期望值代替采样的值，这样就会让训练变得更加稳定。</p>
<p>怎么使用期望值代替采样的值呢？这里就需要引入基于价值的（value-based）的方法。基于价值的方法就是深度 $Q$ 网络。深度 $Q$ 网络有两种函数，有两种评论员。如下图所示，第一种评论员是 $V_{\pi}(s)$。即假设演员的策略是 $\pi$，使用 $\pi$ 与环境交互，当智能体看到状态 $s$ 时，接下来累积奖励的期望值是多少。第二种评论员是 $Q_{\pi}(s, a)$。 $Q_{\pi}(s, a)$ 把 $s$ 与 $a$ 当作输入，它表示在状态 $s$ 采取动作 $a$，接下来用策略 $\pi$ 与环境交互，累积奖励的期望值是多少。 $V_{\pi}$ 接收输入 $s$，输出一个标量。 $Q_{\pi}$ 接收输入 $s$，它会给每一个 $a$ 都分配一个 $Q$ 值。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112155776.png" alt=""></p>
<h2 id="优势演员-评论员算法"><a href="#优势演员-评论员算法" class="headerlink" title="优势演员-评论员算法"></a>优势演员-评论员算法</h2><p>如图所示，随机变量 $G$ 的期望值正好就是 $Q$ 值，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112156592.png" alt=""></p>
<p>此也为 $Q$ 函数的定义。 $Q$ 函数的定义就是在某一个状态 $s$，采取某一个动作 $a$，假设策略是 $\pi$ 的情况下所能得到的累积奖励的期望值，即 $G$ 的期望值。累积奖励的期望值就是 $G$ 的期望值。</p>
<p>有不同的方法表示基线，一个常见的方法是用价值函数 $V_{\pi_{\theta}}(s^{n}_{t})$ 来表示基线。价值函数的定义为，假设策略是 $\pi$，其在某个状态 $s$ 一直与环境交互直到游戏结束，期望奖励有多大。 $V_{\pi_{\theta}}(s^{n}_{t})$ 没有涉及动作， $Q_{\pi_{\theta}}(s^{n}_{t},a^{n}_{t})$ 涉及动作。$V_{\pi_{\theta}}(s^{n}_{t})$ 是 $Q_{\pi_{\theta}}(s^{n}_{t},a^{n}_{t})$ 的期望值， $Q_{\pi_{\theta}}(s^{n}_{t},a^{n}_{t})-V_{\pi_{\theta}}(s^{n}_{t})$ 会有正有负，所以 $\sum_{t^{\prime}=t}^{T_{n}}\gamma^{t^{\prime}-t}r_{t^{\prime}}^{n}-b$ 这一项就会有正有负。所以我们就把策略梯度里面 $\sum_{t^{\prime}=t}^{T_{n}}\gamma^{t^{\prime}-t}r_{t^{\prime}}^{n}-b$ 这一项换成了优势函数 $A^{\theta}(s^{n}_{t},a^{n}_{t})$，即 $Q_{\pi_{\theta}}(s^{n}_{t},a^{n}_{t})-V_{\pi_{\theta}}(s^{n}_{t})$。因此该算法称为优势演员-评论员算法。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112225437.png" alt=""></p>
<p>如果我们这么实现，有一个缺点，即我们需要估计两个网络——$Q$ 网络和 $V$ 网络，估计不准的风险就变成原来的两倍。事实上，在演员-评论员算法中，我们可以只估计网络 $V$，并利用 $V$ 的值来表示 $Q$ 的值， $Q_{\pi}(s^{n}_{t},a^{n}_{t})$ 可以写成 $r_{t}^{n}+V_{\pi}(s^{n}_{t+1})$ 的期望值，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112228101.png" alt=""></p>
<p>在状态 $s$ 采取动作 $a$，我们会得到奖励 $r$，进入状态 $s_{t+1}$。但是我们会得到什么样的奖励 $r$，进入什么样的状态 $s_{t+1}$，这件事本身是有随机性的。所以要把 $r_{t}^{n}+V_{\pi}(s^{n}_{t+1})$ 取期望值才会等于 $Q$ 函数的值。<strong>但我们现在把取期望值去掉</strong>，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112238182.png" alt=""></p>
<p>我们就可以把 $Q$ 函数的值用 $r_{t}^{n}+V_{\pi}(s^{n}_{t+1})$ 取代，可得</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112240086.png" alt=""></p>
<p>把取期望值去掉的好处就是我们不需要估计 $Q$ 了，只需要估计 $V$ 。但与此同时我们会引入一个随机的参数 $r$。 $r$ 是有随机性的，它是一个随机变量，但是 $r$ 相较于累积奖励 $G$ 是一个较小的值，因为它是某一个步骤得到的奖励，而 $G$ 是所有未来会得到的奖励的总和， $G$ 的方差比较大。 $r$ 虽然也有一些方差，但它的方差比 $G$ 的要小。所以把原来方差比较大的 $G$ 换成方差比较小的 $r$ 也是合理的。优势演员-评论员算法的流程如图所示。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112243886.png" alt=""></p>
<p>我们有一个 $\pi$，有个初始的演员与环境交互，先收集资料。在策略梯度方法里收集资料以后，就来更新策略。但是在演员-评论员算法里面，我们不是直接使用那些资料来更新策略。我们先用这些资料去估计价值函数，可以用时序差分方法或蒙特卡洛方法来估计价值函数。接下来，我们再基于价值函数，使用下式更新 $\pi$。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112244924.png" alt=""></p>
<p>实现优势演员-评论员算法的时候，有两个一定会用到的技巧。第一个技巧是，我们需要估计两个网络： $V$ 网络和策略的网络（也就是演员）。评论员网络 $V_{\pi}(s)$ 接收一个状态，输出一个标量。演员的策略 $\pi(s)$ 接收一个状态，如果动作是离散的，输出就是一个动作的分布，如果动作是连续的，输出就是一个连续的向量。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312112250310.png" alt=""></p>
<p>第二个技巧是我们需要探索的机制。在演员-评论员算法中，有一个常见的探索的方法是对 $\pi$ 输出的分布设置一个约束。这个约束用于使分布的熵（entropy）不要太小，也就是希望不同的动作被采用的概率平均一些。这样在测试的时候，智能体才会多尝试各种不同的动作，才会把环境探索得比较好，从而得到比较好的结果。</p>
<h3 id="REINFORCE和A2C的区别"><a href="#REINFORCE和A2C的区别" class="headerlink" title="REINFORCE和A2C的区别"></a>REINFORCE和A2C的区别</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404182204930.png" alt=""></p>
<h2 id="异步优势演员-评论员算法"><a href="#异步优势演员-评论员算法" class="headerlink" title="异步优势演员-评论员算法"></a>异步优势演员-评论员算法</h2><p>异步优势演员-评论员算法同时使用很多个进程（worker），每一个进程就像一个影分身，最后这些影分身会把所有的经验值集合在一起。如果我们没有很多 CPU，不好实现异步优势演员-评论员算法，但可以实现优势演员-评论员算法。</p>
<p>异步优势演员-评论员算法一开始有一个全局网络（global network）。全局网络包含策略网络和价值网络，这两个网络是绑定在一起的，它们的前几个层会被绑在一起。假设全局网络的参数是 $\theta_{1}$，我们使用多个进程，每个进程用一张 CPU 去跑。接下来演员就与环境交互，每一个演员与环境交互的时候，都要收集到比较多样的数据。每一个演员与环境交互完之后，我们就会计算出梯度。计算出梯度以后，要用梯度去更新参数。我们就计算一下梯度，用梯度去更新全局网络的参数。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161337398.png" alt=""></p>
<h2 id="路径衍生策略梯度"><a href="#路径衍生策略梯度" class="headerlink" title="路径衍生策略梯度"></a>路径衍生策略梯度</h2><p>这个方法可以看成深度 $Q$ 网络解连续动作的一种特别的方法，也可以看成一种特别的演员-评论员的方法。一般的演员-评论员算法的评论员就是输入状态或输入状态-动作对，给演员一个值，所以对演员来说，它只知道它做的这个动作到底是好还是不好。但在路径衍生策略梯度里面，评论员会直接告诉演员采取什么样的动作才是好的。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161356657.png" alt=""></p>
<p>路径衍生策略梯度算法如图所示，假设我们学习了一个 $Q$ 函数， $Q$ 函数的输入是 $s$ 与 $a$，输出是 $Q^{\pi}(s, a)$。接下来，我们要学习一个演员，这个演员的工作就是解决 $\arg \max$ 的问题，即输入一个状态 $s$，希望可以输出一个动作 $a$。 $a$ 被代入 $Q$ 函数以后，它可以让 $Q^{\pi}(s,a)$ 尽可能大，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161356414.png" alt=""></p>
<h2 id="与生成对抗网络的联系"><a href="#与生成对抗网络的联系" class="headerlink" title="与生成对抗网络的联系"></a>与生成对抗网络的联系</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161405257.png" alt=""></p>
<h1 id="稀疏奖励"><a href="#稀疏奖励" class="headerlink" title="稀疏奖励"></a>稀疏奖励</h1><p>如果环境中的奖励非常稀疏，强化学习的问题就会变得非常困难，但是人类可以在非常稀疏的奖励上去学习。人生通常多数的时候，就只是活在那里，都没有得到什么奖励或是惩罚。但是，人还是可以采取各种各样的行为。所以，一个真正厉害的人工智能应该能够在稀疏奖励的情况下也学到怎么与环境交互。</p>
<h2 id="设计奖励"><a href="#设计奖励" class="headerlink" title="设计奖励"></a>设计奖励</h2><p>第一个方向是设计奖励（reward shaping）。环境有一个固定的奖励，它是真正的奖励，但是为了让智能体学到的结果是我们想要的，所以我们刻意设计了一些奖励来引导智能体。</p>
<p>如果我们把小孩当成一个智能体，他可以采取两个动作：玩耍或者学习。如果他玩耍，在下一个时间点就会得到奖励 $1$。但是他在月考的时候，成绩可能会很差，所以在 $100$ 个小时之后，他会得到奖励 $−100$。他也可以决定要学习，在下一个时间点，因为他没有玩耍，所以觉得很不爽，所以得到奖励 $−1$。但是在 $100$ 个小时后，他可以得到奖励 $100$。对于一个小孩来说，他可能就会想要采取玩耍的动作而不是学习的动作。我们计算的是累积奖励，但也许对小孩来说，折扣因子会很大，所以他就不太在意未来的奖励。而且因为他是一个小孩，还没有很多经验，所以他的 $Q$ 函数估计是非常不精准的。所以要他去估计很远以后会得到的累积奖励，他是估计不出来的。这时候大人就要引导他，对他说：“如果你学习，我就给你一根棒棒糖。”对小孩来说，下一个时间点他得到的奖励就变成正的，他也许就会认为学习是比玩耍好的。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161414768.png" alt=""></p>
<p>设计奖励是有问题的，因为我们需要领域知识（domain knowledge）。例如，如图所示，机器人想要学会把蓝色的板子从柱子穿过。机器人很难学会，我们可以设计奖励。一个貌似合理的说法是，蓝色的板子离柱子越近，奖励越大。但是机器人靠近的方式会有问题，它会用蓝色的板子打柱子。而机器人要把蓝色板子放在柱子上面，才能让蓝色板子穿过柱子。因此，这种设计奖励的方式是有问题的。至于哪种设计奖励的方式有问题，哪种设计奖励的方式没问题，会变成一个领域知识，是我们要去调整的。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161417520.png" alt=""></p>
<h2 id="好奇心"><a href="#好奇心" class="headerlink" title="好奇心"></a>好奇心</h2><p>接下来介绍各种我们可以自己加入并且一般看起来是有用的奖励。例如，一种技术是给智能体加上好奇心（curiosity），称为好奇心驱动的奖励（curiosity driven reward）。如图所示，我们输入某个状态和某个动作到奖励函数中，奖励函数就会输出在这个状态采取这个动作会得到的奖励，总奖励越大越好。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161420949.png" alt=""></p>
<p>在好奇心驱动的技术里面，我们会加上一个新的奖励函数——内在好奇心模块（intrinsic curiosity module， ICM） ，它用于给智能体加上好奇心。内在好奇心模块需要 3 个输入：状态 $s_{1}$、动作 $a_{1}$ 和状态 $s_{2}$。根据输入，它会输出另外一个奖励 $r_{1}^{i}$。对智能体来说，总奖励并不是只有 $r$，还有 $r^{i}$。它不是只把所有的 $r$ 都加起来，它还把所有 $r^{i}$ 加起来当作总奖励。所以在与环境交互的时候，它不是只希望 $r$ 越大越好，它还同时希望 $r^{i}$ 越大越好，它希望从内在好奇心模块里面得到的奖励越大越好。内在好奇心模块代表一种好奇心。</p>
<p>在内在好奇心模块里面，我们有一个网络，这个网络会接收输入 $a_{t}$ 与 $s_{t}$，输出 $\hat{s}_{t+1}$，也就是这个网络根据 $a_{t}$ 和 $s_{t}$ 去预测 $\hat{s}_{t+1}$。然后再看这个网络的预测 $\hat{s}_{t+1}$ 与真实的情况 $s_{t+1}$ 的相似度，越不相似得到的奖励就越大。所以奖励 $r_{t}^{i}$ 的意思是，未来的状态越难被预测，得到的奖励就越大。这就是鼓励智能体去冒险、去探索，现在采取这个动作，未来会发生什么越难被预测，这个动作的奖励就越大。</p>
<p>怎么让智能体知道什么事情是真正重要的呢？我们要加上另外一个模块，我们要学习一个特征提取器（feature extractor）。如图所示，黄色的格子代表特征提取器，它输入一个状态，输出一个特征向量来代表这个状态，我们期待特征提取器可以把没有意义的画面，状态里面没有意义的东西过滤掉，比如风吹草动、白云的飘动以及树叶飘动。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161438521.png" alt=""></p>
<h2 id="课程学习"><a href="#课程学习" class="headerlink" title="课程学习"></a>课程学习</h2><p>第二个方向是课程学习（curriculum learning） 。具体来说，课程学习是指我们为智能体的学习做规划，给他“喂”的训练数据是有顺序的，通常都是由简单到难的。</p>
<p>有一个比较通用的方法： 逆向课程生成（reverse curriculum generation）。我们可以用一个比较通用的方法来帮智能体设计课程。假设我们一开始有一个状态 $s_{g}$，这是黄金状态（gold state），也就是最后最理想的结果。接下来我们根据黄金状态去找其他的状态，这些其他的状态与黄金状态是比较接近的，记为 $s_{1}$。接下来，智能体再从 $s_{1}$ 开始与环境交互，看它能不能够达到黄金状态 $s_{g}$，在每一个状态下，智能体与环境交互的时候，都会得到一个奖励。接下来，我们把奖励特别极端的情况去掉。奖励特别极端的情况的意思是这些情况太简单或是太难了。接下来，再根据这些奖励适中的情况采样出更多的状态。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161451592.png" alt=""></p>
<h2 id="分层强化学习"><a href="#分层强化学习" class="headerlink" title="分层强化学习"></a>分层强化学习</h2><p>第三个方向是分层强化学习（hierarchical reinforcement learning， HRL）。分层强化学习是指，我们有多个智能体，一些智能体负责比较高级的东西，它们负责定目标，定完目标以后，再将目标分配给其他的智能体，让其他智能体来执行目标。</p>
<p>分层强化学习是指将一个复杂的强化学习问题分解成多个小的、简单的子问题，每个子问题都可以单独用马尔可夫决策过程来建模。这样，我们可以将智能体的策略分为高层次策略和低层次策略，高层次策略根据当前状态决定如何执行低层次策略。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202312161456145.png" alt=""></p>
<h1 id="模仿学习"><a href="#模仿学习" class="headerlink" title="模仿学习"></a>模仿学习</h1><p>模仿学习（imitation learning，IL） 讨论的问题是，假设我们连奖励都没有，要怎么进行更新以及让智能体与环境交互呢？</p>
<h2 id="行为克隆"><a href="#行为克隆" class="headerlink" title="行为克隆"></a>行为克隆</h2><p>行为克隆与监督学习较为相似。专家做什么，智能体就做一模一样的事，这就称为行为克隆。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281429710.png" alt=""></p>
<p>行为克隆虽然非常简单，但它的问题是，如果我们只收集专家的示范，可能我们看过的观测以及状态是非常有限的。假设我们要学习自动驾驶一辆汽车通过弯道。如果是专家，它将顺着红线通过弯道。但假设智能体很笨，它开车的时候撞墙了，它永远不知道撞墙这种状况要怎么处理。因为训练数据里面从来没有撞墙相关的数据，所以它根本就不知道撞墙这种情况要怎么处理。打电玩也是一样的，让专家去玩《超级马里奥》，专家可能非常强，它从来不会跳不上水管，所以智能体根本不知道跳不上水管时要怎么处理。所以仅仅使用行为克隆是不够的，只观察专家的示范是不够的，还需要结合另一个方法：<strong>数据集聚合（dataset aggregation，DAgger）</strong>。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281431999.png" alt=""></p>
<p>我们希望收集更多样的数据，而不是只收集专家所看到的观测。我们希望能够收集专家在各种极端的情况下所采取的行为。如图所示，以自动驾驶汽车为例，一开始我们有演员 $\theta_{1}$，并且让其去驾驶这辆车，同时车上坐了一个专家。这个专家会不断地告诉智能体，如果在这个情境里面，我会怎么样开。所以 $\theta_{1}$ 自己开自己的，但是专家会不断地表达它的想法。比如，一开始的时候，专家可能说往前走。在拐弯的时候，专家可能就会说往右转。但 $\theta_{1}$ 是不管专家的指令的，所以它会继续撞墙。虽然专家说往右转，但是不管他怎么下指令都是没有用的， $\theta_{1}$ 会做自己的事情，因为我们要做的记录的是说，专家在 $\theta_{1}$ 看到这种观测的情况下，它会做什么样的反应。这个方法显然是有一些问题的，因为我们每开一次自动驾驶汽车就会牺牲一个专家。我们用这个方法，牺牲一个专家以后，就会知道，人类在快要撞墙的时候，会采取什么样的行为。再用这些数据训练新的演员 $\theta_{2}$，并反复进行这个过程，这个方法称为数据集聚合。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281436975.png" alt=""></p>
<p>行为克隆还有一个问题：智能体会完全模仿专家的行为，不管专家的行为是否有道理，就算没有道理，没有什么用，就算这是专家本身的习惯，智能体也会把它记下来。例如，如图所示，在学习中文的时候，老师有语音、行为和知识，但其实只有语音部分是重要的，知识部分是不重要的。也许智能体只能学一件事，如果它只学到了语音，没有问题。如果它只学到了手势，这样就有问题了。所以让智能体学习什么东西是需要模仿的、什么东西是不需要模仿的，这件事情是很重要的。而单纯的行为克隆没有学习这件事情，因为智能体只是复制专家所有的行为而已，它不知道哪些行为是重要的，是对接下来有影响的，哪些行为是不重要的、是对接下来没有影响的。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281438171.png" alt=""></p>
<p>行为克隆的问题还在于：我们使用行为克隆的时候，训练数据与测试数据是不匹配的。我们可以用数据集聚合的方法来缓解这个问题。</p>
<h2 id="逆强化学习"><a href="#逆强化学习" class="headerlink" title="逆强化学习"></a>逆强化学习</h2><p>为什么叫逆强化学习？因为原来的强化学习里，有一个环境和一个奖励函数。如图所示，根据环境和奖励函数，通过强化学习这一技术，我们会找到一个演员，并会学习出一个最优演员。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281441117.png" alt=""></p>
<p>但逆强化学习刚好是相反的，如图所示，它没有奖励函数，只有一些专家的示范，但还是有环境的。逆强化学习假设现在有一些专家的示范，用 $\widehat{\tau}$ 来代表专家的示范。如果是在玩电玩，每一个 $\tau$ 就是一个很会玩电玩的人玩一场游戏的记录。如果是自动驾驶汽车，就是人开自动驾驶汽车的记录。这些就是专家的示范，每一个 $\tau$ 是一个轨迹。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281457194.png" alt=""></p>
<p>把所有专家的示范收集起来，再使用逆强化学习这一技术。使用逆强化学习技术的时候，智能体是可以与环境交互的。但它得不到奖励，它的奖励必须从专家那里推出来。有了环境和专家的示范以后，可以反推出奖励函数。强化学习是由奖励函数反推出什么样的动作、演员是最好的。逆强化学习则反过来，我们有专家的示范，我们相信它是不错的，我就反推，专家是因为什么样的奖励函数才会采取这些行为。有了奖励函数以后，接下来，我们就可以使用一般的强化学习的方法去找出最优演员。所以逆强化学习是先找出奖励函数，找出奖励函数以后，再用强化学习找出最优演员。</p>
<p>逆强化学习实际上是怎么做的呢？如图所示，首先，我们有一个专家 $\widehat{ \theta}$，这个专家与环境交互，产生很多轨迹 $\{\hat{\tau}_{1}, \hat{\tau}_{2}, \cdots, \hat{\tau}_{N}\}$。如果我们玩游戏，就让某个电玩高手去玩 $N$ 场游戏，把 $N$ 场游戏的状态与动作的序列都记录下来。接下来，我们有一个演员 $\theta$，一开始演员很烂，这个演员也与环境交互。它也去玩了 $N$ 场游戏，它也有 $N$ 场游戏的记录。接下来，我们要反推出奖励函数。怎么反推出奖励函数呢？原则就是专家永远是最棒的，是先射箭，再画靶的概念。专家去玩一玩游戏，得到这些游戏的记录，演员也去玩一玩游戏，得到这些游戏的记录。接下来，我们要定一个奖励函数，这个奖励函数的原则就是专家得到的分数要比演员得到的分数高（先射箭，再画靶），所以我们就学习出一个奖励函数，这个奖励函数会使专家得到的奖励大于演员得到的奖励。有了新的奖励函数以后，我们就可以使用一般强化学习的方法学习一个演员，这个演员会针对奖励函数最大化它的奖励。它也会采取一些的动作。但是这个演员虽然可以最大化奖励函数，采取大量的动作，得到大量游戏的记录。<br>但接下来，我们更改奖励函数。这个演员就会很生气，它已经可以在这个奖励函数得到高分。但是它得到高分以后，我们就改奖励函数，仍然让专家可以得到比演员更高的分数。这就是逆强化学习。有了新的奖励函数以后，根据这个新的奖励函数，我们就可以得到新的演员，新的演员再与环境交互。它与环境交互以后，我们又会重新定义奖励函数，让专家得到的奖励比演员的大。<br>怎么让专家得到的奖励大过演员呢？如图所示，我们在学习的时候，奖励函数也许就是神经网络。神经网络的输入为 $\tau$，输出就是应该要给 $\tau$ 的分数。或者假设我们觉得输入整个 $\tau$ 太难了，因为 $\tau$ 是 $s$ 和 $a$ 的一个很长的序列。也许就向它输入一个 $s$ 和 $a$ 的对，它会输出一个实数。把整个 $\tau$ 会得到的实数加起来就得到 $R(\tau)$。在训练的时候，对于 $\{\hat{\tau}_{1}, \hat{\tau}_{2}, \cdots, \hat{\tau}_{N}\}$，我们希望它输出的 $R$ 值越大越好。对于$\{\tau_{1}, \tau_{2}, \cdots, \tau_{N}\}$，我们就希望 $R$ 值越小越好。<br>什么可以被称为一个最好的奖励函数呢？最后我们学习出来的奖励函数应该是专家和演员在这个奖励函数上都会得到一样高的分数。最终的奖励函数无法分辨出谁应该会得到比较高的分数。通常在训练的时候，我们会迭代地去做。最早的逆强化学习对奖励函数有些限制，它是假设奖励函数是线性的（linear）。如果奖励函数是线性，我们可以证明这个算法会收敛（converge）。但是如果奖励函数不是线性的，我们就无法证明它会收敛。</p>
<p>逆强化学习的框架如下图所示，其实我们只要把逆强化学习中的演员看成生成器，把奖励函数看成判别器，它就是生成对抗网络。所以逆强化学习会不会收敛就等于生成对抗网络会不会收敛。如果我们已经实现过，就会知道逆强化学习不一定会收敛。但除非我们对 $R$ 执行一个非常严格的限制，否则如果 $R$ 是一个一般的网络，我们就会有很大的麻烦。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281524563.png" alt=""></p>
<p>我们可以把逆强化学习与生成对抗网络详细地比较一下。如图所示，在生成对抗网络里面，我们有一系列很好的图、一个生成器和一个判别器。一开始，生成器不知道要产生什么样的图，它就会乱画。判别器的工作就是给画的图打分，专家画的图得高分，生成器画的图得低分。生成器会想办法去骗过判别器，生成器希望判别器也给它画的图打高分。整个过程与逆强化学习是一模一样的。专家画的图就是专家的示范。生成器就是演员，生成器画很多图，演员与环境交互，产生很多轨迹。演员与环境交互的记录其实就等价于生成对抗网络里面的这些图。然后我们学习一个奖励函数。奖励函数就是判别器。奖励函数要给专家的示范打高分，给演员交互的结果打低分。接下来，演员会想办法，从已经学习出的奖励函数中得到高分，然后迭代地循环。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281526816.png" alt=""></p>
<h2 id="第三人称视角模仿学习"><a href="#第三人称视角模仿学习" class="headerlink" title="第三人称视角模仿学习"></a>第三人称视角模仿学习</h2><p>机器人必须了解到当它是第三人称视角的时候，看到另外一个人在打高尔夫球，与它实际上自己去打高尔夫球的视角显然是不一样的。但它怎么把它是第三人视角所观察到的经验泛化到它是第一人称视角的时候所采取的行为，这就需要用到第三人称视角模仿学习（third person imitation learning）技术。</p>
<p>第三人称视角模仿学习技术其实不只用到了模仿学习，它还用到了领域对抗训练（domainadversarial training）。领域对抗训练也是一种生成对抗网络的技术。我们希望有一个特征提取器，有两幅不同领域（domain）的图像，通过特征提取器以后，无法分辨出图像来自哪一个领域。第一人称视角和第三人称视角模仿学习用的技术是一样的，希望学习一个特征提取器，智能体在第三人称的时候与它在第一人称的时候的视角其实是一样的，就是把最重要的东西抽出来就好了。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281532119.png" alt=""></p>
<h2 id="序列生成和聊天机器人"><a href="#序列生成和聊天机器人" class="headerlink" title="序列生成和聊天机器人"></a>序列生成和聊天机器人</h2><p>我们其实可以把句子生成（sentence generation）或聊天机器人理解为模仿学习。如图所示，机器在模仿人写句子，我们在写句子的时候，将写下的每一个字都想成一个动作，所有的字合起来就是一个回合。例如，句子生成里面，我们会给机器看很多人类写的字。如果要让机器学会写诗，就要给它看唐诗三百首。人类写的字其实就是专家的示范。每一个词汇其实就是一个动作。机器做句子生成的时候，其实就是在模仿专家的轨迹。聊天机器人也是一样的，在聊天机器人里面我们会收集到很多人交互对话的记录，这些就是专家的示范。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281535491.png" alt=""></p>
<p>如果我们单纯用最大似然（maximum likelihood）这个技术来最大化会得到似然（likelihood），这其实就是行为克隆。行为克隆就是看到一个状态，接下来预测我们会得到什么样的动作，有一个标准答案（ground truth）告诉机器什么样的动作是最好的。在做似然的时候也是一样的，给定句子已经产生的部分，接下来机器要预测写哪一个字才是最好的。所以，其实最大似然在做序列生成（sequence generation）的时候，它对应到模仿学习里面就是行为克隆。只有最大似然是不够的，我们想要用序列生成对抗网络（sequence GAN）。其实序列生成对抗网络对应逆强化学习，逆强化学习就是一种生成对抗网络的技术。我们把逆强化学习的技术放在句子生成、聊天机器人里面，其实就是序列生成对抗网络与它的种种变形。</p>
<h2 id="模仿学习和离线强化学习的区别"><a href="#模仿学习和离线强化学习的区别" class="headerlink" title="模仿学习和离线强化学习的区别"></a>模仿学习和离线强化学习的区别</h2><ul>
<li>现有的一些Offline RL算法建立在标准的off-policy RL算法之上，这些算法倾向于优化某种形式的Bellman方程或TD差分误差；而IL算法则更多是监督学习技巧的利用（也有一些工作结合了强化学习的优化方法）</li>
<li>大多数IL问题假设有一个最优的或一个高性能的专家来提供数据；而Offline RL可能需要从次优的数据中进行学习</li>
<li>大多数IL问题没有奖励（reward）的概念；而Offline RL需要显式考虑reward</li>
<li>一些IL问题要求数据被标记为专家经验和非专家经验，而Offline RL不做这个假设</li>
</ul>
<h1 id="深度确定性策略梯度"><a href="#深度确定性策略梯度" class="headerlink" title="深度确定性策略梯度"></a>深度确定性策略梯度</h1><h2 id="离散动作与连续动作的区别"><a href="#离散动作与连续动作的区别" class="headerlink" title="离散动作与连续动作的区别"></a>离散动作与连续动作的区别</h2><p>离散动作与连续动作是相对的概念，一个是可数的，一个是不可数的。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281853834.png" alt=""></p>
<p>对于这些连续的动作， $Q$ 学习、深度 $Q$ 网络等算法是没有办法处理的。那我们怎么输出连续的动作呢？这个时候，“万能”的神经网络又出现了。如图所示，在离散动作的场景下，比如我们输出上、下或是停止这几个动作。有几个动作，神经网络就输出几个概率值，我们用 $\pi_{\theta}(a_{t}|s_{t})$ 来表示这个随机性的策略。在连续的动作场景下，比如我们要输出机械臂弯曲的角度，我们就输出一个具体的浮点数。我们用 $\mu_{\theta}(s_{t})$ 来代表这个确定性的策略。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281859845.png" alt=""></p>
<p>我们再对随机性策略与确定性策略进行解释。对随机性策略来说，输入某一个状态 $s$，采取某一个动作的可能性并不是百分之百的，而是有一个概率的（就好像抽奖一样），根据概率随机抽取一个动作。而对于确定性策略来说，它不受概率的影响。当神经网络的参数固定之后，输入同样的状态，必然输出同样的动作，这就是确定性策略。</p>
<p>要输出离散动作，我们就加一个 $softmax$ 层来确保所有的输出是动作概率，并且所有的动作概率和为 $1$。要输出连续动作，我们一般可以在输出层加一层 $tanh$ 函数。$tanh$ 函数的作用就是把输出限制到 $[−1,1]$ 。我们得到输出后，就可以根据实际动作的范围将其缩放，再输出给环境。比如神经网络输出一个浮点数 $2.8$，经过 $tanh$ 函数之后，它就可以被限制在 $[−1,1]$ 之间，输出 $0.99$。假设小车速度的范围是 $[−2,2]$ ，我们就按比例从 $[−1,1]$ 扩大到 $[−2,2]$， $0.99$ 乘 $2$，最终输出的就是 $1.98$，将其作为小车的速度或者推小车的推力输出给环境。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281904680.png" alt=""></p>
<h2 id="深度确定性策略梯度-1"><a href="#深度确定性策略梯度-1" class="headerlink" title="深度确定性策略梯度"></a>深度确定性策略梯度</h2><p>在连续控制领域，比较经典的强化学习算法就是深度确定性策略梯度（deep deterministic policy gradient，DDPG）。</p>
<p>深度是因为用了神经网络；确定性表示<code>DDPG</code>输出的是一个确定性的动作，可以用于有连续动作的环境；策略梯度代表的是它用到的是策略网络。<code>REINFORCE</code>算法每隔一个回合就更新一次，但<code>DDPG</code>是每个步骤都会更新一次策略网络，它是一个单步更新的策略网络。</p>
<p><code>DDPG</code>是深度<code>Q</code>网络的一个扩展版本，可以扩展到连续动作空间。在<code>DDPG</code>的训练中，它借鉴了深度<code>Q</code>网络的技巧：目标网络和经验回放。经验回放与深度<code>Q</code>网络是一样的，但目标网络的更新与深度<code>Q</code>网络的有点儿不一样。提出<code>DDPG</code>是为了让深度<code>Q</code>网络可以扩展到连续的动作空间，就是我们刚才提到的小车速度、角度和电压等这样的连续值。如图所示，<code>DDPG</code>在深度<code>Q</code>网络基础上加了一个策略网络来直接输出动作值，所以<code>DDPG</code>需要一边学习<code>Q</code>网络，一边学习策略网络。<code>Q</code>网络的参数用 $w$ 来表示。策略网络的参数用 $\theta$ 来表示。我们称这样的结构为演员-评论员的结构。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281921911.png" alt=""></p>
<p>深度<code>Q</code>网络与 <code>DDPG</code> 的联系如图所示。深度<code>Q</code>网络的最佳策略是想要学出一个很好的<code>Q</code>网络，学出这个网络之后，我们希望选取的那个动作使<code>Q</code>值最大。<code>DDPG</code>的目的也是求解让<code>Q</code>值最大的那个动作。演员只是为了迎合评委的打分而已，所以优化策略网络的梯度就是要最大化这个<code>Q</code>值，所以构造的损失函数就是让<code>Q</code>取一个负号。我们写代码的时候把这个损失函数放入优化器里面，它就会自动最小化损失，也就是最大化<code>Q</code>。<br>这里要注意，除了策略网络要做优化，<code>DDPG</code>还有一个<code>Q</code>网络也要优化。评论员一开始也不知道怎么评分，它也是在一步一步的学习当中，慢慢地给出准确的分数。我们优化<code>Q</code>网络的方法其实与深度<code>Q</code>网络优化<code>Q</code>网络的方法是一样的，我们用真实的奖励 $r$ 和下一步的 $Q$ 即 $Q^{\prime}$ 来拟合未来的奖励 $Q_{target}$。然后让<code>Q</code>网络的输出逼近 $Q_{target}$。所以构造的损失函数就是直接求这两个值的均方差。构造好损失函数后，我们将其放到优化器中，让它自动最小化损失。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401281949961.png" alt=""></p>
<p>如图所示，我们可以把两个网络的损失函数构造出来。策略网络的损失函数是一个复合函数。我们把 $a = \mu_{\theta}(s)$ 代入，最终策略网络要优化的是策略网络的参数 $\theta$ 。<code>Q</code>网络要优化的是 $Q_{w}(s, a)$ 和 $Q_{target}$ 之间的一个均方差。但是<code>Q</code>网络的优化存在一个和深度<code>Q</code>网络一模一样的问题就是它后面的 $Q_{target}$ 是不稳定的。此外，后面的 $Q_{\overline{w}}(s^{\prime}, a^{\prime})$ 也是不稳定的，因为 $Q_{\overline{w}}(s^{\prime}, a^{\prime})$ 也是一个预估的值。<br>为了使 $Q_{target}$ 更加稳定，<code>DDPG</code>分别给<code>Q</code>网络和策略网络搭建了目标网络，即 $target_{Q}$ 网络和 $target_{P}$ 策略网络。 $target_{Q}$ 网络是为了计算 $Q_{target}$ 中 $Q_{\overline{w}}(s^{\prime}, a^{\prime})$。 $Q_{\overline{w}}(s^{\prime}, a^{\prime})$ 里面的需要的下一个动作 $a^{\prime}$ 是通过 $target_{P}$ 网络输出的，即 $a^{\prime} = \mu_{\overline{\theta}}(s^{\prime})$。<code>Q</code>网络和策略网络的参数是 $w$， $target_{Q}$ 网络和 $target_{P}$ 策略网络的参数是 $\overline{w}$。<code>DDPG</code>有 $4$ 个网络，策略网络的目标网络和<code>Q</code>网络的目标网络是颜色比较深的这两个，它们只是为了让计算 $Q_{target}$ 更稳定。因为这两个网络也是固定一段时间的参数之后再与评估网络同步最新的参数。</p>
<p>这里训练需要用到的数据就是 $s$、 $a$、 $r$、 $s^{\prime}$，我们只需要用到这 $4$ 个数据。我们用回放缓冲区把这些数据存起来，然后采样进行训练。经验回放的技巧与深度<code>Q</code>网络中的是一样的。注意，因为<code>DDPG</code>使用了经验回放技巧，所以<code>DDPG</code>是一个异策略的算法。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401282031786.png" alt=""></p>
<p><code>DDPG</code>通过异策略的方式来训练一个确定性策略。因为策略是确定的，所以如果智能体使用同策略来探索，在一开始的时候，它很可能不会尝试足够多的动作来找到有用的学习信号。为了让<code>DDPG</code>的策略更好地探索，我们在训练的时候给它们的动作加了噪声。<code>DDPG</code>的原作者推荐使用时间相关的<code>OU</code>噪声，但最近的结果表明不相关的、均值为 $0$ 的高斯噪声的效果非常好。由于后者更简单，因此我们更喜欢使用它。为了便于获得更高质量的训练数据，我们可以在训练过程中把噪声变小。在测试的时候，为了查看策略利用它学到的东西的表现，我们不会在动作中加噪声。</p>
<h2 id="双延迟深度确定性策略梯度"><a href="#双延迟深度确定性策略梯度" class="headerlink" title="双延迟深度确定性策略梯度"></a>双延迟深度确定性策略梯度</h2><p>虽然<code>DDPG</code>有时表现很好，但它对于超参数和其他类型的调整方面经常很敏感。<code>DDPG</code>常见的问题是已经学习好的<code>Q</code>函数开始显著地高估<code>Q</code>值，然后导致策略被破坏，因为它利用了<code>Q</code>函数中的误差。</p>
<p>双延迟深度确定性策略梯度（twin delayed DDPG，TD3）通过引入 $3$ 个关键技巧来解决这个问题。</p>
<ul>
<li>截断的双<code>Q</code>学习（clipped double Q-learning）。<code>TD3</code>学习两个<code>Q</code>函数（因此名字中有“twin”）。<code>TD3</code>通过最小化均方差来同时学习两个<code>Q</code>函数： $Q_{\phi_{1}}$ 和 $Q_{\phi_{2}}$。两个<code>Q</code>函数都使用一个目标，两个<code>Q</code>函数中给出的较小的值会被作为如下的 <code>Q-target</code>：</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401282042062.png" alt=""></p>
<ul>
<li>延迟的策略更新（delayed policy updates） 。相关实验结果表明，同步训练动作网络和评价网络，却不使用目标网络，会导致训练过程不稳定；但是仅固定动作网络时，评价网络往往能够收敛到正确的结果。<strong>因此TD3算法以较低的频率更新动作网络，以较高的频率更新评价网络，通常每更新两次评价网络就更新一次策略。</strong></li>
<li>目标策略平滑（target policy smoothing）。<code>TD3</code>引入了平滑化（smoothing）思想。<code>TD3</code>在目标动作中加入噪声，通过平滑<code>Q</code>沿动作的变化，使策略更难利用<code>Q</code>函数的误差。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401282045795.png" alt=""></p>
<h1 id="TRPO算法"><a href="#TRPO算法" class="headerlink" title="TRPO算法"></a>TRPO算法</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>回顾一下基于策略的方法：参数化智能体的策略，并设计衡量策略好坏的目标函数，通过梯度上升的方法来最大化这个目标函数，使得策略最优。具体来说，假设 $\theta$ 表示策略 $\pi_{\theta}$ 的参数，定义 $J(\theta)=\mathbb{E}_{s_0}[V^{\pi_\theta}(s_0)]=\mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^{\infty}\gamma^tr(s_t,a_t)\right]$，基于策略的方法的目标是找到 $\theta^*=\arg\max_\theta J(\theta)$，策略梯度算法主要沿着 $\nabla_{\theta}J(\theta)$ 方向迭代更新策略参数 $\theta$。但是这种算法有一个明显的缺点：当策略网络是深度模型时，沿着策略梯度更新参数，很有可能由于步长太长，策略突然显著变差，进而影响训练效果。</p>
<p>针对以上问题，我们考虑在更新时找到一块<strong>信任区域</strong>（trust region），在这个区域上更新策略时能够得到某种策略性能的安全性保证，这就是<strong>信任区域策略优化</strong>（trust region policy optimization，TRPO）算法的主要思想。TRPO 算法在 2015 年被提出，它在理论上能够保证策略学习的性能单调性，并在实际应用中取得了比策略梯度算法更好的效果。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191153648.png" alt=""></p>
<h2 id="策略目标"><a href="#策略目标" class="headerlink" title="策略目标"></a>策略目标</h2><p>假设当前策略为 $\pi_{\theta}$，参数为 $\theta$。我们考虑如何借助当前的 $\theta$ 找到一个更优的参数 $\theta^{\prime}$，使得 $J(\theta^{\prime})\geq J(\theta)$。具体来说，由于初始状态 $s_{0}$ 的分布和策略无关，因此上述策略 $\pi_{\theta}$ 下的优化目标 $J(\theta)$ 可以写成在新策略 $\pi_{\theta^{\prime}}$ 的期望形式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292223186.png" alt=""></p>
<p>基于以上等式，我们可以推导新旧策略的目标函数之间的差距：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292224383.png" alt=""></p>
<p>将时序差分残差定义为优势函数 $A$：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292230007.png" alt=""></p>
<p>最后一个等号的成立运用到了状态访问分布的定义： $\nu^\pi(s)=(1-\gamma)\sum_{t=0}^\infty\gamma^tP_t^\pi(s)$，所以只要我们能找到一个新策略，使得 $\mathbb{E}_{s\sim\nu^{\pi_{\theta^{\prime}}}}\mathbb{E}_{a\sim\pi_{\theta^{\prime}}(\cdot|s)}\left[A^{\pi_{\theta}}(s,a)\right]\geq0$，就能保证策略性能单调递增，即 $J(\theta^{\prime})\geq J(\theta)$。</p>
<p>但是直接求解该式是非常困难的，因为 $\pi_{\theta^{\prime}}$ 是我们需要求解的策略，但我们又要用它来收集样本。把所有可能的新策略都拿来收集数据，然后判断哪个策略满足上述条件的做法显然是不现实的。于是 TRPO 做了一步近似操作，对状态访问分布进行了相应处理。具体而言，忽略两个策略之间的状态访问分布变化，直接采用旧的策略 $\pi_{\theta}$ 的状态分布，定义如下替代优化目标：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292235376.png" alt=""></p>
<p>当新旧策略非常接近时，状态访问分布变化很小，这么近似是合理的。其中，动作仍然用新策略 $\pi_{\theta^{\prime}}$ 采样得到，我们可以用重要性采样对动作分布进行处理：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292237297.png" alt=""></p>
<p>这样，我们就可以基于旧策略 $\pi_{\theta}$ 已经采样出的数据来估计并优化新策略 $\pi_{\theta^{\prime}}$ 了。为了保证新旧策略足够接近，TRPO 使用了库尔贝克-莱布勒（Kullback-Leibler，KL）散度来衡量策略之间的距离，并给出了整体的优化公式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292239973.png" alt=""></p>
<p>这里的不等式约束定义了策略空间中的一个 KL 球，被称为信任区域。在这个区域中，可以认为当前学习策略和环境交互的状态分布与上一轮策略最后采样的状态分布一致，进而可以基于一步行动的重要性采样方法使当前学习策略稳定提升。</p>
<h2 id="近似求解"><a href="#近似求解" class="headerlink" title="近似求解"></a>近似求解</h2><p>直接求解上式带约束的优化问题比较麻烦，TRPO 在其具体实现中做了一步近似操作来快速求解。为方便起见，我们在接下来的式子中用 $\theta_{k}$ 代替之前的 $\theta$，表示这是第 $k$ 次迭代之后的策略。首先对目标函数和约束在 $\theta_{k}$ 进行泰勒展开，分别用 1 阶、2 阶进行近似：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292242571.png" alt=""></p>
<p>其中 $g=\nabla_{\theta^{\prime}}\mathbb{E}_{s\sim\nu^{\pi_{\theta_{k}}}}\mathbb{E}_{a\sim\pi_{\theta_{k}}(\cdot|s)}\left[\frac{\pi_{\theta^{\prime}}(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a)\right]$，表示目标函数的梯度， $H=\mathbf{H}[\mathbb{E}_{s\sim\nu^{\pi_{\theta_k}}}[D_{KL}(\pi_{\theta_k}(\cdot|s),\pi_{\theta^{\prime}}(\cdot|s))]]$ 表示策略之间平均 KL 距离的<strong>黑塞矩阵</strong>（Hessian matrix）。</p>
<p>于是我们的优化目标变成了：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292307106.png" alt=""></p>
<p>此时，我们可以用卡罗需-库恩-塔克（Karush-Kuhn-Tucker，KKT）条件直接导出上述问题的解：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292308216.png" alt=""></p>
<h2 id="共轭梯度"><a href="#共轭梯度" class="headerlink" title="共轭梯度"></a>共轭梯度</h2><p>一般来说，用神经网络表示的策略函数的参数数量都是成千上万的，计算和存储黑塞矩阵 $H$ 的逆矩阵会耗费大量的内存资源和时间。TRPO 通过共轭梯度法（conjugate gradient method）回避了这个问题，它的核心思想是直接计算 $x=H^{-1}g$， $x$ 即参数更新方向。假设满足 KL 距离约束的参数更新时的最大步长为 $\beta$，于是，根据 KL 距离约束条件，有 $\frac{1}{2}(\beta x)^{T}H(\beta x)=\delta $。求解 $\beta$，得到 $\beta=\sqrt{\frac{2\delta}{x^{T}Hx}}$。因此，此时参数更新方式为</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292312441.png" alt=""></p>
<p>因此，只要可以直接计算 $x=H^{-1}g$，就可以根据该式更新参数，问题转化为解 $Hx=g$。实际上 $H$ 为对称正定矩阵，所以我们可以使用共轭梯度法来求解。共轭梯度法的具体流程如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292317432.png" alt=""></p>
<p>在共轭梯度运算过程中，直接计算 $\alpha_{k}$ 和 $r_{k+1}$ 需要计算和存储海森矩阵 $H$。为了避免这种大矩阵的出现，我们只计算 $Hx$ 向量，而不直接计算和存储 $H$ 矩阵。这样做比较容易，因为对于任意的列向量 $v$，容易验证：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292319887.png" alt=""></p>
<p>即先用梯度和向量 $v$ 点乘后计算梯度。</p>
<h2 id="线性搜索"><a href="#线性搜索" class="headerlink" title="线性搜索"></a>线性搜索</h2><p>由于 TRPO 算法用到了泰勒展开的 1 阶和 2 阶近似，这并非精准求解，因此， $\theta^{\prime}$ 可能未必比 $\theta_{k}$ 好，或未必能满足 KL 散度限制。TRPO 在每次迭代的最后进行一次线性搜索（Line Search），以确保找到满足条件。具体来说，就是找到一个最小的非负整数 $i$，使得按照</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292321160.png" alt=""></p>
<p>求出的 $\theta_{k+1}$ 依然满足最初的 KL 散度限制，并且确实能够提升目标函数 $L_{\theta_{k}}$，这其中 $\alpha\in(0,1)$ 是一个决定线性搜索长度的超参数。</p>
<p>至此，我们已经基本上清楚了 TRPO 算法的大致过程，它具体的算法流程如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292324578.png" alt=""></p>
<h2 id="广义优势估计"><a href="#广义优势估计" class="headerlink" title="广义优势估计"></a>广义优势估计</h2><p>从上节中，我们尚未得知如何估计优势函数 $A$。目前比较常用的一种方法为广义优势估计（Generalized Advantage Estimation，GAE），接下来我们简单介绍一下 GAE 的做法。首先，用 $\delta_t=r_t+\gamma V(s_{t+1})-V(s_t)$ 表示时序差分误差，其中 $V$ 是一个已经学习的状态价值函数。于是，根据多步时序差分的思想，有：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292326710.png" alt=""></p>
<p>然后，GAE 将这些不同步数的优势估计进行指数加权平均：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202401292326672.png" alt=""></p>
<p>其中， $\lambda\in[0,1]$ 是在 GAE 中额外引入的一个超参数。当时 $\lambda=0$， $A_t^{GAE}=\delta_t=r_t+\gamma V(s_{t+1})-V(s_t)$，也即是仅仅只看一步差分得到的优势；当时 $\lambda=1$， $A_t^{GAE}=\sum_{l=0}^\infty\gamma^l\delta_{t+l}=\sum_{l=0}^\infty\gamma^lr_{t+l}-V(s_t)$，则是看每一步差分得到优势的完全平均值。</p>
<h1 id="SAC算法"><a href="#SAC算法" class="headerlink" title="SAC算法"></a>SAC算法</h1><p>柔性动作-评价（Soft Actor-Critic，SAC）算法的网络结构有5个。SAC算法解决的问题是离散动作空间和连续动作空间的强化学习问题，是<code>off-policy</code>的强化学习算法。</p>
<p>SAC的论文有两篇，一篇是《Soft Actor-Critic Algorithms and Applications》，它包括1个<code>Actor</code>网络，4个<code>Q Critic</code>网络。其具体流程如下：</p>
<ol>
<li>始化 Actor、Critic1、Critic2、TargetCritic1 、TargetCritic2 网络</li>
<li>Buffer中采样 (state, action, reward, next_state)</li>
<li>Actor 输入 next_state 对应输出 next_action 和 next_log_prob</li>
<li>Actor 输入 state 对应输出 new_action 和 log_prob</li>
<li>TargetCritic1 和 TargetCritic2 分别输入next_state 和 next_action 取其中较小输出经熵正则计算得 target_q_value</li>
<li>使用 MSE_loss(Critic1(state, action), target_q_value) 更新 Critic1</li>
<li>使用 MSE_loss(Critic2(state, action), target_q_value) 更新 Critic2</li>
<li>使用 (alpha * log_prob - critic1(state, new_action)).mean() 更新 Actor</li>
<li>软更新TargetCritic1和TargetCritic2</li>
</ol>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202505080048169.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202505080103535.png" alt=""></p>
<p>一篇是《Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor》，它包括1个<code>Actor</code>网络，2个<code>V Critic</code>网络（1个<code>V Critic</code>网络，1个<code>Target V Critic</code>网络），2个<code>Q Critic</code>网络。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202505080051698.png" alt=""></p>
<p>Q Critic网络的更新流程：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202505080109956.png" alt=""></p>
<p>V Critic网络的更新流程：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202505080109806.png" alt=""></p>
<p>Actor网络的更新流程，这里$Q_{0}$和$Q_{1}$是等价的：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202505080110232.png" alt=""></p>
<h1 id="多智能体强化学习"><a href="#多智能体强化学习" class="headerlink" title="多智能体强化学习"></a>多智能体强化学习</h1><h2 id="设定"><a href="#设定" class="headerlink" title="设定"></a>设定</h2><ul>
<li>合作（fully cooperative）</li>
<li>竞争（fully competitive）</li>
<li>合作竞争混合（mixed cooperative &amp; competitive）</li>
<li>利己主义（self-interest）</li>
</ul>
<h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191203225.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191203263.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191204290.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191205005.png" alt=""></p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><ul>
<li>完全去中心化</li>
<li>完全中心化</li>
<li>中心化训练，去中心化执行</li>
</ul>
<h3 id="Fully-Decentralized-Training"><a href="#Fully-Decentralized-Training" class="headerlink" title="Fully Decentralized Training"></a>Fully Decentralized Training</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191219908.png" alt=""></p>
<h3 id="Centralized-Training"><a href="#Centralized-Training" class="headerlink" title="Centralized Training"></a>Centralized Training</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191222752.png" alt=""></p>
<h3 id="Centralized-Training-with-Decentralized-Execution"><a href="#Centralized-Training-with-Decentralized-Execution" class="headerlink" title="Centralized Training with Decentralized Execution"></a>Centralized Training with Decentralized Execution</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191235939.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404191237313.png" alt=""></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%80%BB%E7%BB%93/" rel="tag"># 总结</a>
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># 强化学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/08/23/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" rel="prev" title="推荐系统">
                  <i class="fa fa-chevron-left"></i> 推荐系统
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/04/09/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" rel="next" title="大语言模型">
                  大语言模型 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">木霈玖</span>
</div>

    </div>
  </footer>

  
  <script src="//unpkg.com/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//unpkg.com/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
