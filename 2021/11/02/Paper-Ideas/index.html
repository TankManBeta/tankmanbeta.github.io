<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="前言记录一下读过论文的idea">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper Ideas">
<meta property="og:url" content="http://example.com/2021/11/02/Paper-Ideas/index.html">
<meta property="og:site_name" content="木霈玖的博客">
<meta property="og:description" content="前言记录一下读过论文的idea">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211104195000.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211104195040.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211104203300.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/swin_1.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/swin_2.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/image-20211128164821263.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/image-20211128222044474.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222142.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222208.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222342.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222510.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222753.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_1.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_2.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_3.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_4.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_5.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/174$P881EFD_5N]C69PU[M1.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309201330.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309202447.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309202721.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309204135.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309204238.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309210508.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310155509.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310165322.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310170640.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310171021.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310171036.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310180129.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310180331.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310180732.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310182117.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310182545.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311161000.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311163413.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311163508.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311163540.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311164311.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311164651.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311170307.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317183104.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317184032.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317184906.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317185502.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317192154.png">
<meta property="article:published_time" content="2021-11-02T13:40:57.000Z">
<meta property="article:modified_time" content="2022-03-17T11:24:35.878Z">
<meta property="article:author" content="木霈玖">
<meta property="article:tag" content="总结">
<meta property="article:tag" content="论文">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211104195000.png">


<link rel="canonical" href="http://example.com/2021/11/02/Paper-Ideas/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>Paper Ideas | 木霈玖的博客</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">木霈玖的博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Physics-Coupled-Spatio-Temporal-Active-Learning-for-Dynamical-Systems"><span class="nav-number">2.</span> <span class="nav-text">Physics-Coupled Spatio-Temporal Active Learning for Dynamical Systems</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ACTIVE-LEARNING-OF-DEEP-SURROGATES-FOR-PDES"><span class="nav-number">3.</span> <span class="nav-text">ACTIVE LEARNING OF DEEP SURROGATES FOR PDES</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows"><span class="nav-number">4.</span> <span class="nav-text">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Architecture"><span class="nav-number">4.1.</span> <span class="nav-text">Architecture</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Adversarial-Sampling-for-Solving-Differential-Equations-with-Neural-Networks"><span class="nav-number">5.</span> <span class="nav-text">Adversarial Sampling for Solving Differential Equations with Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">5.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Key-Idea"><span class="nav-number">5.2.</span> <span class="nav-text">Key Idea</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Architecture-1"><span class="nav-number">5.3.</span> <span class="nav-text">Architecture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Problem"><span class="nav-number">5.4.</span> <span class="nav-text">Problem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Solution"><span class="nav-number">5.5.</span> <span class="nav-text">Solution</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Machine-Learning-of-Linear-Differential-Equations-using-Gaussian-Processes"><span class="nav-number">6.</span> <span class="nav-text">Machine Learning of Linear Differential Equations using Gaussian Processes</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Priors"><span class="nav-number">6.1.</span> <span class="nav-text">Priors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kernels"><span class="nav-number">6.2.</span> <span class="nav-text">Kernels</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training"><span class="nav-number">6.3.</span> <span class="nav-text">Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Predictions"><span class="nav-number">6.4.</span> <span class="nav-text">Predictions</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Learning-Physics-Informed-Neural-Networks-without-Stacked-Back-propagation"><span class="nav-number">7.</span> <span class="nav-text">Learning Physics-Informed Neural Networks without Stacked Back-propagation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Problems"><span class="nav-number">7.1.</span> <span class="nav-text">Problems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Contribution"><span class="nav-number">7.2.</span> <span class="nav-text">Contribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Advantages"><span class="nav-number">7.3.</span> <span class="nav-text">Advantages</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Notice"><span class="nav-number">7.4.</span> <span class="nav-text">Notice</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Two-Sources-of-Inefficiency-In-Computing-the-PINN-Loss"><span class="nav-number">7.5.</span> <span class="nav-text">Two Sources of Inefficiency In Computing the PINN Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Method"><span class="nav-number">7.6.</span> <span class="nav-text">Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Back-propagation-free-Derivative-Estimators"><span class="nav-number">7.6.1.</span> <span class="nav-text">4.1 Back-propagation-free Derivative Estimators</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Proof"><span class="nav-number">7.6.1.1.</span> <span class="nav-text">Proof</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Model-Capacity"><span class="nav-number">7.6.2.</span> <span class="nav-text">4.2 Model Capacity</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Proof-1"><span class="nav-number">7.6.2.1.</span> <span class="nav-text">Proof</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Variance-Reduced-Stein%E2%80%99s-Derivative-Estimators"><span class="nav-number">7.6.3.</span> <span class="nav-text">4.3 Variance-Reduced Stein’s Derivative Estimators</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#The-control-variate-method"><span class="nav-number">7.6.3.1.</span> <span class="nav-text">The control variate method</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Further-improvement-using-the-antithetic-variable-method"><span class="nav-number">7.6.3.2.</span> <span class="nav-text">Further improvement using the antithetic variable method</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Neural-Galerkin-Scheme-with-Active-Learning-for-High-Dimensional-Evolution-Equations"><span class="nav-number">8.</span> <span class="nav-text">Neural Galerkin Scheme with Active Learning for High-Dimensional Evolution Equations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">8.1.</span> <span class="nav-text">Abstract</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#problems"><span class="nav-number">8.1.1.</span> <span class="nav-text">problems</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-1"><span class="nav-number">8.2.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Main-contributions"><span class="nav-number">8.2.1.</span> <span class="nav-text">Main contributions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Related-works"><span class="nav-number">8.2.2.</span> <span class="nav-text">Related works</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Galerkin-schemes"><span class="nav-number">8.3.</span> <span class="nav-text">Neural Galerkin schemes</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Neural-Galerkin"><span class="nav-number">8.3.1.</span> <span class="nav-text">Neural Galerkin</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Parametrizing-the-solution"><span class="nav-number">8.3.1.1.</span> <span class="nav-text">Parametrizing the solution</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Controlling-the-residual"><span class="nav-number">8.3.1.2.</span> <span class="nav-text">Controlling the residual</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural-Galerkin-equations"><span class="nav-number">8.3.1.3.</span> <span class="nav-text">Neural Galerkin equations</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Estimating-M-theta-and-F-t-theta"><span class="nav-number">8.3.2.</span> <span class="nav-text">Estimating $M(\theta)$ and $F(t, \theta)$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Importance-sampling-with-a-fixed-measure"><span class="nav-number">8.3.2.1.</span> <span class="nav-text">Importance sampling with a fixed measure</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Direct-sampling-with-an-adaptive-measure"><span class="nav-number">8.3.2.2.</span> <span class="nav-text">Direct sampling with an adaptive measure</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Discretization-in-time"><span class="nav-number">8.3.3.</span> <span class="nav-text">Discretization in time</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Explicit-integrators"><span class="nav-number">8.3.3.1.</span> <span class="nav-text">Explicit integrators</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Implicit-integrators"><span class="nav-number">8.3.3.2.</span> <span class="nav-text">Implicit integrators</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Neural-architectures"><span class="nav-number">8.3.4.</span> <span class="nav-text">Neural architectures</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AUTOIP-A-UNITED-FRAMEWORK-TO-INTEGRATE-PHYSICS-INTO-GAUSSIAN-PROCESSES"><span class="nav-number">9.</span> <span class="nav-text">AUTOIP: A UNITED FRAMEWORK TO INTEGRATE PHYSICS INTO GAUSSIAN PROCESSES</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-2"><span class="nav-number">9.1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Contribution-1"><span class="nav-number">9.1.1.</span> <span class="nav-text">Contribution</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gaussian-Process-Regression"><span class="nav-number">9.2.</span> <span class="nav-text">Gaussian Process Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model"><span class="nav-number">9.3.</span> <span class="nav-text">Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Algorithm"><span class="nav-number">9.4.</span> <span class="nav-text">Algorithm</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RESPECTING-CAUSALITY-IS-ALL-YOU-NEED-FOR-TRAINING-PHYSICS-INFORMED-NEURAL-NETWORKS"><span class="nav-number">10.</span> <span class="nav-text">RESPECTING CAUSALITY IS ALL YOU NEED FOR TRAINING PHYSICS-INFORMED NEURAL NETWORKS</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-3"><span class="nav-number">10.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Causal-training-for-physics-informed-neural-networks"><span class="nav-number">10.2.</span> <span class="nav-text">Causal training for physics-informed neural networks</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">木霈玖</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/02/Paper-Ideas/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="木霈玖">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="木霈玖的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Paper Ideas
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-11-02 21:40:57" itemprop="dateCreated datePublished" datetime="2021-11-02T21:40:57+08:00">2021-11-02</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-03-17 19:24:35" itemprop="dateModified" datetime="2022-03-17T19:24:35+08:00">2022-03-17</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">总结</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>14k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>记录一下读过论文的idea</p>
<a id="more"></a>
<h1 id="Physics-Coupled-Spatio-Temporal-Active-Learning-for-Dynamical-Systems"><a href="#Physics-Coupled-Spatio-Temporal-Active-Learning-for-Dynamical-Systems" class="headerlink" title="Physics-Coupled Spatio-Temporal Active Learning for Dynamical Systems"></a>Physics-Coupled Spatio-Temporal Active Learning for Dynamical Systems</h1><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211104195000.png" alt="framework"></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211104195040.png" alt="FN-PN"></p>
<ul>
<li>初始化<ul>
<li>选定 $n$ 个点 $\longrightarrow$ $\Omega^{n}_{temp}$</li>
<li>创建训练数据 $\longrightarrow$ $D_{temp}$ （选定 $n$ 个点都取 $T_{\omega}$ 时长的数据）</li>
</ul>
</li>
<li>训练<ul>
<li>learn $\lambda$ $\longleftarrow$ 最小化 $E_{q}$ ，偏微分方程</li>
<li>train ST-PCNN $\longleftarrow$ $\lambda$</li>
<li>predict $[\hat{s}]$ $\longleftarrow$ at all locations</li>
<li>$\Omega_{Kriging}^{n}$ $\longleftarrow$ $n$ 个：largest estimate error</li>
<li>$D_{Kriging}$ $\longleftarrow$ 上一步新选出的 $n$ 个，选取 $T_{\omega}$ 时长数据</li>
<li>更新 $D$</li>
</ul>
</li>
</ul>
<h1 id="ACTIVE-LEARNING-OF-DEEP-SURROGATES-FOR-PDES"><a href="#ACTIVE-LEARNING-OF-DEEP-SURROGATES-FOR-PDES" class="headerlink" title="ACTIVE LEARNING OF DEEP SURROGATES FOR PDES"></a>ACTIVE LEARNING OF DEEP SURROGATES FOR PDES</h1><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211104203300.png" alt="model"></p>
<p>algorithm: reduce number of training points(selected based on error measure)</p>
<p>AL: adding the most uncertain points</p>
<ul>
<li><p>Initialize:</p>
<p>random choose $n_{init}$ train 50 epochs $\longrightarrow$ $\widetilde{t^{0}}(p)$</p>
</li>
<li><p>Do T times:</p>
<ul>
<li>evaluate $\widetilde{t^{i}}(p)$ at $M×K$ points</li>
<li>choose $K$ points(largest $\sigma_{*}^{2}$ )</li>
<li>put the $K$ points into training set</li>
</ul>
</li>
</ul>
<h1 id="Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows"><a href="#Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows" class="headerlink" title="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"></a>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h1><p>Application：language $\longrightarrow$ vision</p>
<p>Challenges:</p>
<ul>
<li>scale(language Transformer: word tokens)</li>
<li>high resolution of pixels(计算复杂度： $n^{2}$ )</li>
</ul>
<p>Key point:</p>
<ul>
<li>小批量开始 $\longrightarrow$ 逐渐合并邻居</li>
<li>如何实现线性复杂度：在无重叠窗口计算自注意力<ul>
<li>standard transformer architecture: global self-attention $\longrightarrow$ quadratic complexity</li>
<li>Swin Transformer: local self-attention $\longrightarrow$ linear complexity</li>
</ul>
</li>
</ul>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><ul>
<li><p>Overall Framework</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/swin_1.png" alt=""></p>
</li>
<li><p>Two Successive Swin Transformer Blocks  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/swin_2.png" alt=""></p>
</li>
</ul>
<h1 id="Adversarial-Sampling-for-Solving-Differential-Equations-with-Neural-Networks"><a href="#Adversarial-Sampling-for-Solving-Differential-Equations-with-Neural-Networks" class="headerlink" title="Adversarial Sampling for Solving Differential Equations with Neural Networks"></a>Adversarial Sampling for Solving Differential Equations with Neural Networks</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>sample points adversarially to maximize the loss of the current solution estimate  </p>
<p>Advantages on using neural networks:</p>
<ul>
<li>instead of obtaining solution values at discretized points, we get a closed and differentiable solution function</li>
<li>it is more effective in solving high dimensional PDEs by faring better against the “curse of dimensionality” </li>
<li>numerical errors are not accumulated in each iteration</li>
<li>initial and boundary conditions are satisfied by construction</li>
</ul>
<p>Drawbacks  of using a predefined sampling scheme: agnostic to the equation being solved as well as our current estimate $\hat{y}$</p>
<h2 id="Key-Idea"><a href="#Key-Idea" class="headerlink" title="Key Idea"></a>Key Idea</h2><p>present a sampling scheme that is dependent on the current estimate $\hat{y}$, using a neural network to represent a variable sampling distribution.</p>
<p>In each iteration, the sampler is trained to <strong>produce points which maximize the loss of the solver (and a secondary loss). </strong></p>
<p>Thus, it competes with the solver whose weights are updated to minimize the loss at these very points.  </p>
<h2 id="Architecture-1"><a href="#Architecture-1" class="headerlink" title="Architecture"></a>Architecture</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/image-20211128164821263.png" alt=""></p>
<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>It is observed that if the sampler is purely optimized with the objective of maximizing $\hat{L}(\hat{y}; x)$(residual loss corresponding to the $DE$ at samples $x$), it tends to collapse all samples to one single point of high loss. </p>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>Therefore, use <strong>an additional loss term $D_{k}$,</strong>Given points$\begin{Bmatrix}x_{1},x_{2},\dots,x_{n}\end{Bmatrix}$, we define $d_{k}(x_{i})$ to be the sum of distances of $x_{i}$ from its $k$ nearest neighbors.</p>
<h1 id="Machine-Learning-of-Linear-Differential-Equations-using-Gaussian-Processes"><a href="#Machine-Learning-of-Linear-Differential-Equations-using-Gaussian-Processes" class="headerlink" title="Machine Learning of Linear Differential Equations using Gaussian Processes"></a>Machine Learning of Linear Differential Equations using Gaussian Processes</h1><p>Gaussian process priors are modified according to the particular form of such operators and are employed to infer parameters of the linear equations from scarce and possibly noisy observations.  </p>
<p>optimal model parameters and hyper-parameters are all learned directly from the data by maximizing the joint marginal log-likelihood of the probabilistic model instead of being guessed or tuned manually by the user.  </p>
<h2 id="Priors"><a href="#Priors" class="headerlink" title="Priors"></a>Priors</h2><p>place the $GP$ prior on $u(x)$ instead of $f(x)$ </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/image-20211128222044474.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222142.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222208.png" alt=""></p>
<h2 id="Kernels"><a href="#Kernels" class="headerlink" title="Kernels"></a>Kernels</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222342.png" alt=""></p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>employing a $Quasi-Newton$ optimizer $L-BFGS$ to minimize the negative log marginal likelihood</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222510.png" alt=""></p>
<h2 id="Predictions"><a href="#Predictions" class="headerlink" title="Predictions"></a>Predictions</h2><p>one can predict the values $u(x)$ and $f(x)$ at a new test point $x$ by writing the posterior distributions</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222753.png" alt=""></p>
<h1 id="Learning-Physics-Informed-Neural-Networks-without-Stacked-Back-propagation"><a href="#Learning-Physics-Informed-Neural-Networks-without-Stacked-Back-propagation" class="headerlink" title="Learning Physics-Informed Neural Networks without Stacked Back-propagation"></a>Learning Physics-Informed Neural Networks without Stacked Back-propagation</h1><h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><p>PINN training suffers from a significant scalability issue</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><ul>
<li>developing a novel approach to train the model without stacked back-propagation</li>
<li>parameterize the PDE solution $u(x; θ)$ as a Gaussian smoothed model, $u(x;\theta)=E_{\delta\thicksim\mathcal{N}(0,\sigma^{2}\mathbf{I})}f(x+\delta,\theta)$, where $u$ transforms arbitrary base network $f$ by injecting Gaussian noise into input $x$. This transformation gives rise to a key property for $u$ where its derivatives to the input can be efficiently calculated <em>without back-propagation</em>.<ul>
<li>Such property is derived from the well-known Stein’s Identity that essentially tells that the derivatives of any Gaussian smoothed function $u$ can be reformulated as some expectation terms of the output of its base $f$, which can be estimated using Monte Carlo methods. </li>
</ul>
</li>
<li>given any PDE problem, we can replace the derivative terms in the PDE with Stein’s<br>derivative estimators.</li>
</ul>
<h2 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h2><ol>
<li>no longer need stacked back-propagation to compute the loss</li>
<li>parallelize the computation into distributed machines to further accelerate the training </li>
</ol>
<h2 id="Notice"><a href="#Notice" class="headerlink" title="Notice"></a>Notice</h2><p>for large $\sigma$, the induced Gaussian smoothed models may not be expressive enough to approximate functions (i.e., learn solutions) with a large Lipschitz constant. Therefore, using a small value of $\sigma$ is usually a better choice in practice. However, a small $\sigma$ will lead to high-variance Stein’s derivative estimation, which inevitably causes unstable training.  </p>
<h2 id="Two-Sources-of-Inefficiency-In-Computing-the-PINN-Loss"><a href="#Two-Sources-of-Inefficiency-In-Computing-the-PINN-Loss" class="headerlink" title="Two Sources of Inefficiency In Computing the PINN Loss"></a>Two Sources of Inefficiency In Computing the PINN Loss</h2><ol>
<li>different orders of derivatives can only be calculated sequentially  </li>
<li>the dimension-level inefficiency </li>
</ol>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="4-1-Back-propagation-free-Derivative-Estimators"><a href="#4-1-Back-propagation-free-Derivative-Estimators" class="headerlink" title="4.1 Back-propagation-free Derivative Estimators"></a>4.1 Back-propagation-free Derivative Estimators</h3><p>define $u(x)=E_{\delta\thicksim\mathcal{N}(0,\sigma^{2}\mathbf{I})}f(x+\delta,\theta)$, then we have $\bigtriangledown_{x}u=E_{\delta\thicksim\mathcal{N}(0,\sigma^{2}\mathbf{I})}[\frac{\delta}{\sigma^{2}}f(x+\delta)]$</p>
<h4 id="Proof"><a href="#Proof" class="headerlink" title="Proof"></a>Proof</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_1.png" alt=""></p>
<p>From the above theorem, we can see that the first-order derivative rxu can be reformulated as an expectation term $E_{\delta\thicksim\mathcal{N}(0,\sigma^{2}\mathbf{I})}[\frac{\delta}{\sigma^{2}}f(x+\delta)]$, To calculate the value of the expectation, we can use Monte Carlo method to obtain an unbiased estimation from K.  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_2.png" alt=""></p>
<h3 id="4-2-Model-Capacity"><a href="#4-2-Model-Capacity" class="headerlink" title="4.2 Model Capacity"></a>4.2 Model Capacity</h3><p>For any measurable function $f : R^{d}\rightarrow R$, define $u(x)=E_{\delta\thicksim\mathcal{N}(0,\sigma^{2}\mathbf{I})}f(x+\delta,\theta)$, then<br>$u(x) $is $\frac{F}{\sigma}\sqrt{\frac{2}{\pi}}$-Lipschitz with respect to $l_{2}$-norm, where $F=sup_{x\in R^{d}}|f(x)|$.  </p>
<h4 id="Proof-1"><a href="#Proof-1" class="headerlink" title="Proof"></a>Proof</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_3.png" alt=""></p>
<h3 id="4-3-Variance-Reduced-Stein’s-Derivative-Estimators"><a href="#4-3-Variance-Reduced-Stein’s-Derivative-Estimators" class="headerlink" title="4.3 Variance-Reduced Stein’s Derivative Estimators"></a>4.3 Variance-Reduced Stein’s Derivative Estimators</h3><h4 id="The-control-variate-method"><a href="#The-control-variate-method" class="headerlink" title="The control variate method"></a>The control variate method</h4><p>One generic approach to reducing the variance of Monte Carlo estimates of integrals is to use an additive control variate, which is known as baseline.  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_4.png" alt=""></p>
<h4 id="Further-improvement-using-the-antithetic-variable-method"><a href="#Further-improvement-using-the-antithetic-variable-method" class="headerlink" title="Further improvement using the antithetic variable method"></a>Further improvement using the antithetic variable method</h4><p>The antithetic variable method is yet another powerful technique for variance reduction.  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_5.png" alt=""></p>
<h1 id="Neural-Galerkin-Scheme-with-Active-Learning-for-High-Dimensional-Evolution-Equations"><a href="#Neural-Galerkin-Scheme-with-Active-Learning-for-High-Dimensional-Evolution-Equations" class="headerlink" title="Neural Galerkin Scheme with Active Learning for High-Dimensional Evolution Equations"></a>Neural Galerkin Scheme with Active Learning for High-Dimensional Evolution Equations</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><h3 id="problems"><a href="#problems" class="headerlink" title="problems"></a>problems</h3><ol>
<li>no data are available  </li>
<li>the principal aim is to gather insights from a known model  </li>
</ol>
<p>高维逼近问题需要一个完全不同的“离线”自适应概念来规避维数的诅咒。</p>
<h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><p>develop time-integrators for PDEs that use DNNs to represent the solution but update the parameters sequentially from one time slice to another rather than globally over the whole time-space domain.  </p>
<p>use the structural form of the PDEs, but no a priori data about their solution.</p>
<p>leverage adaptivity in both function approximation and data acquisition.   </p>
<h3 id="Main-contributions"><a href="#Main-contributions" class="headerlink" title="Main contributions"></a>Main contributions</h3><ol>
<li><p>derive a nonlinear evolution equation for the parameters. </p>
<p>This equation can then be integrated using standard solvers with different level of sophistication.   </p>
<p>the proposed approach takes larger time steps when possible and corrects to smaller time-step sizes if the dynamics of the solution require it.  </p>
</li>
<li><p>The evolution equations that we derive for the DNN parameters involve operators that require estimation via sampling in space.  propose a dynamical estimation of the loss.   </p>
</li>
<li><p>We illustrate the viability and usefulness of our approach on a series of test cases.  </p>
</li>
</ol>
<h3 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h3><ol>
<li>The need for adaptive data acquisition in the context of machine learning for problems<br>in science and engineering has been emphasized in previous works.</li>
<li>There also is a large body of work on numerically solving PDEs with DNN parametrization based on collocation over the spatio-temporal domain.   </li>
<li>There also is a range of surrogate-modeling methods based on nonlinear parametrizations.  </li>
</ol>
<h2 id="Neural-Galerkin-schemes"><a href="#Neural-Galerkin-schemes" class="headerlink" title="Neural Galerkin schemes"></a>Neural Galerkin schemes</h2><h3 id="Neural-Galerkin"><a href="#Neural-Galerkin" class="headerlink" title="Neural Galerkin"></a>Neural Galerkin</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/174$P881EFD_5N]C69PU[M1.png" alt=""></p>
<h4 id="Parametrizing-the-solution"><a href="#Parametrizing-the-solution" class="headerlink" title="Parametrizing the solution"></a>Parametrizing the solution</h4><p>use ansatz $u(t,x)=U(\theta(t),x)$ , It is important to emphasize that U may depend nonlinearly on $\theta (t)$ , which is in stark contrast to the majority of classical approximations in scientific computing that have a linear dependence on the parameter.</p>
<h4 id="Controlling-the-residual"><a href="#Controlling-the-residual" class="headerlink" title="Controlling the residual"></a>Controlling the residual</h4><p>Since we do not have access to the solution $u(t)$ , we will use the structure of the governing equation to control the approximation error. To this end, note that inserting the ansatz solution $U(\theta(t))$ in Eq. (1). assuming differentiability of $\theta(t)$ and using $\partial_{t}U(\theta(t))=\triangledown_{\theta}U(\theta)\cdot \dot{\theta}(t)$ , leads to the residual function r:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309201330.png" alt=""></p>
<p>we will opt for controlling the residual locally in time, which leads to an initial value problem that can be solved over arbitrary long times. Specifically, we will seek $\theta(t)$ such that for all $t &gt; 0$ it holds</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309202447.png" alt=""></p>
<p>where we define the objective function $J_{t}:\Theta \times \dot{\Theta} \rightarrow \mathbb{R}$</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309202721.png" alt=""></p>
<h4 id="Neural-Galerkin-equations"><a href="#Neural-Galerkin-equations" class="headerlink" title="Neural Galerkin equations"></a>Neural Galerkin equations</h4><p>Since $J_{t}(\theta(t); \eta)$ is quadratic in $\eta$ and positive semi-definite, its minimum is unique and its minimizers solve the Euler-Lagrange equation</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309204135.png" alt=""></p>
<p>Written explicitly, Eq. (6) is a system of ODEs for $\theta(t)$:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309204238.png" alt=""></p>
<p>where we defined</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309210508.png" alt=""></p>
<p>in which $\bigotimes$ denotes the outer product.  The initial condition $\theta_{0}$ can be obtained via e.g. minimization of the least-squares loss between $u_{0}$ and $U(\theta_{0})$:  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310155509.png" alt=""></p>
<p>where ν is some user-prescribed measure with full support on $\mathcal{X}$ .  </p>
<h3 id="Estimating-M-theta-and-F-t-theta"><a href="#Estimating-M-theta-and-F-t-theta" class="headerlink" title="Estimating $M(\theta)$ and $F(t, \theta)$"></a>Estimating $M(\theta)$ and $F(t, \theta)$</h3><p>integrals in Eq. (8) do not admit a closed-form solution and so will need to be numerically estimated. In low dimensions,  we perform quadrature on a grid; in high dimensions, If $ν_{\theta}$<br>is a probability measure, we can consider using a vanilla Monte-Carlo estimator for each term,     </p>
<p>by drawing $n$ samples $\{x_{i}\}_{i=1}^{n}$ from $ν_{\theta}$ and replacing the expectations by empirical averages over these samples.  This estimator is efficient to approximate certain kernels uniformly over high-dimensional spaces, but not necessarily if the solution to the PDE develops spatially localized structures. Here are two options:</p>
<h4 id="Importance-sampling-with-a-fixed-measure"><a href="#Importance-sampling-with-a-fixed-measure" class="headerlink" title="Importance sampling with a fixed measure"></a>Importance sampling with a fixed measure</h4><p>importance sampling: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/41217212">重要性采样</a></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310165322.png" alt=""></p>
<h4 id="Direct-sampling-with-an-adaptive-measure"><a href="#Direct-sampling-with-an-adaptive-measure" class="headerlink" title="Direct sampling with an adaptive measure"></a>Direct sampling with an adaptive measure</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310170640.png" alt=""></p>
<h3 id="Discretization-in-time"><a href="#Discretization-in-time" class="headerlink" title="Discretization in time"></a>Discretization in time</h3><p>To update $\theta^{k}$, we can either use:  </p>
<h4 id="Explicit-integrators"><a href="#Explicit-integrators" class="headerlink" title="Explicit integrators"></a>Explicit integrators</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310171021.png" alt=""></p>
<h4 id="Implicit-integrators"><a href="#Implicit-integrators" class="headerlink" title="Implicit integrators"></a>Implicit integrators</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310171036.png" alt=""></p>
<h3 id="Neural-architectures"><a href="#Neural-architectures" class="headerlink" title="Neural architectures"></a>Neural architectures</h3><p>The first is a shallow (one-hidden-layer) network with m nodes given by</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310180129.png" alt=""></p>
<p> The first is the Gaussian kernel, which we use when $\mathcal{X} = \mathbb{R}^{d}$</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310180331.png" alt=""></p>
<p>the second is we use when $\mathcal{X} = L\mathbb{T}^{d}$ with L &gt; 0 and we need to enforce periodicity.</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310180732.png" alt=""></p>
<p>The other neural architecture that we use is a feedforward neural network with $l\in \mathbb{N}$ hidden layers and m nodes per layer:  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310182117.png" alt=""></p>
<p>$\varphi_{tanh}^{L}$ is the nonlinear unit</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310182545.png" alt=""></p>
<h1 id="AUTOIP-A-UNITED-FRAMEWORK-TO-INTEGRATE-PHYSICS-INTO-GAUSSIAN-PROCESSES"><a href="#AUTOIP-A-UNITED-FRAMEWORK-TO-INTEGRATE-PHYSICS-INTO-GAUSSIAN-PROCESSES" class="headerlink" title="AUTOIP: A UNITED FRAMEWORK TO INTEGRATE PHYSICS INTO GAUSSIAN PROCESSES"></a>AUTOIP: A UNITED FRAMEWORK TO INTEGRATE PHYSICS INTO GAUSSIAN PROCESSES</h1><h2 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h2><p>To model a system, one usually writes down a set of partial differential equations (PDEs) and/or ordinary differential equations (ODEs) that characterize how the system runs according to physical laws.  Then, one identifies the boundary and/or initial conditions and solves the equations.</p>
<p>Machine learning and data science use a completely different paradigm. They estimate or reconstruct target functions from observed data rather than from solving the equations.</p>
<h3 id="Contribution-1"><a href="#Contribution-1" class="headerlink" title="Contribution"></a>Contribution</h3><p>consider incorporating physics knowledge into Gaussian processes (GPs). Not only flexible enough to learn various, complex functions from data, but also convenient to quantify the uncertainty due to their closed-form posterior distribution. </p>
<ol>
<li>联合采样目标函数在input的值，方程有关的微分的值，多元高斯分布在配点的潜在源. we couple the target function and its derivatives in a probabilistic framework, without the need for conducting differential operations on a nonlinear surrogate (like NNs).  </li>
<li>Next, we feed these samples to two likelihoods. One is to fit the training data. The other is a virtual Gaussian likelihood that encourages the conformity to the equation.</li>
<li>we use the whitening trick to parameterize the latent random variables with a standard Gaussian noise.       </li>
</ol>
<h2 id="Gaussian-Process-Regression"><a href="#Gaussian-Process-Regression" class="headerlink" title="Gaussian Process Regression"></a>Gaussian Process Regression</h2><p>Consider a training dataset $\mathcal{D}=(X,y)$, where $X = [x_{1},\dots,x_{N}]$, $y = [y_{1},\dots, y_{N}]$, each $x_{n}$ is an input, and $y_{n}$ is a noisy observation of $f(x_{n})$.  Then the function values at the training inputs, $f = [f(x_{1}),\dots,f(x_{N})]$, follow a multivariate Gaussian distribution, $p(f|X) = N(f|0,K)$ where each $[K]_{i,j} = κ(x_{i}, x_{j})$  </p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>Specifically, we first construct a GP prior over $u$, $g$ and the equation-related derivatives, i.e., $\partial_{t}u$ and $\partial_{x}^{2}u$  , The covariance and cross-covariance among $u$ and its derivatives can be obtained outright from $κ_{u}$ via kernel differentiation   </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311161000.png" alt=""></p>
<p>Now, we can leverage the covariance functions in (4) and $k_{g}$ to construct a joint Gaussian prior over $f = [u; \hat{u}; \hat{u}_{t}; \hat{u}_{xx}; g]$ ,</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311163413.png" alt=""></p>
<p>Given $f$ , we feed them to two data likelihoods. One is to fit the actual observations from a Gaussian noise model,  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311163508.png" alt=""></p>
<p>The other is a virtual Gaussian likelihood that integrates the physics knowledge in the differential equation</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311163540.png" alt=""></p>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p> the virtual likelihood (7) couples the components of $f$ to reflect the equation. Hence, we develop a general variational inference algorithm to jointly estimate the posterior<br>of $f$ and kernel parameters, inverse noise variance $\beta$, $v$, etc. However, we found that a straightforward implementation to optimize the variational posterior $q(f)$ is often stuck at an inferior estimate.  </p>
<p>That is, we parameterize $f$ with a Gaussian noise,</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311164311.png" alt=""></p>
<p>where $\eta \thicksim N(0, I)$, and $A$ is the Cholesky decomposition of the covariance matrix $\Sigma$  i.e., $\Sigma = AA^{T}$ . Therefore, the joint probability of the model can be rewritten as</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311164651.png" alt=""></p>
<p>We then introduce a Gaussian variational posterior for the noise,  $q(\eta) = \mathcal{N}(\eta|\mu,LL^{T})$ where $L$ is a lower-triangular matrix to ensure the positive definiteness of the covariance matrix.   </p>
<p>We then construct a variational evidence lower bound,</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311170307.png" alt=""></p>
<h1 id="RESPECTING-CAUSALITY-IS-ALL-YOU-NEED-FOR-TRAINING-PHYSICS-INFORMED-NEURAL-NETWORKS"><a href="#RESPECTING-CAUSALITY-IS-ALL-YOU-NEED-FOR-TRAINING-PHYSICS-INFORMED-NEURAL-NETWORKS" class="headerlink" title="RESPECTING CAUSALITY IS ALL YOU NEED FOR TRAINING PHYSICS-INFORMED NEURAL NETWORKS"></a>RESPECTING CAUSALITY IS ALL YOU NEED FOR TRAINING PHYSICS-INFORMED NEURAL NETWORKS</h1><h2 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h2><p>Extensions to enhance the accuracy and robustness of PINNs: novel optimization algorithms for adaptive training;  adaptive algorithms for selecting batches of training data; novel network architectures; domain decomposition strategies; new types of activation functions; sequential learning strategies.</p>
<p>notion of temporal dependence is absent in most continuous-time PINNs formulations   </p>
<p>Specific contributions can be summarized as:  </p>
<ul>
<li>We reveal an implicit bias suggesting that continuous-time PINNs models can violate causality, and hence are susceptible to converge towards erroneous solutions.  </li>
<li>We put forth a simple re-formulation of PINNs loss functions that allows us to explicitly respect the causal structure that characterizes the solution of general nonlinear PDEs. </li>
<li>Strikingly, we demonstrate that this simple modification alone is enough to introduce significant accuracy improvements, allowing us to tackle problems that have remained elusive to PINNs.  </li>
<li>We provide a practical quantitative criterion for assessing the training convergence of a PINNs model.  </li>
<li>We examine a collection of challenging benchmarks for which existing PINNs formulations fail, and demonstrate that the proposed causal training strategy leads to state-of-the-art results.  </li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317183104.png" alt=""></p>
<h2 id="Causal-training-for-physics-informed-neural-networks"><a href="#Causal-training-for-physics-informed-neural-networks" class="headerlink" title="Causal training for physics-informed neural networks"></a>Causal training for physics-informed neural networks</h2><p>To this end, we define a weighted residual loss as</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317184032.png" alt=""></p>
<p>We recognize that the weights $w_{i}$ should be large – and therefore allow the minimization of $\mathcal{L}_{r}(t_{i}, \theta)$ – only if all residuals $\{\mathcal{L}_{r}(t_{k}, \theta)\}^{i}_{k=1}$ before $t_{i}$ are minimized properly, and vice versa.   </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317184906.png" alt=""></p>
<p>As such, the weighted residual loss can be written as  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317185502.png" alt=""></p>
<p>$\mathcal{L}_{r}(t_{i}, \theta)$ will not be minimized unless all previous residuals $\{ \mathcal{L}_{r}(t_{k}, \theta) \}_{k=1}^{i-1}$ decrease to<br>some small value such that $w_{i}$ is large enough.</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317192154.png" alt=""></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%80%BB%E7%BB%93/" rel="tag"># 总结</a>
              <a href="/tags/%E8%AE%BA%E6%96%87/" rel="tag"># 论文</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/10/09/%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E6%80%BB%E7%BB%93/" rel="prev" title="项目部署总结">
                  <i class="fa fa-chevron-left"></i> 项目部署总结
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/11/07/Python%E6%8B%BE%E9%81%97/" rel="next" title="Python知识点">
                  Python知识点 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">木霈玖</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
