<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//unpkg.com/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//unpkg.com/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="前言记录一下读过论文的idea">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper Ideas">
<meta property="og:url" content="http://example.com/2021/11/02/Paper-Ideas/index.html">
<meta property="og:site_name" content="木霈玖的博客">
<meta property="og:description" content="前言记录一下读过论文的idea">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211104195000.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211104195040.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211104203300.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/swin_1.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/swin_2.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/image-20211128164821263.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/image-20211128222044474.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222142.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222208.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222342.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222510.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222753.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_1.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_2.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_3.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_4.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_5.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309201020.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309201330.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309202447.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309202721.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309204135.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309204238.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309210508.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310155509.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310165322.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310170640.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310171021.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310171036.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310180129.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310180331.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310180732.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310182117.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310182545.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311161000.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311163413.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311163508.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311163540.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311164311.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311164651.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311170307.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317183104.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317184032.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317184906.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317185502.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317192154.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409163226.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409163632.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409170339.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409190020.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409191356.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409192717.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409192809.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409192845.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409192956.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409193039.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409193200.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409193248.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418100747.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418101053.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418102832.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418103015.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418103050.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418104326.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418104941.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418105440.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418112136.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418171923.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418172009.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418172045.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419144543.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419144750.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419210104.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419210302.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419210602.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419212746.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419213250.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419213846.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419214445.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419214930.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419215013.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506153709.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506161343.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506163025.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506185405.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506190851.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506191803.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506193734.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506195407.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922185741.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922190052.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922182417.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922183152.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922183302.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922183445.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922183525.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922183605.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922184617.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922184701.png">
<meta property="article:published_time" content="2021-11-02T13:40:57.000Z">
<meta property="article:modified_time" content="2022-09-22T11:15:47.772Z">
<meta property="article:author" content="木霈玖">
<meta property="article:tag" content="总结">
<meta property="article:tag" content="论文">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211104195000.png">


<link rel="canonical" href="http://example.com/2021/11/02/Paper-Ideas/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>Paper Ideas | 木霈玖的博客</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">木霈玖的博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Physics-Coupled-Spatio-Temporal-Active-Learning-for-Dynamical-Systems"><span class="nav-number">2.</span> <span class="nav-text">Physics-Coupled Spatio-Temporal Active Learning for Dynamical Systems</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ACTIVE-LEARNING-OF-DEEP-SURROGATES-FOR-PDES"><span class="nav-number">3.</span> <span class="nav-text">ACTIVE LEARNING OF DEEP SURROGATES FOR PDES</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows"><span class="nav-number">4.</span> <span class="nav-text">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Architecture"><span class="nav-number">4.1.</span> <span class="nav-text">Architecture</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Adversarial-Sampling-for-Solving-Differential-Equations-with-Neural-Networks"><span class="nav-number">5.</span> <span class="nav-text">Adversarial Sampling for Solving Differential Equations with Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">5.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Key-Idea"><span class="nav-number">5.2.</span> <span class="nav-text">Key Idea</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Architecture-1"><span class="nav-number">5.3.</span> <span class="nav-text">Architecture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Problem"><span class="nav-number">5.4.</span> <span class="nav-text">Problem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Solution"><span class="nav-number">5.5.</span> <span class="nav-text">Solution</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Machine-Learning-of-Linear-Differential-Equations-using-Gaussian-Processes"><span class="nav-number">6.</span> <span class="nav-text">Machine Learning of Linear Differential Equations using Gaussian Processes</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Priors"><span class="nav-number">6.1.</span> <span class="nav-text">Priors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kernels"><span class="nav-number">6.2.</span> <span class="nav-text">Kernels</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training"><span class="nav-number">6.3.</span> <span class="nav-text">Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Predictions"><span class="nav-number">6.4.</span> <span class="nav-text">Predictions</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Learning-Physics-Informed-Neural-Networks-without-Stacked-Back-propagation"><span class="nav-number">7.</span> <span class="nav-text">Learning Physics-Informed Neural Networks without Stacked Back-propagation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Problems"><span class="nav-number">7.1.</span> <span class="nav-text">Problems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Contribution"><span class="nav-number">7.2.</span> <span class="nav-text">Contribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Advantages"><span class="nav-number">7.3.</span> <span class="nav-text">Advantages</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Notice"><span class="nav-number">7.4.</span> <span class="nav-text">Notice</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Two-Sources-of-Inefficiency-In-Computing-the-PINN-Loss"><span class="nav-number">7.5.</span> <span class="nav-text">Two Sources of Inefficiency In Computing the PINN Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Method"><span class="nav-number">7.6.</span> <span class="nav-text">Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Back-propagation-free-Derivative-Estimators"><span class="nav-number">7.6.1.</span> <span class="nav-text">4.1 Back-propagation-free Derivative Estimators</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Proof"><span class="nav-number">7.6.1.1.</span> <span class="nav-text">Proof</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Model-Capacity"><span class="nav-number">7.6.2.</span> <span class="nav-text">4.2 Model Capacity</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Proof-1"><span class="nav-number">7.6.2.1.</span> <span class="nav-text">Proof</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Variance-Reduced-Stein%E2%80%99s-Derivative-Estimators"><span class="nav-number">7.6.3.</span> <span class="nav-text">4.3 Variance-Reduced Stein’s Derivative Estimators</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#The-control-variate-method"><span class="nav-number">7.6.3.1.</span> <span class="nav-text">The control variate method</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Further-improvement-using-the-antithetic-variable-method"><span class="nav-number">7.6.3.2.</span> <span class="nav-text">Further improvement using the antithetic variable method</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Neural-Galerkin-Scheme-with-Active-Learning-for-High-Dimensional-Evolution-Equations"><span class="nav-number">8.</span> <span class="nav-text">Neural Galerkin Scheme with Active Learning for High-Dimensional Evolution Equations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">8.1.</span> <span class="nav-text">Abstract</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#problems"><span class="nav-number">8.1.1.</span> <span class="nav-text">problems</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-1"><span class="nav-number">8.2.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Main-contributions"><span class="nav-number">8.2.1.</span> <span class="nav-text">Main contributions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Related-works"><span class="nav-number">8.2.2.</span> <span class="nav-text">Related works</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Galerkin-schemes"><span class="nav-number">8.3.</span> <span class="nav-text">Neural Galerkin schemes</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Neural-Galerkin"><span class="nav-number">8.3.1.</span> <span class="nav-text">Neural Galerkin</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Parametrizing-the-solution"><span class="nav-number">8.3.1.1.</span> <span class="nav-text">Parametrizing the solution</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Controlling-the-residual"><span class="nav-number">8.3.1.2.</span> <span class="nav-text">Controlling the residual</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural-Galerkin-equations"><span class="nav-number">8.3.1.3.</span> <span class="nav-text">Neural Galerkin equations</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Estimating-M-theta-and-F-t-theta"><span class="nav-number">8.3.2.</span> <span class="nav-text">Estimating $M(\theta)$ and $F(t, \theta)$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Importance-sampling-with-a-fixed-measure"><span class="nav-number">8.3.2.1.</span> <span class="nav-text">Importance sampling with a fixed measure</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Direct-sampling-with-an-adaptive-measure"><span class="nav-number">8.3.2.2.</span> <span class="nav-text">Direct sampling with an adaptive measure</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Discretization-in-time"><span class="nav-number">8.3.3.</span> <span class="nav-text">Discretization in time</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Explicit-integrators"><span class="nav-number">8.3.3.1.</span> <span class="nav-text">Explicit integrators</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Implicit-integrators"><span class="nav-number">8.3.3.2.</span> <span class="nav-text">Implicit integrators</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Neural-architectures"><span class="nav-number">8.3.4.</span> <span class="nav-text">Neural architectures</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AUTOIP-A-UNITED-FRAMEWORK-TO-INTEGRATE-PHYSICS-INTO-GAUSSIAN-PROCESSES"><span class="nav-number">9.</span> <span class="nav-text">AUTOIP: A UNITED FRAMEWORK TO INTEGRATE PHYSICS INTO GAUSSIAN PROCESSES</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-2"><span class="nav-number">9.1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Contribution-1"><span class="nav-number">9.1.1.</span> <span class="nav-text">Contribution</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gaussian-Process-Regression"><span class="nav-number">9.2.</span> <span class="nav-text">Gaussian Process Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model"><span class="nav-number">9.3.</span> <span class="nav-text">Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Algorithm"><span class="nav-number">9.4.</span> <span class="nav-text">Algorithm</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RESPECTING-CAUSALITY-IS-ALL-YOU-NEED-FOR-TRAINING-PHYSICS-INFORMED-NEURAL-NETWORKS"><span class="nav-number">10.</span> <span class="nav-text">RESPECTING CAUSALITY IS ALL YOU NEED FOR TRAINING PHYSICS-INFORMED NEURAL NETWORKS</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-3"><span class="nav-number">10.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Causal-training-for-physics-informed-neural-networks"><span class="nav-number">10.2.</span> <span class="nav-text">Causal training for physics-informed neural networks</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Deep-Implicit-Moving-Least-Squares-Functions-for-3D-Reconstruction"><span class="nav-number">11.</span> <span class="nav-text">Deep Implicit Moving Least-Squares Functions for 3D Reconstruction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-1"><span class="nav-number">11.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-4"><span class="nav-number">11.2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Related-Work"><span class="nav-number">11.3.</span> <span class="nav-text">Related Work</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-representations-for-3D-generation"><span class="nav-number">11.3.1.</span> <span class="nav-text">Deep representations for 3D generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Surface-reconstruction-from-point-clouds"><span class="nav-number">11.3.2.</span> <span class="nav-text">Surface reconstruction from point clouds</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Method-1"><span class="nav-number">11.4.</span> <span class="nav-text">Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#IMLS-surface"><span class="nav-number">11.4.1.</span> <span class="nav-text">IMLS surface</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-IMLS-surface"><span class="nav-number">11.4.2.</span> <span class="nav-text">Deep IMLS surface</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Scaffold-prediction"><span class="nav-number">11.4.2.1.</span> <span class="nav-text">Scaffold prediction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MLS-point-prediction"><span class="nav-number">11.4.2.2.</span> <span class="nav-text">MLS point prediction</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Network-structure"><span class="nav-number">11.4.3.</span> <span class="nav-text">Network structure</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Loss-function-design"><span class="nav-number">11.4.4.</span> <span class="nav-text">Loss function design</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Octree-structure-loss"><span class="nav-number">11.4.4.1.</span> <span class="nav-text">Octree structure loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SDF-loss"><span class="nav-number">11.4.4.2.</span> <span class="nav-text">SDF loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MLS-point-repulsion-loss"><span class="nav-number">11.4.4.3.</span> <span class="nav-text">MLS point repulsion loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Projection-smoothness-loss"><span class="nav-number">11.4.4.4.</span> <span class="nav-text">Projection smoothness loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Radius-smoothness-loss"><span class="nav-number">11.4.4.5.</span> <span class="nav-text">Radius smoothness loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Weight-decay"><span class="nav-number">11.4.4.6.</span> <span class="nav-text">Weight decay</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Efficient-Training-of-Physics-Informed-Neural-Networks-via-Importance-Sampling"><span class="nav-number">12.</span> <span class="nav-text">Efficient Training of Physics-Informed Neural Networks via Importance Sampling</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-2"><span class="nav-number">12.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-5"><span class="nav-number">12.2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-Learning-of-Differential-Equations"><span class="nav-number">12.3.</span> <span class="nav-text">Deep Learning of Differential Equations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Importance-Sampling-for-Training-of-PINNs"><span class="nav-number">12.4.</span> <span class="nav-text">Importance Sampling for Training of PINNs</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PAGP-A-physics-assisted-Gaussian-process-framework-with-active-learning-for-forward-and-inverse-problems-of-partial-differential-equations"><span class="nav-number">13.</span> <span class="nav-text">PAGP: A physics-assisted Gaussian process framework with active learning for forward and inverse problems of partial differential equations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-3"><span class="nav-number">13.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-6"><span class="nav-number">13.2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Methodology"><span class="nav-number">13.3.</span> <span class="nav-text">Methodology</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Gaussian-process-regression"><span class="nav-number">13.3.1.</span> <span class="nav-text">Gaussian process regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Derivatives-of-Gaussian-process-regression"><span class="nav-number">13.3.2.</span> <span class="nav-text">Derivatives of Gaussian process regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Models"><span class="nav-number">13.3.3.</span> <span class="nav-text">Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Active-Learning"><span class="nav-number">13.3.4.</span> <span class="nav-text">Active Learning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">13.4.</span> <span class="nav-text">Conclusion</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Breaking-the-Dilemma-of-Medical-Image-to-image-Translation"><span class="nav-number">14.</span> <span class="nav-text">Breaking the Dilemma of Medical Image-to-image Translation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-4"><span class="nav-number">14.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-7"><span class="nav-number">14.2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Methodology-1"><span class="nav-number">14.3.</span> <span class="nav-text">Methodology</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Theoretical-Motivation"><span class="nav-number">14.3.1.</span> <span class="nav-text">Theoretical Motivation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RegGAN"><span class="nav-number">14.3.2.</span> <span class="nav-text">RegGAN</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Characterizing-possible-failure-modes-in-physics-informed-neural-networks"><span class="nav-number">15.</span> <span class="nav-text">Characterizing possible failure modes in physics-informed neural networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-5"><span class="nav-number">15.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-8"><span class="nav-number">15.2.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Problem-overview"><span class="nav-number">15.2.1.</span> <span class="nav-text">Problem overview</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Main-contributions-1"><span class="nav-number">15.2.2.</span> <span class="nav-text">Main contributions</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Related-work"><span class="nav-number">15.3.</span> <span class="nav-text">Related work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Possible-failure-modes-for-physics-informed-neural-networks"><span class="nav-number">15.4.</span> <span class="nav-text">Possible failure modes for physics-informed neural networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Experiment-setup"><span class="nav-number">15.4.1.</span> <span class="nav-text">Experiment setup.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#convection%E3%80%81reaction-diffusion"><span class="nav-number">15.4.2.</span> <span class="nav-text">convection、reaction-diffusion</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Diagnosing-possible-failure-modes-for-physics-informed-NNs"><span class="nav-number">15.5.</span> <span class="nav-text">Diagnosing possible failure modes for physics-informed NNs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Soft-PDE-regularization-and-optimization-difficulties"><span class="nav-number">15.5.1.</span> <span class="nav-text">Soft PDE regularization and optimization difficulties</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Expressivity-versus-optimization-difficulty"><span class="nav-number">15.6.</span> <span class="nav-text">Expressivity versus optimization difficulty</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Curriculum-PINN-Regularization"><span class="nav-number">15.6.1.</span> <span class="nav-text">Curriculum PINN Regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sequence-to-sequence-learning-vs-learning-the-entire-space-time-solution"><span class="nav-number">15.6.2.</span> <span class="nav-text">Sequence-to-sequence learning vs learning the entire space-time solution</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Uncertainty-Quantification-in-Scientific-Machine-Learning-Methods-Metrics-and-Comparisons"><span class="nav-number">16.</span> <span class="nav-text">Uncertainty Quantification in Scientific Machine Learning:Methods, Metrics, and Comparisons</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-6"><span class="nav-number">16.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-9"><span class="nav-number">16.2.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Novel-contributions-of-this-work"><span class="nav-number">16.2.1.</span> <span class="nav-text">Novel contributions of this work</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-PDEs-and-neural-operators"><span class="nav-number">16.3.</span> <span class="nav-text">Neural PDEs and neural operators</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Solving-forward-and-mixed-PDE-problems-Overview-of-PINN-method"><span class="nav-number">16.3.1.</span> <span class="nav-text">Solving forward and mixed PDE problems: Overview of PINN method</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-operator-mappings-Overview-of-DeepONet-method"><span class="nav-number">16.3.2.</span> <span class="nav-text">Learning operator mappings: Overview of DeepONet method</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Modeling-total-uncertainty"><span class="nav-number">16.4.</span> <span class="nav-text">Modeling total uncertainty</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Uncertainty-in-function-approximation"><span class="nav-number">16.4.1.</span> <span class="nav-text">Uncertainty in function approximation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Uncertainty-in-PINNs"><span class="nav-number">16.4.2.</span> <span class="nav-text">Uncertainty in PINNs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Uncertainty-in-DeepONets"><span class="nav-number">16.4.3.</span> <span class="nav-text">Uncertainty in DeepONets</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Methods-for-uncertainty-quantification"><span class="nav-number">16.5.</span> <span class="nav-text">Methods for uncertainty quantification</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multi-Objective-Loss-Balancing-for-Physics-Informed-Deep-Learning"><span class="nav-number">17.</span> <span class="nav-text">Multi-Objective Loss Balancing for Physics-Informed Deep Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-7"><span class="nav-number">17.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-10"><span class="nav-number">17.2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Physics-Informed-Neural-Networks-PINNs"><span class="nav-number">17.3.</span> <span class="nav-text">Physics-Informed Neural Networks (PINNs)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Methodology-2"><span class="nav-number">17.4.</span> <span class="nav-text">Methodology</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Objective-Optimisation"><span class="nav-number">17.4.1.</span> <span class="nav-text">Multi-Objective Optimisation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adaptive-Loss-Balancing-Methods"><span class="nav-number">17.4.2.</span> <span class="nav-text">Adaptive Loss Balancing Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Learning-Rate-Annealing"><span class="nav-number">17.4.2.1.</span> <span class="nav-text">Learning Rate Annealing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GradNorm"><span class="nav-number">17.4.2.2.</span> <span class="nav-text">GradNorm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SoftAdapt"><span class="nav-number">17.4.2.3.</span> <span class="nav-text">SoftAdapt</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Relative-Loss-Balancing-with-Random-Lookback-ReLoBRaLo"><span class="nav-number">17.5.</span> <span class="nav-text">Relative Loss Balancing with Random Lookback (ReLoBRaLo)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hyperparameter-Tuning-and-Meta-Learning"><span class="nav-number">17.6.</span> <span class="nav-text">Hyperparameter Tuning and Meta Learning</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">木霈玖</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/02/Paper-Ideas/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="木霈玖">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="木霈玖的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Paper Ideas
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-11-02 21:40:57" itemprop="dateCreated datePublished" datetime="2021-11-02T21:40:57+08:00">2021-11-02</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-09-22 19:15:47" itemprop="dateModified" datetime="2022-09-22T19:15:47+08:00">2022-09-22</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">总结</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>31k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>28 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>记录一下读过论文的idea</p>
<a id="more"></a>
<h1 id="Physics-Coupled-Spatio-Temporal-Active-Learning-for-Dynamical-Systems"><a href="#Physics-Coupled-Spatio-Temporal-Active-Learning-for-Dynamical-Systems" class="headerlink" title="Physics-Coupled Spatio-Temporal Active Learning for Dynamical Systems"></a>Physics-Coupled Spatio-Temporal Active Learning for Dynamical Systems</h1><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211104195000.png" alt="framework"></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211104195040.png" alt="FN-PN"></p>
<ul>
<li>初始化<ul>
<li>选定 $n$ 个点 $\longrightarrow$ $\Omega^{n}_{temp}$</li>
<li>创建训练数据 $\longrightarrow$ $D_{temp}$ （选定 $n$ 个点都取 $T_{\omega}$ 时长的数据）</li>
</ul>
</li>
<li>训练<ul>
<li>learn $\lambda$ $\longleftarrow$ 最小化 $E_{q}$ ，偏微分方程</li>
<li>train ST-PCNN $\longleftarrow$ $\lambda$</li>
<li>predict $[\hat{s}]$ $\longleftarrow$ at all locations</li>
<li>$\Omega_{Kriging}^{n}$ $\longleftarrow$ $n$ 个：largest estimate error</li>
<li>$D_{Kriging}$ $\longleftarrow$ 上一步新选出的 $n$ 个，选取 $T_{\omega}$ 时长数据</li>
<li>更新 $D$</li>
</ul>
</li>
</ul>
<h1 id="ACTIVE-LEARNING-OF-DEEP-SURROGATES-FOR-PDES"><a href="#ACTIVE-LEARNING-OF-DEEP-SURROGATES-FOR-PDES" class="headerlink" title="ACTIVE LEARNING OF DEEP SURROGATES FOR PDES"></a>ACTIVE LEARNING OF DEEP SURROGATES FOR PDES</h1><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211104203300.png" alt="model"></p>
<p>algorithm: reduce number of training points(selected based on error measure)</p>
<p>AL: adding the most uncertain points</p>
<ul>
<li><p>Initialize:</p>
<p>random choose $n_{init}$ train 50 epochs $\longrightarrow$ $\widetilde{t^{0}}(p)$</p>
</li>
<li><p>Do T times:</p>
<ul>
<li>evaluate $\widetilde{t^{i}}(p)$ at $M×K$ points</li>
<li>choose $K$ points(largest $\sigma_{*}^{2}$ )</li>
<li>put the $K$ points into training set</li>
</ul>
</li>
</ul>
<h1 id="Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows"><a href="#Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows" class="headerlink" title="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"></a>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h1><p>Application：language $\longrightarrow$ vision</p>
<p>Challenges:</p>
<ul>
<li>scale(language Transformer: word tokens)</li>
<li>high resolution of pixels(计算复杂度： $n^{2}$ )</li>
</ul>
<p>Key point:</p>
<ul>
<li>小批量开始 $\longrightarrow$ 逐渐合并邻居</li>
<li>如何实现线性复杂度：在无重叠窗口计算自注意力<ul>
<li>standard transformer architecture: global self-attention $\longrightarrow$ quadratic complexity</li>
<li>Swin Transformer: local self-attention $\longrightarrow$ linear complexity</li>
</ul>
</li>
</ul>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><ul>
<li><p>Overall Framework</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/swin_1.png" alt=""></p>
</li>
<li><p>Two Successive Swin Transformer Blocks  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/swin_2.png" alt=""></p>
</li>
</ul>
<h1 id="Adversarial-Sampling-for-Solving-Differential-Equations-with-Neural-Networks"><a href="#Adversarial-Sampling-for-Solving-Differential-Equations-with-Neural-Networks" class="headerlink" title="Adversarial Sampling for Solving Differential Equations with Neural Networks"></a>Adversarial Sampling for Solving Differential Equations with Neural Networks</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>sample points adversarially to maximize the loss of the current solution estimate  </p>
<p>Advantages on using neural networks:</p>
<ul>
<li>instead of obtaining solution values at discretized points, we get a closed and differentiable solution function</li>
<li>it is more effective in solving high dimensional PDEs by faring better against the “curse of dimensionality” </li>
<li>numerical errors are not accumulated in each iteration</li>
<li>initial and boundary conditions are satisfied by construction</li>
</ul>
<p>Drawbacks  of using a predefined sampling scheme: agnostic to the equation being solved as well as our current estimate $\hat{y}$</p>
<h2 id="Key-Idea"><a href="#Key-Idea" class="headerlink" title="Key Idea"></a>Key Idea</h2><p>present a sampling scheme that is dependent on the current estimate $\hat{y}$, using a neural network to represent a variable sampling distribution.</p>
<p>In each iteration, the sampler is trained to <strong>produce points which maximize the loss of the solver (and a secondary loss). </strong></p>
<p>Thus, it competes with the solver whose weights are updated to minimize the loss at these very points.  </p>
<h2 id="Architecture-1"><a href="#Architecture-1" class="headerlink" title="Architecture"></a>Architecture</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/image-20211128164821263.png" alt=""></p>
<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>It is observed that if the sampler is purely optimized with the objective of maximizing $\hat{L}(\hat{y}; x)$(residual loss corresponding to the $DE$ at samples $x$), it tends to collapse all samples to one single point of high loss. </p>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>Therefore, use <strong>an additional loss term $D_{k}$,</strong>Given points$\begin{Bmatrix}x_{1},x_{2},\dots,x_{n}\end{Bmatrix}$, we define $d_{k}(x_{i})$ to be the sum of distances of $x_{i}$ from its $k$ nearest neighbors.</p>
<h1 id="Machine-Learning-of-Linear-Differential-Equations-using-Gaussian-Processes"><a href="#Machine-Learning-of-Linear-Differential-Equations-using-Gaussian-Processes" class="headerlink" title="Machine Learning of Linear Differential Equations using Gaussian Processes"></a>Machine Learning of Linear Differential Equations using Gaussian Processes</h1><p>Gaussian process priors are modified according to the particular form of such operators and are employed to infer parameters of the linear equations from scarce and possibly noisy observations.  </p>
<p>optimal model parameters and hyper-parameters are all learned directly from the data by maximizing the joint marginal log-likelihood of the probabilistic model instead of being guessed or tuned manually by the user.  </p>
<h2 id="Priors"><a href="#Priors" class="headerlink" title="Priors"></a>Priors</h2><p>place the $GP$ prior on $u(x)$ instead of $f(x)$ </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/image-20211128222044474.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222142.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222208.png" alt=""></p>
<h2 id="Kernels"><a href="#Kernels" class="headerlink" title="Kernels"></a>Kernels</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222342.png" alt=""></p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>employing a $Quasi-Newton$ optimizer $L-BFGS$ to minimize the negative log marginal likelihood</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222510.png" alt=""></p>
<h2 id="Predictions"><a href="#Predictions" class="headerlink" title="Predictions"></a>Predictions</h2><p>one can predict the values $u(x)$ and $f(x)$ at a new test point $x$ by writing the posterior distributions</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20211128222753.png" alt=""></p>
<h1 id="Learning-Physics-Informed-Neural-Networks-without-Stacked-Back-propagation"><a href="#Learning-Physics-Informed-Neural-Networks-without-Stacked-Back-propagation" class="headerlink" title="Learning Physics-Informed Neural Networks without Stacked Back-propagation"></a>Learning Physics-Informed Neural Networks without Stacked Back-propagation</h1><h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><p>PINN training suffers from a significant scalability issue</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><ul>
<li>developing a novel approach to train the model without stacked back-propagation</li>
<li>parameterize the PDE solution $u(x; θ)$ as a Gaussian smoothed model, $u(x;\theta)=E_{\delta\thicksim\mathcal{N}(0,\sigma^{2}\mathbf{I})}f(x+\delta,\theta)$, where $u$ transforms arbitrary base network $f$ by injecting Gaussian noise into input $x$. This transformation gives rise to a key property for $u$ where its derivatives to the input can be efficiently calculated <em>without back-propagation</em>.<ul>
<li>Such property is derived from the well-known Stein’s Identity that essentially tells that the derivatives of any Gaussian smoothed function $u$ can be reformulated as some expectation terms of the output of its base $f$, which can be estimated using Monte Carlo methods. </li>
</ul>
</li>
<li>given any PDE problem, we can replace the derivative terms in the PDE with Stein’s<br>derivative estimators.</li>
</ul>
<h2 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h2><ol>
<li>no longer need stacked back-propagation to compute the loss</li>
<li>parallelize the computation into distributed machines to further accelerate the training </li>
</ol>
<h2 id="Notice"><a href="#Notice" class="headerlink" title="Notice"></a>Notice</h2><p>for large $\sigma$, the induced Gaussian smoothed models may not be expressive enough to approximate functions (i.e., learn solutions) with a large Lipschitz constant. Therefore, using a small value of $\sigma$ is usually a better choice in practice. However, a small $\sigma$ will lead to high-variance Stein’s derivative estimation, which inevitably causes unstable training.  </p>
<h2 id="Two-Sources-of-Inefficiency-In-Computing-the-PINN-Loss"><a href="#Two-Sources-of-Inefficiency-In-Computing-the-PINN-Loss" class="headerlink" title="Two Sources of Inefficiency In Computing the PINN Loss"></a>Two Sources of Inefficiency In Computing the PINN Loss</h2><ol>
<li>different orders of derivatives can only be calculated sequentially  </li>
<li>the dimension-level inefficiency </li>
</ol>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="4-1-Back-propagation-free-Derivative-Estimators"><a href="#4-1-Back-propagation-free-Derivative-Estimators" class="headerlink" title="4.1 Back-propagation-free Derivative Estimators"></a>4.1 Back-propagation-free Derivative Estimators</h3><p>define $u(x)=E_{\delta\thicksim\mathcal{N}(0,\sigma^{2}\mathbf{I})}f(x+\delta,\theta)$, then we have $\bigtriangledown_{x}u=E_{\delta\thicksim\mathcal{N}(0,\sigma^{2}\mathbf{I})}[\frac{\delta}{\sigma^{2}}f(x+\delta)]$</p>
<h4 id="Proof"><a href="#Proof" class="headerlink" title="Proof"></a>Proof</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_1.png" alt=""></p>
<p>From the above theorem, we can see that the first-order derivative rxu can be reformulated as an expectation term $E_{\delta\thicksim\mathcal{N}(0,\sigma^{2}\mathbf{I})}[\frac{\delta}{\sigma^{2}}f(x+\delta)]$, To calculate the value of the expectation, we can use Monte Carlo method to obtain an unbiased estimation from K.  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_2.png" alt=""></p>
<h3 id="4-2-Model-Capacity"><a href="#4-2-Model-Capacity" class="headerlink" title="4.2 Model Capacity"></a>4.2 Model Capacity</h3><p>For any measurable function $f : R^{d}\rightarrow R$, define $u(x)=E_{\delta\thicksim\mathcal{N}(0,\sigma^{2}\mathbf{I})}f(x+\delta,\theta)$, then<br>$u(x) $is $\frac{F}{\sigma}\sqrt{\frac{2}{\pi}}$-Lipschitz with respect to $l_{2}$-norm, where $F=sup_{x\in R^{d}}|f(x)|$.  </p>
<h4 id="Proof-1"><a href="#Proof-1" class="headerlink" title="Proof"></a>Proof</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_3.png" alt=""></p>
<h3 id="4-3-Variance-Reduced-Stein’s-Derivative-Estimators"><a href="#4-3-Variance-Reduced-Stein’s-Derivative-Estimators" class="headerlink" title="4.3 Variance-Reduced Stein’s Derivative Estimators"></a>4.3 Variance-Reduced Stein’s Derivative Estimators</h3><h4 id="The-control-variate-method"><a href="#The-control-variate-method" class="headerlink" title="The control variate method"></a>The control variate method</h4><p>One generic approach to reducing the variance of Monte Carlo estimates of integrals is to use an additive control variate, which is known as baseline.  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_4.png" alt=""></p>
<h4 id="Further-improvement-using-the-antithetic-variable-method"><a href="#Further-improvement-using-the-antithetic-variable-method" class="headerlink" title="Further improvement using the antithetic variable method"></a>Further improvement using the antithetic variable method</h4><p>The antithetic variable method is yet another powerful technique for variance reduction.  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220221_5.png" alt=""></p>
<h1 id="Neural-Galerkin-Scheme-with-Active-Learning-for-High-Dimensional-Evolution-Equations"><a href="#Neural-Galerkin-Scheme-with-Active-Learning-for-High-Dimensional-Evolution-Equations" class="headerlink" title="Neural Galerkin Scheme with Active Learning for High-Dimensional Evolution Equations"></a>Neural Galerkin Scheme with Active Learning for High-Dimensional Evolution Equations</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><h3 id="problems"><a href="#problems" class="headerlink" title="problems"></a>problems</h3><ol>
<li>no data are available  </li>
<li>the principal aim is to gather insights from a known model  </li>
</ol>
<p>高维逼近问题需要一个完全不同的“离线”自适应概念来规避维数的诅咒。</p>
<h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><p>develop time-integrators for PDEs that use DNNs to represent the solution but update the parameters sequentially from one time slice to another rather than globally over the whole time-space domain.  </p>
<p>use the structural form of the PDEs, but no a priori data about their solution.</p>
<p>leverage adaptivity in both function approximation and data acquisition.   </p>
<h3 id="Main-contributions"><a href="#Main-contributions" class="headerlink" title="Main contributions"></a>Main contributions</h3><ol>
<li><p>derive a nonlinear evolution equation for the parameters. </p>
<p>This equation can then be integrated using standard solvers with different level of sophistication.   </p>
<p>the proposed approach takes larger time steps when possible and corrects to smaller time-step sizes if the dynamics of the solution require it.  </p>
</li>
<li><p>The evolution equations that we derive for the DNN parameters involve operators that require estimation via sampling in space.  propose a dynamical estimation of the loss.   </p>
</li>
<li><p>We illustrate the viability and usefulness of our approach on a series of test cases.  </p>
</li>
</ol>
<h3 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h3><ol>
<li>The need for adaptive data acquisition in the context of machine learning for problems<br>in science and engineering has been emphasized in previous works.</li>
<li>There also is a large body of work on numerically solving PDEs with DNN parametrization based on collocation over the spatio-temporal domain.   </li>
<li>There also is a range of surrogate-modeling methods based on nonlinear parametrizations.  </li>
</ol>
<h2 id="Neural-Galerkin-schemes"><a href="#Neural-Galerkin-schemes" class="headerlink" title="Neural Galerkin schemes"></a>Neural Galerkin schemes</h2><h3 id="Neural-Galerkin"><a href="#Neural-Galerkin" class="headerlink" title="Neural Galerkin"></a>Neural Galerkin</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309201020.png" alt=""></p>
<h4 id="Parametrizing-the-solution"><a href="#Parametrizing-the-solution" class="headerlink" title="Parametrizing the solution"></a>Parametrizing the solution</h4><p>use ansatz $u(t,x)=U(\theta(t),x)$ , It is important to emphasize that U may depend nonlinearly on $\theta (t)$ , which is in stark contrast to the majority of classical approximations in scientific computing that have a linear dependence on the parameter.</p>
<h4 id="Controlling-the-residual"><a href="#Controlling-the-residual" class="headerlink" title="Controlling the residual"></a>Controlling the residual</h4><p>Since we do not have access to the solution $u(t)$ , we will use the structure of the governing equation to control the approximation error. To this end, note that inserting the ansatz solution $U(\theta(t))$ in Eq. (1). assuming differentiability of $\theta(t)$ and using $\partial_{t}U(\theta(t))=\triangledown_{\theta}U(\theta)\cdot \dot{\theta}(t)$ , leads to the residual function r:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309201330.png" alt=""></p>
<p>we will opt for controlling the residual locally in time, which leads to an initial value problem that can be solved over arbitrary long times. Specifically, we will seek $\theta(t)$ such that for all $t &gt; 0$ it holds</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309202447.png" alt=""></p>
<p>where we define the objective function $J_{t}:\Theta \times \dot{\Theta} \rightarrow \mathbb{R}$</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309202721.png" alt=""></p>
<h4 id="Neural-Galerkin-equations"><a href="#Neural-Galerkin-equations" class="headerlink" title="Neural Galerkin equations"></a>Neural Galerkin equations</h4><p>Since $J_{t}(\theta(t); \eta)$ is quadratic in $\eta$ and positive semi-definite, its minimum is unique and its minimizers solve the Euler-Lagrange equation</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309204135.png" alt=""></p>
<p>Written explicitly, Eq. (6) is a system of ODEs for $\theta(t)$:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309204238.png" alt=""></p>
<p>where we defined</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220309210508.png" alt=""></p>
<p>in which $\bigotimes$ denotes the outer product.  The initial condition $\theta_{0}$ can be obtained via e.g. minimization of the least-squares loss between $u_{0}$ and $U(\theta_{0})$:  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310155509.png" alt=""></p>
<p>where ν is some user-prescribed measure with full support on $\mathcal{X}$ .  </p>
<h3 id="Estimating-M-theta-and-F-t-theta"><a href="#Estimating-M-theta-and-F-t-theta" class="headerlink" title="Estimating $M(\theta)$ and $F(t, \theta)$"></a>Estimating $M(\theta)$ and $F(t, \theta)$</h3><p>integrals in Eq. (8) do not admit a closed-form solution and so will need to be numerically estimated. In low dimensions,  we perform quadrature on a grid; in high dimensions, If $ν_{\theta}$<br>is a probability measure, we can consider using a vanilla Monte-Carlo estimator for each term,     </p>
<p>by drawing $n$ samples $\{x_{i}\}_{i=1}^{n}$ from $ν_{\theta}$ and replacing the expectations by empirical averages over these samples.  This estimator is efficient to approximate certain kernels uniformly over high-dimensional spaces, but not necessarily if the solution to the PDE develops spatially localized structures. Here are two options:</p>
<h4 id="Importance-sampling-with-a-fixed-measure"><a href="#Importance-sampling-with-a-fixed-measure" class="headerlink" title="Importance sampling with a fixed measure"></a>Importance sampling with a fixed measure</h4><p>importance sampling: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/41217212">重要性采样</a></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310165322.png" alt=""></p>
<h4 id="Direct-sampling-with-an-adaptive-measure"><a href="#Direct-sampling-with-an-adaptive-measure" class="headerlink" title="Direct sampling with an adaptive measure"></a>Direct sampling with an adaptive measure</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310170640.png" alt=""></p>
<h3 id="Discretization-in-time"><a href="#Discretization-in-time" class="headerlink" title="Discretization in time"></a>Discretization in time</h3><p>To update $\theta^{k}$, we can either use:  </p>
<h4 id="Explicit-integrators"><a href="#Explicit-integrators" class="headerlink" title="Explicit integrators"></a>Explicit integrators</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310171021.png" alt=""></p>
<h4 id="Implicit-integrators"><a href="#Implicit-integrators" class="headerlink" title="Implicit integrators"></a>Implicit integrators</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310171036.png" alt=""></p>
<h3 id="Neural-architectures"><a href="#Neural-architectures" class="headerlink" title="Neural architectures"></a>Neural architectures</h3><p>The first is a shallow (one-hidden-layer) network with m nodes given by</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310180129.png" alt=""></p>
<p> The first is the Gaussian kernel, which we use when $\mathcal{X} = \mathbb{R}^{d}$</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310180331.png" alt=""></p>
<p>the second is we use when $\mathcal{X} = L\mathbb{T}^{d}$ with L &gt; 0 and we need to enforce periodicity.</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310180732.png" alt=""></p>
<p>The other neural architecture that we use is a feedforward neural network with $l\in \mathbb{N}$ hidden layers and m nodes per layer:  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310182117.png" alt=""></p>
<p>$\varphi_{tanh}^{L}$ is the nonlinear unit</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220310182545.png" alt=""></p>
<h1 id="AUTOIP-A-UNITED-FRAMEWORK-TO-INTEGRATE-PHYSICS-INTO-GAUSSIAN-PROCESSES"><a href="#AUTOIP-A-UNITED-FRAMEWORK-TO-INTEGRATE-PHYSICS-INTO-GAUSSIAN-PROCESSES" class="headerlink" title="AUTOIP: A UNITED FRAMEWORK TO INTEGRATE PHYSICS INTO GAUSSIAN PROCESSES"></a>AUTOIP: A UNITED FRAMEWORK TO INTEGRATE PHYSICS INTO GAUSSIAN PROCESSES</h1><h2 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h2><p>To model a system, one usually writes down a set of partial differential equations (PDEs) and/or ordinary differential equations (ODEs) that characterize how the system runs according to physical laws.  Then, one identifies the boundary and/or initial conditions and solves the equations.</p>
<p>Machine learning and data science use a completely different paradigm. They estimate or reconstruct target functions from observed data rather than from solving the equations.</p>
<h3 id="Contribution-1"><a href="#Contribution-1" class="headerlink" title="Contribution"></a>Contribution</h3><p>consider incorporating physics knowledge into Gaussian processes (GPs). Not only flexible enough to learn various, complex functions from data, but also convenient to quantify the uncertainty due to their closed-form posterior distribution. </p>
<ol>
<li>联合采样目标函数在input的值，方程有关的微分的值，多元高斯分布在配点的潜在源. we couple the target function and its derivatives in a probabilistic framework, without the need for conducting differential operations on a nonlinear surrogate (like NNs).  </li>
<li>Next, we feed these samples to two likelihoods. One is to fit the training data. The other is a virtual Gaussian likelihood that encourages the conformity to the equation.</li>
<li>we use the whitening trick to parameterize the latent random variables with a standard Gaussian noise.       </li>
</ol>
<h2 id="Gaussian-Process-Regression"><a href="#Gaussian-Process-Regression" class="headerlink" title="Gaussian Process Regression"></a>Gaussian Process Regression</h2><p>Consider a training dataset $\mathcal{D}=(X,y)$, where $X = [x_{1},\dots,x_{N}]$, $y = [y_{1},\dots, y_{N}]$, each $x_{n}$ is an input, and $y_{n}$ is a noisy observation of $f(x_{n})$.  Then the function values at the training inputs, $f = [f(x_{1}),\dots,f(x_{N})]$, follow a multivariate Gaussian distribution, $p(f|X) = N(f|0,K)$ where each $[K]_{i,j} = κ(x_{i}, x_{j})$  </p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>Specifically, we first construct a GP prior over $u$, $g$ and the equation-related derivatives, i.e., $\partial_{t}u$ and $\partial_{x}^{2}u$  , The covariance and cross-covariance among $u$ and its derivatives can be obtained outright from $κ_{u}$ via kernel differentiation   </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311161000.png" alt=""></p>
<p>Now, we can leverage the covariance functions in (4) and $k_{g}$ to construct a joint Gaussian prior over $f = [u; \hat{u}; \hat{u}_{t}; \hat{u}_{xx}; g]$ ,</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311163413.png" alt=""></p>
<p>Given $f$ , we feed them to two data likelihoods. One is to fit the actual observations from a Gaussian noise model,  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311163508.png" alt=""></p>
<p>The other is a virtual Gaussian likelihood that integrates the physics knowledge in the differential equation</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311163540.png" alt=""></p>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p> the virtual likelihood (7) couples the components of $f$ to reflect the equation. Hence, we develop a general variational inference algorithm to jointly estimate the posterior<br>of $f$ and kernel parameters, inverse noise variance $\beta$, $v$, etc. However, we found that a straightforward implementation to optimize the variational posterior $q(f)$ is often stuck at an inferior estimate.  </p>
<p>That is, we parameterize $f$ with a Gaussian noise,</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311164311.png" alt=""></p>
<p>where $\eta \thicksim N(0, I)$, and $A$ is the Cholesky decomposition of the covariance matrix $\Sigma$  i.e., $\Sigma = AA^{T}$ . Therefore, the joint probability of the model can be rewritten as</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311164651.png" alt=""></p>
<p>We then introduce a Gaussian variational posterior for the noise,  $q(\eta) = \mathcal{N}(\eta|\mu,LL^{T})$ where $L$ is a lower-triangular matrix to ensure the positive definiteness of the covariance matrix.   </p>
<p>We then construct a variational evidence lower bound,</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220311170307.png" alt=""></p>
<h1 id="RESPECTING-CAUSALITY-IS-ALL-YOU-NEED-FOR-TRAINING-PHYSICS-INFORMED-NEURAL-NETWORKS"><a href="#RESPECTING-CAUSALITY-IS-ALL-YOU-NEED-FOR-TRAINING-PHYSICS-INFORMED-NEURAL-NETWORKS" class="headerlink" title="RESPECTING CAUSALITY IS ALL YOU NEED FOR TRAINING PHYSICS-INFORMED NEURAL NETWORKS"></a>RESPECTING CAUSALITY IS ALL YOU NEED FOR TRAINING PHYSICS-INFORMED NEURAL NETWORKS</h1><h2 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h2><p>Extensions to enhance the accuracy and robustness of PINNs: novel optimization algorithms for adaptive training;  adaptive algorithms for selecting batches of training data; novel network architectures; domain decomposition strategies; new types of activation functions; sequential learning strategies.</p>
<p>notion of temporal dependence is absent in most continuous-time PINNs formulations   </p>
<p>Specific contributions can be summarized as:  </p>
<ul>
<li>We reveal an implicit bias suggesting that continuous-time PINNs models can violate causality, and hence are susceptible to converge towards erroneous solutions.  </li>
<li>We put forth a simple re-formulation of PINNs loss functions that allows us to explicitly respect the causal structure that characterizes the solution of general nonlinear PDEs. </li>
<li>Strikingly, we demonstrate that this simple modification alone is enough to introduce significant accuracy improvements, allowing us to tackle problems that have remained elusive to PINNs.  </li>
<li>We provide a practical quantitative criterion for assessing the training convergence of a PINNs model.  </li>
<li>We examine a collection of challenging benchmarks for which existing PINNs formulations fail, and demonstrate that the proposed causal training strategy leads to state-of-the-art results.  </li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317183104.png" alt=""></p>
<h2 id="Causal-training-for-physics-informed-neural-networks"><a href="#Causal-training-for-physics-informed-neural-networks" class="headerlink" title="Causal training for physics-informed neural networks"></a>Causal training for physics-informed neural networks</h2><p>To this end, we define a weighted residual loss as</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317184032.png" alt=""></p>
<p>We recognize that the weights $w_{i}$ should be large – and therefore allow the minimization of $\mathcal{L}_{r}(t_{i}, \theta)$ – only if all residuals $\{\mathcal{L}_{r}(t_{k}, \theta)\}^{i}_{k=1}$ before $t_{i}$ are minimized properly, and vice versa.   </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317184906.png" alt=""></p>
<p>As such, the weighted residual loss can be written as  </p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317185502.png" alt=""></p>
<p>$\mathcal{L}_{r}(t_{i}, \theta)$ will not be minimized unless all previous residuals $\{ \mathcal{L}_{r}(t_{k}, \theta) \}_{k=1}^{i-1}$ decrease to<br>some small value such that $w_{i}$ is large enough.</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220317192154.png" alt=""></p>
<h1 id="Deep-Implicit-Moving-Least-Squares-Functions-for-3D-Reconstruction"><a href="#Deep-Implicit-Moving-Least-Squares-Functions-for-3D-Reconstruction" class="headerlink" title="Deep Implicit Moving Least-Squares Functions for 3D Reconstruction"></a>Deep Implicit Moving Least-Squares Functions for 3D Reconstruction</h1><h2 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h2><p>点集因为其灵活和轻量级的表示被广泛应用于3D深度学习，但是它的离散特性限制了对连续和精细的几何图形的表示。这篇文章通过引入隐式最小二乘（IMLS）曲面公式，将离散的点集转化成光滑的曲面，定义了点集上的局部隐式函数。IMLSNet预测一个八叉树结构，作为再需要时生成MLS点的支架，并且用学到的先验知识来表征形状几何。同时，一旦MLS点被预测，隐式函数的评估将独立于神经网络，从而实现了快速运行时评估。</p>
<h2 id="Introduction-4"><a href="#Introduction-4" class="headerlink" title="Introduction"></a>Introduction</h2><p>与多边形网格和体积网格等其他3D表示相比，点集作为神经元被自然地嵌入DNN中，易于获取，拥有最小的额外结构，动态捕捉复杂的几何和拓扑，并且不会浪费自由空间区域的计算。事实上，点集已经被用于基于深度学习的不同任务的3D分析。在使用点集进行深度学习生成三维数据时，一方面可以灵活地建模变化的拓扑和复杂的表面，另一方面也会受到离散和粗糙几何的影响。</p>
<p>近年来的研究主要集中在以网格和多边形块的形式生成形状，但由于其离散性和非光滑性，其形状表达能力仍然受到限制。而深度隐式函数方法则在整个三维域上定义光滑函数，以保证结果的连续性，在高质量的三维重建中具有很好的应用前景。然而，隐式曲面生成是低效的，因为对于三维域中的每个点，在提取曲面之前，网络都必须单独评估。在本文中，我们结合了隐函数方法和点集方法的优点，在保留显式点集固有的灵活性和计算效率的同时，将点集表示方法扩展到隐式曲面模型中，以实现高质量的三维生成。</p>
<p>对于通过点集建模光滑曲面，我们采用点集曲面，并使用点的移动最小二乘插值来定义在点集的窄带区域内的局部隐函数。特别是对于本文使用的隐式MLS公式，对于狭窄区域内的任意空间点，采用隐式MLS函数将其映射到零水平集表面的有符号距离值，定义为附近点支持的有符号距离的定向平面的加权混合；然后将零水平集曲面提取为光滑连续的曲面，用于形状表示。</p>
<p>虽然MLS曲面在三维重建和绘制方面已经得到了很好的研究，但将其表示整合到深度学习框架中带来了新的挑战和机遇，这在现有的基于点集或隐式表示的方法中是看不到的。首先，当点足够密集且均匀分布于重构形状上时，才能最有效地定义隐式MLS曲面。然而大多数点生成方法固定点的数量并且消耗大量资源用于难以概括的密集点的预测,我们引入一个octree-based脚手架，只在需要时根据目标形状生成可变数量的MLS点，通过定制损失函数进一步调整点的分布。第二，为了度量训练监督和测试评估的预测隐函数，而现有的隐式方法必须在整个3D域上使用密集抽样，MLS表面自然地定位在生成点的窄带区域内，这促使我们只在八叉树节点上使用更简洁的抽样来进行监督和评估。此外，一旦所有的MLS点都嵌入到三维域中，评估就独立于网络，从而避免了其他隐式方法典型的点网络评估的代价。</p>
<p>我们使用广泛的消融试验来验证设计的选择。我们也通过三维物体的重建任务证明了我们的深度隐式MLS曲面方法比点集生成方法和其他全局或局部隐式函数方法都有更好的性能。</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Deep-representations-for-3D-generation"><a href="#Deep-representations-for-3D-generation" class="headerlink" title="Deep representations for 3D generation"></a>Deep representations for 3D generation</h3><p>点集是许多著作中常用的表示方法。但由于通常指定了点的个数，其表示细节几何的能力受到限制，需要进一步的点上采样来提高点密度和形状质量。</p>
<p>密集的体素很好地表征了形状占用率，但其高昂的内存成本妨碍了其用于表示高分辨率3D内容。稀疏体素包括八叉树克服了这些问题，在内存和计算方面都有很大的效率。</p>
<p>网格表示和基于patch的表示是提高形状质量的方便的3D表示。然而，它们的能力受到预定义的网格拓扑和分辨率，或多个补丁的断开连接的限制。中间3D表示，如粗体素或形状骨架是进一步提高其质量的可能方法。中间3D表示，如粗体素或形状骨架是进一步提高其质量的可能方法。</p>
<p>基于基元的表示使用一组简单的几何对象，如平面和立方体来近似3D形状。基于结构的表示明确地将语义部件结构编码为长方体，并使用体素表示在每个长方体内重构部件。虽然它们适用于表征形状结构，但由于基元的简单性，它们的近似质量也受到限制。</p>
<p>最近出现了基于隐式表面的深度学习方法提供了一种平滑连续的3D表示，并在连续空间中实现函数评价。对于一个给定的点，网络预测它的占用率或从它到表面的符号距离。这些技术最近得到了进一步的改进，通过整合局部特征来建模更详细的形状几何。</p>
<p>我们的方法属于深度隐式类，通过对光滑、连续的隐式MLS曲面进行建模，同时具有显式点集生成的灵活性和效率；它是一种混合的3D深度学习表示，结合了点集和隐函数的优点。</p>
<h3 id="Surface-reconstruction-from-point-clouds"><a href="#Surface-reconstruction-from-point-clouds" class="headerlink" title="Surface reconstruction from point clouds"></a>Surface reconstruction from point clouds</h3><p>表面重建技术已经研究了几十年。其中，利用全局或局部平滑先验对点云进行高质量重建的方法包括：多层分割的单位云(MPU)、泊松重建、径向基函数和移动最小二乘曲面(MLS) 广泛用于点集曲面建模和绘制。</p>
<p>由于MLS的快速和局部评价特性，我们选择MLS表面作为我们的深层3D表示。MLS曲面可以分为两种类型：投影MLS曲面和隐式MLS曲面(IMLS)。前者通过迭代投影定义一组平稳点，后者直接定义隐函数。我们使用基于IMLS的带符号的距离监控，实现了快速的功能评估。</p>
<h2 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h2><h3 id="IMLS-surface"><a href="#IMLS-surface" class="headerlink" title="IMLS surface"></a>IMLS surface</h3><p>隐式MLS曲面定义如下: 表示 $\mathcal{P}=\{p_{i} \in \mathbb{R}^{3} \}_{i=1}^{N}$ 为三维点集，每个点都具有单位法向 $n_{i} \in \mathbb{R}^{3}$，控制半径 $r_{i} \in \mathbb{R}^{+}$ 。为了方便起见，我们称这些点为MLS点。</p>
<p>对于每个MLS点 $p_{i}$ 到其切平面的有符号距离函数定义为 $＜x-p_{i},n_{i}＞$ ，其中 $＜\cdot,\cdot＞$ 是内积。通过加权平均所有逐点带符号的距离函数，我们得到了一个隐函数 $F(x)$ 的零水平集定义了隐式曲面 $\mathcal{S}$ :</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409163226.png" alt=""></p>
<p>这里我们设权函数 $\theta(d,r)=exp(-d^{2}/r^{2})$ 。Kolluri证明了在均匀采样条件下IMLS曲面 $\mathcal{S}$ 是对 $\mathcal{P}$ 进行采样的原始曲面的几何和拓扑正确的近似，而IMLS函数 $F$ 是对原始曲面的有符号距离函数的紧密近似。</p>
<p>由于当 $x$ 远离 $p_{i}$ 时，权函数衰减，因此只考虑附近的MLS点可以加速 $F(x)$ 的求值。Eq.(1)可以修改为:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409163632.png" alt=""></p>
<p>其中 $\Omega(x)$ 表示以 $x$ 为中心，半径为 $r_{b}$ 的球内的MLS点集合。 $r_{b}$ 可由用户设置为截断点距离值。</p>
<p>由于上述公式，函数值为零 $F(x)$ 存在于IMLS点的窄带区域。通过对边界区域内的规则网格进行函数求值，通过移动立方体可以有效地将 $\mathcal{S}$ 显式地提取为三角形网格。</p>
<h3 id="Deep-IMLS-surface"><a href="#Deep-IMLS-surface" class="headerlink" title="Deep IMLS surface"></a>Deep IMLS surface</h3><p>在实际三维重建中，稀疏的、无方向的点云可能带有噪声和缺失区域，这是典型的输入，但传统方法无法很好地处理这些输入三维重建方法，比如泊松重建。为了处理这种不完美的数据，我们的目标是设计一个自编码的神经网络来生成IMLS曲面。</p>
<p>定义网络输出的一种简单方法是设置固定数量的IMLS点元组 $\{ p_{i},n_{i},r_{i} \}_{i=1}^{N}$ 。但是，它会限制IMLS的表示能力，不能很好地从数据中学习局部几何先验。我们引入了一个中间网络输出：基于八叉树的脚手架，以帮助生成需要的MLS点。基于八叉树的支架是一个 $d$ 深度的八叉树 $\mathcal{O}$ ，它在多分辨率下大致近似3D表面。对于每一个最细的非空八分位数 $o_{k}$ ，也就是八叉树中最小的非空体素，我们关联一个小集合的MLS点，这些点的位置靠近八分位数中心 $c_{k}$ 。具体来说与 $o_{k}$ 关联的MLS点定义为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409170339.png" alt=""></p>
<p>其中 $t_{k,l}\in \mathbb{R}^{3}$ 是 $p_{k,l}$ 到 $c_{k}$ 的偏移向量， $s$ 是一个预定义的点的数目。由于基于八叉树的支架的结构依赖于目标表面，因此需要自适应地确定MLS点的总数及其位置。</p>
<p>有了上面的设置，一个适合IMLS生成的网络应该输出：</p>
<p>（1）一个基于八叉树的脚手架 $\mathcal{O}$ </p>
<p>（2）一个最细的非空八分圆 $o_{k}$ 的MLS点偏移量、MLS点法线和控制半径</p>
<p>这里我们注意到，由输入噪声和稀疏点云创建的八叉树不能用作脚手架，因为它可能是不完整和不准确的，并且不同于目标形状。</p>
<h4 id="Scaffold-prediction"><a href="#Scaffold-prediction" class="headerlink" title="Scaffold prediction"></a>Scaffold prediction</h4><p>使用基于八叉树的卷积神经网络(O-CNN)自动编码器来生成支架。其编码器采用输入点云构建的深度 $d_{in}$ 八叉树作为输入，并仅在八叉树内进行CNN计算。它的解码器以4 × 4 × 4的单元格开始，预测每个单元格是否空，如果单元格不空，则将其细分为8个八边形。这个过程递归地对每个非空的八进制执行，直到达到最大输出八叉树深度 $d_{out}$ 。</p>
<h4 id="MLS-point-prediction"><a href="#MLS-point-prediction" class="headerlink" title="MLS point prediction"></a>MLS point prediction</h4><p>与之前工作不同，解码器在每个最细的非空八分圆处回归定向点或平面补丁以实现亚体素精度，我们通过一个具有如下隐含层的多层感知器(MLP)来预测MLS点元组，该元组的特征向量用 $f(o_{k})$ 表示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409190020.png" alt=""></p>
<p>注意，MLP预测了 $p_{k,l}$ 的局部坐标，即 $t_{k,l}$ ，因此它可以从数据中学习局部先验。为了保证 $p_{k,l}$ 接近 $c_{k}$ ， $t_{k,l}$ 的每个坐标分量的取值范围限制在 $[−\beta h， \beta h]$ ，这里 $h$ 为最细的八分区的大小， $\beta$ 默认设置为1.5。我们还将 $r_{k,s}$ 限制在 $[l_{r}/ 2,2l_{r}]$ 内，其中 $l_{r} = \frac{h}{\sqrt{s}}$ 。这些约束是通过使用tanh激活网络输出来实现的，并通过其范围缩放值。</p>
<h3 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409191356.png" alt=""></p>
<p>论文使用类似 U-Net 的 O-CNN 自动编码器，其中包含 O-CNN ResNet 块和输出引导的跳过连接。对于给定的无方向点云，论文提出为其构建一个深度八叉树。在每个最细的八分圆中，通过将八分圆内输入点的平均位置的偏移量与带有二进制标量的八分圆中心的偏移量与=连接来设置输入 4 维信号，该二进制标量指示八分圆是否为空。</p>
<p>Resblock(n, c) 表示一个 n 层基于 O-CNN 的残差块，通道编号为 c。Downsample(c) 和 Upsample(c) 是基于八叉树的卷积和反卷积算子 [49]，然后是批量归一化和 ReLU。对于第一个 Resblock，c 设置为 64，并且在每个 Downsample 算子之后增加 2 倍，并且在每个 Upsample 算子之后除以 2。在论文的实验中，论文设置 n = 3。隐藏层 MLP 用于预测八分圆是否为空。</p>
<h3 id="Loss-function-design"><a href="#Loss-function-design" class="headerlink" title="Loss function design"></a>Loss function design</h3><h4 id="Octree-structure-loss"><a href="#Octree-structure-loss" class="headerlink" title="Octree structure loss"></a>Octree structure loss</h4><p>八分圆状态的确定是一个二元分类问题：0 代表空，1 代表非空。 论文使用 O 的每个八分圆处的 sigmoid 交叉熵损失的加权求和来定义八叉树结构损失。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409192717.png" alt=""></p>
<h4 id="SDF-loss"><a href="#SDF-loss" class="headerlink" title="SDF loss"></a>SDF loss</h4><p>IMLS曲面的预测值和真实值之间的差异用SDF损失表示</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409192809.png" alt=""></p>
<p>这里 F 的梯度可以近似为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409192845.png" alt=""></p>
<h4 id="MLS-point-repulsion-loss"><a href="#MLS-point-repulsion-loss" class="headerlink" title="MLS point repulsion loss"></a>MLS point repulsion loss</h4><p>用于改善生成的 MLS 点的局部规律性。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409192956.png" alt=""></p>
<p>其中 $||p_{i}-p_{j}||_{proj}=||(T-n_{i}n_{i}^{T})(p_{i}-p_{j})||$ 是 $p_{i}-p_{j}$ 在 $p_{i}$ 处的切平面上的投影长度， $w_{ij}$ 是关于两个 MLS 点差和法向差的双边权重，定义如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409193039.png" alt=""></p>
<p>上述设计将 $p_{j}$ 推离 $p_{i}$ ，尤其是当他们的法线和他们的位置是相似的。</p>
<h4 id="Projection-smoothness-loss"><a href="#Projection-smoothness-loss" class="headerlink" title="Projection smoothness loss"></a>Projection smoothness loss</h4><p>为了实现局部表面平滑度，论文鼓励 MLS 点靠近其相邻 MLS 点的切平面。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409193200.png" alt=""></p>
<h4 id="Radius-smoothness-loss"><a href="#Radius-smoothness-loss" class="headerlink" title="Radius smoothness loss"></a>Radius smoothness loss</h4><p>同样，为了提高表面平滑度，相邻 MLS 点的半径变化通过对半径进行加权拉普拉斯平滑来惩罚</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220409193248.png" alt=""></p>
<h4 id="Weight-decay"><a href="#Weight-decay" class="headerlink" title="Weight decay"></a>Weight decay</h4><p>在损失函数中加入一项小的权重衰减项，系数为$\lambda_{w}$</p>
<h1 id="Efficient-Training-of-Physics-Informed-Neural-Networks-via-Importance-Sampling"><a href="#Efficient-Training-of-Physics-Informed-Neural-Networks-via-Importance-Sampling" class="headerlink" title="Efficient Training of Physics-Informed Neural Networks via Importance Sampling"></a>Efficient Training of Physics-Informed Neural Networks via Importance Sampling</h1><h2 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h2><p>PINN训练只需要问题描述，定义域，初始/边界条件，这种训练通常涉及使用随机梯度下降法的变体来解决一个非凸优化问题，将损失函数的梯度近似于一批配点中，在每次迭代中按均匀分布随机选取。在每次训练迭代中，按照与损失函数成正比的分布对搭配点进行采样，将改善PINNs训练的收敛行为。</p>
<h2 id="Introduction-5"><a href="#Introduction-5" class="headerlink" title="Introduction"></a>Introduction</h2><p>由于计算资源和优化算法的限制，PINN没有得到太多的关注。PINN网络的训练通常涉及到使用迭代法求解非凸优化问题。在给定的迭代中，这种批量选择可能会导致在一些配置点上计算梯度，在这些配置点上，近似解相对于其他点已经满足了微分算子的一个令人满意的程度。因此，得到的梯度信息很少或根本没有，从而延缓了收敛速度。或者，通过采用重要采样方案，在每次迭代中，我们可以选择一批能提供更多梯度信息的配点，以加速收敛。</p>
<p>首先，我们借鉴文献的理论发现，提出了一种基于重要度抽样的PINN网络加速训练方法。据作者所知，这是第一次使用重要抽样方案对pin网络进行训练。其次，我们展示了如何使用最近邻搜索或Voronoi分布来逼近建议分布，以进一步改善PINN训练的收敛行为。提出的重要度抽样方法简单明了，易于应用于现有的重要度抽样方法通过修改代码的几行PINN代码。此外，该方法没有引入新的超参数。</p>
<h2 id="Deep-Learning-of-Differential-Equations"><a href="#Deep-Learning-of-Differential-Equations" class="headerlink" title="Deep Learning of Differential Equations"></a>Deep Learning of Differential Equations</h2><p>PINNs基础知识</p>
<h2 id="Importance-Sampling-for-Training-of-PINNs"><a href="#Importance-Sampling-for-Training-of-PINNs" class="headerlink" title="Importance Sampling for Training of PINNs"></a>Importance Sampling for Training of PINNs</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418100747.png" alt=""></p>
<p>$f(x)$ 是训练点的采样分布，一种典型的采样分布是定义域上的均匀分布。在一种重要抽样方法中，我们寻求从一个备选抽样分布（用 $q(x)$ 表示）中提取训练样本，并根据如下更新网络参数：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418101053.png" alt=""></p>
<p>在本工作中，我们有效地实现了一种离散采样方案，将连续域 $D$ 转化为 $N$  个采样点在 $D$ 上均匀选取的离散集合，其中 $N&gt;&gt; 1$ 。因此，我们将分别处理离散分布 $\{f_{j}\}_{j=1}^{N}$ 和 $\{ q_{j} \}_{j=1}^{N}$ ，而不是采样密度函数 $f(x_{j})$ 和 $q(x_{j})$，在任意候选点 $j$ 处 $f_{j} = \frac{1}{N}$ 。</p>
<p>为了构建相应的SGD，为了简洁起见，我们先考虑no mini batch，即 $m = 1$ ，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418102832.png" alt=""></p>
<p>在这项工作中，我们的目标是设计一个具有抽样分布 $q$ 的训练方案，它可以加速Eq. 12的收敛。[32]中的作者考虑了以下关于收敛速度的定义</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418103015.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418103050.png" alt=""></p>
<p>然后得出结论，通过从一个最小的分布 $Tr(\mathbb{V}_{P}[G^{(i)}])$ 中采样输入变量，可以加速收敛。如[32,34]所示，如果根据 $q^{\star}\propto||\triangledown_{\theta}J(\theta^{(i)})||_{2}$ 选择训练样本，则这一项可以被最小化。对于批量大小为m的小批量SGD，这个目标可以有效地被实现通过计算抽样分布</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418104326.png" alt=""></p>
<p>通过从概率为 $p^{(i)} = \{q_{1}^{(i)},\dots,q_{N}^{(i)}\}$ 的多项式中抽样 $M$ 个指标，选择小批量样本集 $M^{(i)}$ 。为了得到梯度 $\triangledown_{\theta}J(\theta)$ 的无偏估计，则根据式8，11给出的小批量梯度下降更新规则</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418104941.png" alt=""></p>
<p>这一推导提供了一个理论证据，即使用重要抽样方法可以加速PINN网络的训练，其中训练样本是根据与模型参数相关的损失函数梯度的2-范数成比例的分布获得的。然而，在每次迭代中为所有的并置点计算这个梯度的2-范数需要通过计算图进行额外的反向传播，这在计算上可能非常昂贵。为了缓解这一问题，在[33]中从理论上和数值上表明，在训练示例中损耗值的线性变换总是大于该示例中损耗梯度的2-范数，配点的梯度范数排序与损失值排序是一致的。因此，可以使用损失值代替梯度值作为重要性度量</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418105440.png" alt=""></p>
<p>具体来说，利用该proposal分布，可以选择前面解释的m个小批量样本，并结合公式16中的梯度下降规则来更新模型参数。</p>
<p>虽然与梯度计算相比，损失函数的计算成本较低，但在每次迭代中对整个配置点集进行这样的计算仍然非常昂贵。为了缓解这种情况，我们提出了一个分段常数近似的损失函数。也就是说，我们不是在每个配置点上计算损失函数，而是只在一个点子集上计算损失，以下称为”种子”，用 $\{x_{s}\}_{s=1}^{N}$ 表示， $S&lt;N$ 。然后，利用最近邻搜索算法，对每个配点 $j$ ，求出最接近的种子 $s = \rho(j)$，并设置该搭配点的损失值等于最接近种子的损失，即 $J(\cdot;x_{j}):=J(\cdot;x_{\rho(j)})$ 。这相当于使用种子生成Voronoi镶嵌，并在每个Voronoi单元中使用一个常数近似的损失。在数值例子中可以看出，与对整个配点求损函数的情况相比，这种分段常数近似提供了更高的计算效率。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418112136.png" alt=""></p>
<h1 id="PAGP-A-physics-assisted-Gaussian-process-framework-with-active-learning-for-forward-and-inverse-problems-of-partial-differential-equations"><a href="#PAGP-A-physics-assisted-Gaussian-process-framework-with-active-learning-for-forward-and-inverse-problems-of-partial-differential-equations" class="headerlink" title="PAGP: A physics-assisted Gaussian process framework with active learning for forward and inverse problems of partial differential equations"></a>PAGP: A physics-assisted Gaussian process framework with active learning for forward and inverse problems of partial differential equations</h1><h2 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h2><p>在这项工作中，建立了一个在偏微分方程(PDEs)中包含给定物理信息的高斯过程回归(GPR)模型:物理辅助高斯过程(PAGP)。该模型的目标可分为两类问题:给定偏微分方程的初始条件和边界条件的解或未知系数的发现。给定的物理信息被集成到高斯过程模型通过我们设计的GP损失函数。基于两种不同的训练标准GP模型的方法，本文给出了三种类型的损失函数。本文的第一部分介绍了连续时间模型，该模型将时域和空间域等同看待。已知的未知系数通过最小化设计的损失函数，偏微分方程可以与GP超参数联合学习。在离散时间模型中，我们首先选择一种时间离散方案来离散时域。然后在每个时间步上应用PAGP模型和该方案来近似最后时刻给定测试点的偏微分方程解。为了在这种情况下发现未知系数，需要两个特定时间的观测数据，并构造一个混合均方误差函数来获得最优系数。最后，提出了一种新的连续时间和离散时间混合模型。它融合了连续时间模型的灵活性和离散时间模型的精确性。讨论了采用不同GP损失函数选择不同模型的性能。建议的有效性PAGP方法在我们的数值部分进行了说明。</p>
<h2 id="Introduction-6"><a href="#Introduction-6" class="headerlink" title="Introduction"></a>Introduction</h2><p>如今，数据驱动的机器学习(ML)模型在科学计算和许多学科的发现中取得了巨大的成功。然而，仅仅将ML模型作为黑盒函数使用可能会由于忽略现有的物理规律或其他领域专业知识而导致性能较差。此外，目前大多数黑盒ML模型往往需要大量的数据和有限的泛化属性。因此，将ML模型与通常以偏微分方程(PDEs)形式存在的物理规律相结合就成为一个自然的热门话题。在所有数据驱动的ML模型中，高斯过程回归(GPR)，又称地质统计学中的克里格(Kriging)，是一种应用广泛的非参数模型贝叶斯模型构建一个廉价的代理复杂的科学和工程问题。高斯过程是唯一由其规定形式的均值和协方差函数决定的。它具有一个概率工作流，具有分析的易处理性，并从其后验分布返回稳健的方差估计。这也自然地量化了模型的不确定性。本文旨在将偏微分方程中包含的物理信息与GPR模型结合起来，解决正问题，即求解给定偏微分方程的解，以及反问题，即发现给定偏微分方程的未知系数。</p>
<p>简要介绍以前处理这类问题的两篇著作。</p>
<p>本文提出了一种将物理原理引入高斯过程回归模型的新方法。在标准GP中，均值函数和协方差函数的最优超参数可以通过两种不同的方式进行优化，即最小化负对数边际似然(NLML)或使用交叉验证的方法。在PAGP模型中，基于这两种方法构造了三种类型的损失函数。对于第一种方法，我们应用留一交叉验证(LOO-CV)构造一个损失函数。验证密度的对数作为拟合的交叉验证测度。根据惩罚GPR的思想，在这个损失函数中增加了一个额外的惩罚条款。这一项实际上是由配置点集上的PDE残差的绝对误差之和组成。第二个损失函数是相似的，除了对LOO-CV的拟合度的衡量是平方误差。对于最后一个，在原来的NLML函数中增加了一个相同的惩罚项。此外，还提出了一种自适应权值选择方法，以确定与惩罚项相乘的权值系数，使损失函数具有意义。在GP训练过程中，惩罚期限需要根据预先设定的配点来计算。因此，首先需要推导GP预测对时间t和空间位置x的导数。我们根据不同的问题设置开发连续时间模型和离散时间模型。对于连续时间模型，我们遵循中的步骤，直接使用GP预测公式，根据给定推导出关于t和x的n阶导数的解析表达式PDEs。对于离散时间模型，可以用类似的方法计算GP对x的导数。但是GP对t的导数需要用不同的方法计算。这里采用有限差分法进行计算。此外，还提出了一种新的两步混合模型，将连续时间模型和离散时间模型结合在一起。第一步遵循离散时间模型，但具有较大的时间步长。每一项的预测时间步长，和根据给定的初始条件和边界条件抽取的样本一起构成了偏微分方程第二步训练的数据集。然后利用连续时间模型对整个域的测试点进行预测。</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>本文的目的是利用包含物理信息的GP来求解偏微分方程的正问题和反问题。在这项工作中，我们考虑一般形式的参数化偏微分方程</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418171923.png" alt=""></p>
<p>边界条件</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418172009.png" alt=""></p>
<p>初值条件</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220418172045.png" alt=""></p>
<p>其中 $\mathcal{T}_{x}^{\lambda}$ 是一个一般的微分算子，可以是线性的，也可以是非线性的。下标表示操作符 $\mathcal{T}$ 作用的空间位置 $x$ 。上标表示只能部分知道的参数 $\lambda$ 。 $\Omega$ 是 $\mathbb{R}^{d}$ 的一个子集，$\Gamma$ 是 $\Omega$ 的边界。例如，考虑一维热方程： $u_{t}-\lambda u_{xx}=0$ 。这里，微分算子是 $\mathcal{T}_{x}^{\lambda}=\lambda \frac{\partial^{2}}{\partial x^{2}}$ 并且 $\lambda$ 是位置参数。在这种情况下我们考虑的正问题是找出解 $u(x,t)$ 给定特定边界条件 $g(x,t)$ ，初始条件 $h(x)$ 和系数 $\lambda$ ，而反问题是在偏微分方程中恢复未知系数 $\lambda$ 。注意，对于这两种类型的问题，偏微分方程的边界和初始条件也可以用可能被噪声污染的观测值来代替。</p>
<h3 id="Gaussian-process-regression"><a href="#Gaussian-process-regression" class="headerlink" title="Gaussian process regression"></a>Gaussian process regression</h3><p>略</p>
<h3 id="Derivatives-of-Gaussian-process-regression"><a href="#Derivatives-of-Gaussian-process-regression" class="headerlink" title="Derivatives of Gaussian process regression"></a>Derivatives of Gaussian process regression</h3><p>略</p>
<h3 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h3><p>连续模型、离散模型、混合模型</p>
<h3 id="Active-Learning"><a href="#Active-Learning" class="headerlink" title="Active Learning"></a>Active Learning</h3><p>假设训练数据集D由N个样本组成。这代表了知识的当前状态，给定PDE域中信息量最大的样本是通过最大化获取函数 $a_{N}(x)$ 来选择的</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419144543.png" alt=""></p>
<p>获取功能实际上量化了我们可以获得多少信息来评估或在这个数据站点上执行一个昂贵的实验。然后 $(x_{N+1},y_{N+1})$ 加到原始训练数据集D中，此时如果达到了预先设定的条件，则停止处理。否则，该过程重复迭代，直到满足停止条件或达到最大迭代次数。在我们的PAGP模型中，采集函数被选择为后验分布的方差:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419144750.png" alt=""></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>这篇文章主要就是在训练GPR时通过对不同问题模型的设定设置不同的损失函数，然后在新一轮选点时采用后验方差大的点。</p>
<h1 id="Breaking-the-Dilemma-of-Medical-Image-to-image-Translation"><a href="#Breaking-the-Dilemma-of-Medical-Image-to-image-Translation" class="headerlink" title="Breaking the Dilemma of Medical Image-to-image Translation"></a>Breaking the Dilemma of Medical Image-to-image Translation</h1><h2 id="Abstract-4"><a href="#Abstract-4" class="headerlink" title="Abstract"></a>Abstract</h2><p>有监督的Pix2Pix和无监督的Cycle-consistency两种模式在医学图像到图像的转换中占主导地位。然而，这两种模式都不是理想的。Pix2Pix模式具有出色的性能。但它需要成对和像素对齐的图像，这可能不总是可以实现，因为在获得成对图像期间，呼吸运动或解剖变化。循环一致性模式对训练数据不太严格，对未配对或错位的图像也能很好地工作。但它的性能可能不是最优的。为了打破现有模式的困境，我们提出了一种新的无监督模式RegGAN用于医学图像到图像的转换。它基于“损失校正”理论。在RegGAN算法中，将失调的目标图像作为噪声标签，利用附加的配准网络对生成器进行训练，自适应地拟合失调的噪声分布。目标是搜索图像之间的转换和配准任务的共同最优解。我们将RegGAN合并到一些最先进的图像到图像的转换方法中，并证明RegGAN可以很容易地与这些方法结合起来，以提高它们的性能。例如，在我们的模式中，简单的CycleGAN超过了最新的NICEGAN，即使使用更少的网络参数。根据我们的结果，RegGAN在对齐数据上优于Pix2Pix，在未对齐或未配对的数据上优于Cycle-consistency。RegGAN对噪声不敏感，这使得它在很多情况下都是更好的选择，特别是在无法获得像素级对齐数据的医学图像到图像转换任务中。</p>
<h2 id="Introduction-7"><a href="#Introduction-7" class="headerlink" title="Introduction"></a>Introduction</h2><p>生成对抗网络(GANs)是一个通过对抗过程同时训练生成器G和鉴别器D的框架。该生成器用于将源域图像X的分布转换为目标域图像Y的分布。判别器用于确定目标域图像可能来自生成器还是来自真实数据。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419210104.png" alt=""></p>
<p>Pix2Pix更新生成器 $(G:X \rightarrow Y)$ ，使源图像x和目标图像Y之间的像素级L1损失最小。因此，它要求对齐良好的成对图像，其中每个像素都有对应的标签。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419210302.png" alt=""></p>
<p>然而，在现实场景中，对齐良好的成对图像并不总是可用的。为了解决图像不对齐所带来的挑战，我们开发了循环一致性算法，该算法基于这样的假设:从源域X到目标域Y $(G:X \rightarrow Y)$ 是生成器F从Y到X的反向 $(F:Y \rightarrow X)$ .与Pix2Pix模式相比，循环一致性模式在不对齐或未配对的图像上工作得更好。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419210602.png" alt=""></p>
<p>然而，Cycle-consistency模式有其局限性。在医学图像到图像的转换中，不仅需要图像域之间的风格转换，还需要特定图像对之间的转换。最佳解决方案应该是唯一的。例如，翻译后的图像应尽可能保持原图像的解剖特征。众所周知，Cycle-consistency模式可能会产生多个解，这意味着训练过程可能比较混乱，结果可能不准确。Pix2Pix模式也不理想。即使它有唯一的解决方案，也很难满足成对图像对齐的要求。对于错位的图像，误差通过Pix2Pix模式，可能导致最终的平移图像不合理的位移。</p>
<p>到目前为止，还没有一种图像到图像的转换模式可以在对齐数据上优于Pix2Pix模式，在未对齐或未配对数据上优于Cycle-consistency模式。受[6-10]的启发，我们将失调的目标图像视为有噪声的标签，这意味着我们将存在的问题视为带噪声标签的监督学习。</p>
<h2 id="Methodology-1"><a href="#Methodology-1" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="Theoretical-Motivation"><a href="#Theoretical-Motivation" class="headerlink" title="Theoretical Motivation"></a>Theoretical Motivation</h3><p>如果我们将失调的目标图像视为有噪声的标签，那么图像到图像的翻译训练就变成了一个有噪声标签的监督学习过程。给定一个训练数据集 $\{ (x_{n},\widetilde{y}_{n}) \}_{n=1}^{N}$ ，有n个噪声标签，其中 $x_{n}$, $\widetilde{y}_{n}$ 是来自两种模式的图像，假设 $y_{n}$ 是 $x_{n}$ 的正确标签，但在现实场景中是未知的。我们的目标是使用数据集 $\{ (x_{n},\widetilde{y}_{n}) \}_{n=1}^{N}$ 带噪声的标签，其性能相当于在干净数据集 $\{ (x_{n},y_{n}) \}_{n=1}^{N}$ 尽可能多。基于方程4的直接优化通常是无效的，并且会导致不好的结果，因为发生器不能挤出噪声的影响。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419212746.png" alt=""></p>
<p>为了解决噪声问题，我们提出了一个基于“损耗校正”的解决方案，如方程5所示。我们的解决方案通过建模噪声转移 $\phi$ 来匹配噪声分布来校正生成器 $G(x_{n})$ 的输出。之前，Patrini et al从数学上证明了用噪声标签训练的模型可以等价于用干净标签训练的模型，只要噪声发生转移 $\phi$ 匹配噪声分布。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419213250.png" alt=""></p>
<p>为此，Goldberger等提出将正确的标签视为潜在的随机变量，并将标签噪声显式建模为网络结构的一部分，记为 $R$ 。方程5可以改写为对数似然的形式，将对数似然作为神经网络训练的损失函数。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419213846.png" alt=""></p>
<h3 id="RegGAN"><a href="#RegGAN" class="headerlink" title="RegGAN"></a>RegGAN</h3><p>与现有的使用的求解方程6的方法例如最大化期望，全连通层、锚点估计和Drichlet分布比较。在我们的问题中，噪声分布的类型更清楚，它可以表示为位移误差: $\widetilde{y}=y\circ T$ 。这里T表示为一个随机变形场，它为每个像素产生随机位移。因此，我们采用生成器G后的配准网络R作为标签噪声模型对结果进行校正。修正损失如式7所示</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419214445.png" alt=""></p>
<p>式中 $R(G(x), \widetilde{y})$ 为变形场， $\circ$ 代表重采样操作。注册网络基于U-Net。在式8中定义了平滑损失来评价变形场的平滑性，使变形场的梯度最小。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419214930.png" alt=""></p>
<p>最后，将产生器与鉴别器之间的平均损耗相加(式1)，总损耗如式9所示</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220419215013.png" alt=""></p>
<h1 id="Characterizing-possible-failure-modes-in-physics-informed-neural-networks"><a href="#Characterizing-possible-failure-modes-in-physics-informed-neural-networks" class="headerlink" title="Characterizing possible failure modes in physics-informed neural networks"></a>Characterizing possible failure modes in physics-informed neural networks</h1><h2 id="Abstract-5"><a href="#Abstract-5" class="headerlink" title="Abstract"></a>Abstract</h2><p>我们证明，虽然现有的PINN方法可以学习相对次要问题的良好模型，但它们很容易无法学习相关的物理现象，即使是稍微复杂一点的问题。特别地，我们分析了几个不同的情况，广泛的物理兴趣，包括学习微分方程的对流，反应和扩散算子。我们提供证据，软正则化的PINN，其中涉及到基于偏微分算子，可以引入许多微妙的问题，包括使问题更病态。重要的是，我们表明，这些可能的失效模式不是由于缺乏神经网络结构的表现力，而是PINN的设置使损失景观非常难以优化。然后，我们描述了解决这些故障模式的两个有希望的解决方案。第一种方法是使用课程正则化，其中PINN的损失项从一个简单的PDE正则化开始，并随着NN的训练逐渐变得更加复杂。第二种方法是将问题作为一个顺序对顺序的学习任务，而不是学习一次性预测整个时空。大量的测试表明，与常规的PINN训练相比，我们可以实现高达1-2个数量级的误差。</p>
<h2 id="Introduction-8"><a href="#Introduction-8" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Problem-overview"><a href="#Problem-overview" class="headerlink" title="Problem overview"></a>Problem overview</h3><p>PINN 相关知识</p>
<h3 id="Main-contributions-1"><a href="#Main-contributions-1" class="headerlink" title="Main contributions"></a>Main contributions</h3><ul>
<li>我们分析了简单但物理相关的对流、反应和反应扩散问题的PINN模型。我们发现，普通/常规的PINN方法只适用于非常简单的参数体系。</li>
<li>我们分析了训练过的PINN模型的损失情况，发现增加基于pde的软约束正则化使其更加复杂和难以优化，特别是对于具有非平凡系数的情况。</li>
<li>我们证明了 NN 架构有能力/表现力来找到一个好的解，从而表明这些问题不是由于NN网格结构的容量有限引起的。相反，我们认为失败是由于相关的优化困难使用 PINN 的软 PDE 约束。</li>
<li>我们提出了解决这些失败模式的两种途径:(i)课程正规化(ii)将学习问题作为一个序列对序列的学习任务。</li>
</ul>
<h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p>一些关于机器学习和PDE结合的工作</p>
<h2 id="Possible-failure-modes-for-physics-informed-neural-networks"><a href="#Possible-failure-modes-for-physics-informed-neural-networks" class="headerlink" title="Possible failure modes for physics-informed neural networks"></a>Possible failure modes for physics-informed neural networks</h2><h3 id="Experiment-setup"><a href="#Experiment-setup" class="headerlink" title="Experiment setup."></a>Experiment setup.</h3><p>4-layer fully-connected NN with 50 neurons per layer; tangent activation function; randomly sample collocation points on the domain; measure relative error and absolute error</p>
<h3 id="convection、reaction-diffusion"><a href="#convection、reaction-diffusion" class="headerlink" title="convection、reaction-diffusion"></a>convection、reaction-diffusion</h3><p> we can see that the PINN also fails to learn advection and reaction-diffusion.  </p>
<h2 id="Diagnosing-possible-failure-modes-for-physics-informed-NNs"><a href="#Diagnosing-possible-failure-modes-for-physics-informed-NNs" class="headerlink" title="Diagnosing possible failure modes for physics-informed NNs"></a>Diagnosing possible failure modes for physics-informed NNs</h2><h3 id="Soft-PDE-regularization-and-optimization-difficulties"><a href="#Soft-PDE-regularization-and-optimization-difficulties" class="headerlink" title="Soft PDE regularization and optimization difficulties"></a>Soft PDE regularization and optimization difficulties</h3><p>我们表明，添加软正则化实际上可以使问题更难优化，即正则化导致更不平滑的损失景观。</p>
<p>最后，我们研究了改变软正则化项的权重/乘子的影响，这可能与提高PINN性能有关。虽然我们发现调谐λ可以帮助改变误差，但它不能解决问题。</p>
<h2 id="Expressivity-versus-optimization-difficulty"><a href="#Expressivity-versus-optimization-difficulty" class="headerlink" title="Expressivity versus optimization difficulty"></a>Expressivity versus optimization difficulty</h2><h3 id="Curriculum-PINN-Regularization"><a href="#Curriculum-PINN-Regularization" class="headerlink" title="Curriculum PINN Regularization"></a>Curriculum PINN Regularization</h3><p>我们设计了一个“课程正则化”，方法通过为权值找到一个好的初始化值来热启动神经网络训练。对于$\beta/\rho$较高的情况，我们不是训练PINN立即学习解，而是先在较低的$\beta/\rho$上训练PINN(对PINN来说更容易学习)，然后逐渐分别在较高的$\beta/\rho$上训练PINN。</p>
<h3 id="Sequence-to-sequence-learning-vs-learning-the-entire-space-time-solution"><a href="#Sequence-to-sequence-learning-vs-learning-the-entire-space-time-solution" class="headerlink" title="Sequence-to-sequence learning vs learning the entire space-time solution"></a>Sequence-to-sequence learning vs learning the entire space-time solution</h3><p>在这里，我们证明，将问题作为一个序列对序列(seq2seq)学习任务可能更好，其中神经网络学习预测下一个时间步骤的解决方案，而不是一直预测。这样，我们就可以使用时间推进方案来预测不同的序列/时间点。注意，这里唯一可用的数据来自PDE本身，也就是说，只有初始条件。我们取在$t=\Delta t$处的预测，以此作为在$t=2\Delta t$处的预测的初始条件，依此类推。</p>
<h1 id="Uncertainty-Quantification-in-Scientific-Machine-Learning-Methods-Metrics-and-Comparisons"><a href="#Uncertainty-Quantification-in-Scientific-Machine-Learning-Methods-Metrics-and-Comparisons" class="headerlink" title="Uncertainty Quantification in Scientific Machine Learning:Methods, Metrics, and Comparisons"></a>Uncertainty Quantification in Scientific Machine Learning:Methods, Metrics, and Comparisons</h1><h2 id="Abstract-6"><a href="#Abstract-6" class="headerlink" title="Abstract"></a>Abstract</h2><p>神经网络在如何将数据和物理工程方面的数学定理融合方面正在改变新的计算范式。然而，在基于神经网络的推理中，对误差和不确定性的量化比传统方法更加复杂。这是因为除了与噪声数据相关的任意不确定性外，还存在数据有限的不确定性，还有神经网络超参数、过度参数化、优化和采样误差以及模型误规范等。在这项工作中，我们提出了一个全面的框架，包括不确定性建模、新的和现有的解决方法，以及评估指标和事后改进方法。为了证明我们的框架的适用性和可靠性，我们提出了一个广泛的比较研究，其中各种方法在原型问题上进行了测试，包括混合输入输出数据问题和高维随机问题。</p>
<h2 id="Introduction-9"><a href="#Introduction-9" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Novel-contributions-of-this-work"><a href="#Novel-contributions-of-this-work" class="headerlink" title="Novel contributions of this work"></a>Novel contributions of this work</h3><ol>
<li>我们测试并将各种用于后验推理、先验学习、数据噪声建模以及训练后校准的方法集成到物理信息神经网络、神经算子和SPDE求解器中。</li>
<li>我们演示了如何使用函数先验来解决具有异方差噪声的历史数据的函数逼近问题。</li>
<li>将高斯过程回归和生成式对抗网络相结合，提出了一种解决确定性正向偏微分方程问题的新方法，并与现有方法进行了比较。</li>
<li>我们求解源项、问题参数和解数据中含有异方差噪声的混合偏微分方程问题。</li>
<li>我们解决了带有噪声的随机实现的混合SPDE问题，并提出了一种新的神经网络结构，用于使用多项式混沌量化不确定性。</li>
<li>我们演示了如何处理有噪声和不完整的推断数据给出一个预先训练的神经算子对干净的数据。</li>
<li>我们提出了检测神经算子外分布数据的方法，这对风险相关的应用是至关重要的。</li>
<li>我们提出了一个统一的UQ框架，通过无缝地将物理与可能被各种类型的噪声污染的新数据和历史数据结合起来，解决科学机器学习中的各种问题。</li>
</ol>
<h2 id="Neural-PDEs-and-neural-operators"><a href="#Neural-PDEs-and-neural-operators" class="headerlink" title="Neural PDEs and neural operators"></a>Neural PDEs and neural operators</h2><h3 id="Solving-forward-and-mixed-PDE-problems-Overview-of-PINN-method"><a href="#Solving-forward-and-mixed-PDE-problems-Overview-of-PINN-method" class="headerlink" title="Solving forward and mixed PDE problems: Overview of PINN method"></a>Solving forward and mixed PDE problems: Overview of PINN method</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506153709.png" alt=""></p>
<h3 id="Learning-operator-mappings-Overview-of-DeepONet-method"><a href="#Learning-operator-mappings-Overview-of-DeepONet-method" class="headerlink" title="Learning operator mappings: Overview of DeepONet method"></a>Learning operator mappings: Overview of DeepONet method</h3><p>DeepONet方法通过构造一个以 $\theta$ 为参数的神经网络逼近器来处理算子学习问题，来对 $u(x;\xi)$ 。</p>
<h2 id="Modeling-total-uncertainty"><a href="#Modeling-total-uncertainty" class="headerlink" title="Modeling total uncertainty"></a>Modeling total uncertainty</h2><h3 id="Uncertainty-in-function-approximation"><a href="#Uncertainty-in-function-approximation" class="headerlink" title="Uncertainty in function approximation"></a>Uncertainty in function approximation</h3><p>为了定义 $p(u|x,\theta)$ ，我们构建一个模型 $u_{\theta}(x)$ 在一些 $x$ 点来捕捉 $u(x)$ 的确定性部分并且为噪声假定一个模型。例如，因式高斯似然函数，通过如下式子给出</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506161343.png" alt=""></p>
<p>下标 $d$ 表示 $u$ 的每个 $D_{u}$ 维，通常用于多维函数逼近问题。在公式(4)中，输出向量 $u_{\theta}(x)$ 是假设 $u$ 在位置 $x$ 处的高斯分布的均值， $diag(\Sigma^{2}_{u})$ 是一个对角协方差矩阵 $\Sigma_{u}^{2} = [\sigma_{u}^{2},\dots,\sigma_{u}^{2}]$ 可以是已知的，也可以是假设的，也可以是从数据推断出来的。</p>
<p>在给定数据 $\mathcal{D}$ 的情况下， $x$ 位置 $u$ 的值是一个随机变量，表示为 $(u|x,\mathcal{D})$。为了求 $(u|x;\mathcal{D})$ 积分出模型参数 $\theta$，即</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506163025.png" alt=""></p>
<p>利用贝叶斯规则，后验 $p(\theta|\mathcal{D})$ 可以通过下式获得</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506185405.png" alt=""></p>
<p>式(6)中， $p(\mathcal{D}|\theta)$ 是数据的似然，即 $p(\mathcal{D}|\theta) = \prod_{i=1}^{N}p(u_{i}|x_{i},\theta)$ 为独立同分布(i.i.d.)数据； $p(\theta)$为模型 $\mathcal{H}$ 定义的参数 $\theta$ 的先验概率；而 $p(\mathcal{D})$ 被称为边际可能性或证据，因为它代表了在所有由 $\mathcal{H}$ 建模的可能数据集中，我们观察到 $\mathcal{D}$ 发生的概率。证据 $p(\mathcal{D})$ 给出如下</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506190851.png" alt=""></p>
<p>即，给定从先验 $p(\theta)$ 中抽取的随机样本，然后与似然函数结合使用， $p(\mathcal{D})$ 表示数据集 $\mathcal{D}$ 产生的概率。</p>
<p>后验推断阶段之后，Eq.(5)的BMA可以用蒙特卡罗(MC)来近似。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506191803.png" alt=""></p>
<p>这个方程提供了 $(u|x,\mathcal{D})$ 以预测PDF $\overline{p}(\mu|x)$ 的形式。 $(\mu|x,\mathcal{D})$ 通过 $\hat{u}(x) = E[\mu|x]$ 建模并且用 $\overline{\mu}(x)$近似表示为</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506193734.png" alt=""></p>
<p>其中 $\{\mu_{\hat{\theta}_{j}}(x)\}_{j=1}^{M}$ 是样本 $\{\hat{\theta}_{j} \}_{j=1}^{M}$ 对应的NN预测集。求 $(u|x,\mathcal{D})$ ，将式(4)的高斯似然值代入式(8)，得到高斯混合协方差矩阵的对角线部分由下式给出</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/20220506195407.png" alt=""></p>
<h3 id="Uncertainty-in-PINNs"><a href="#Uncertainty-in-PINNs" class="headerlink" title="Uncertainty in PINNs"></a>Uncertainty in PINNs</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922185741.png" alt=""></p>
<h3 id="Uncertainty-in-DeepONets"><a href="#Uncertainty-in-DeepONets" class="headerlink" title="Uncertainty in DeepONets"></a>Uncertainty in DeepONets</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922190052.png" alt=""></p>
<h2 id="Methods-for-uncertainty-quantification"><a href="#Methods-for-uncertainty-quantification" class="headerlink" title="Methods for uncertainty quantification"></a>Methods for uncertainty quantification</h2><ul>
<li>Bayesian methods</li>
<li>Ensembles</li>
<li>Functional priors (FPs)</li>
<li>Solving stochastic PDEs (SPDEs)</li>
<li>Towards a unified view of the presented methods</li>
</ul>
<h1 id="Multi-Objective-Loss-Balancing-for-Physics-Informed-Deep-Learning"><a href="#Multi-Objective-Loss-Balancing-for-Physics-Informed-Deep-Learning" class="headerlink" title="Multi-Objective Loss Balancing for Physics-Informed Deep Learning"></a>Multi-Objective Loss Balancing for Physics-Informed Deep Learning</h1><h2 id="Abstract-7"><a href="#Abstract-7" class="headerlink" title="Abstract"></a>Abstract</h2><p>在这项工作中，我们观察到对多个竞争损失函数组合进行正确加权对有效训练PINN的重要作用。为此，我们实现并评估了不同的方法，旨在平衡PINN损失函数的多个项及其梯度的贡献。我们提出了一种新的自适应损失平衡称为ReLoBRaLo(相对随机回看的损失平衡)。我们的模拟研究证明了这一点与使用其他平衡方法训练PINN相比，ReLoBRaLo训练速度更快，准确率更高，因此非常有效，并增加了PINN算法的可持续性。</p>
<h2 id="Introduction-10"><a href="#Introduction-10" class="headerlink" title="Introduction"></a>Introduction</h2><p>物理信息神经网络的出现引起了人们对经常面临低数据系统问题的领域的极大兴趣。通过利用已知的物理定律，并将其作为隐式先验并入深度学习管道，PINN被证明需要很少或不需要数据，以近似不同复杂度的偏微分方程(PDE)。</p>
<h2 id="Physics-Informed-Neural-Networks-PINNs"><a href="#Physics-Informed-Neural-Networks-PINNs" class="headerlink" title="Physics-Informed Neural Networks (PINNs)"></a>Physics-Informed Neural Networks (PINNs)</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922182417.png" alt=""></p>
<p>然而，PINN训练的效率、收敛性和准确性仍面临严峻挑战。目前的研究可分为四种主要方法:修改神经网络的结构、分治/区域分解、参数初始化和损失平衡。</p>
<p>根据文献综述，自适应PINN训练过程可以被视为PDE约束的优化问题。本文关注的是对竞争力和适应性的仔细考虑，并从跨机器学习的几个领域提出的损失平衡技术中寻求灵感。</p>
<h2 id="Methodology-2"><a href="#Methodology-2" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="Multi-Objective-Optimisation"><a href="#Multi-Objective-Optimisation" class="headerlink" title="Multi-Objective Optimisation"></a>Multi-Objective Optimisation</h3><p>多目标优化可以通过线性扩展转化为单一目标：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922183152.png" alt=""></p>
<h3 id="Adaptive-Loss-Balancing-Methods"><a href="#Adaptive-Loss-Balancing-Methods" class="headerlink" title="Adaptive Loss Balancing Methods"></a>Adaptive Loss Balancing Methods</h3><h4 id="Learning-Rate-Annealing"><a href="#Learning-Rate-Annealing" class="headerlink" title="Learning Rate Annealing"></a>Learning Rate Annealing</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922183302.png" alt=""></p>
<h4 id="GradNorm"><a href="#GradNorm" class="headerlink" title="GradNorm"></a>GradNorm</h4><p>更新内部刻度的损失GradNorm的计算方法如下:</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922183445.png" alt=""></p>
<p>更新网络参数的最终损失只是使用之前更新的缩放值进行线性扩展：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922183525.png" alt=""></p>
<h4 id="SoftAdapt"><a href="#SoftAdapt" class="headerlink" title="SoftAdapt"></a>SoftAdapt</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922183605.png" alt=""></p>
<h2 id="Relative-Loss-Balancing-with-Random-Lookback-ReLoBRaLo"><a href="#Relative-Loss-Balancing-with-Random-Lookback-ReLoBRaLo" class="headerlink" title="Relative Loss Balancing with Random Lookback (ReLoBRaLo)"></a>Relative Loss Balancing with Random Lookback (ReLoBRaLo)</h2><p>从现有平衡技术中汲取灵感，我们提出了一种新的方法和实现，用于平衡扩展MOO损失函数中的多个项，用于训练PINN：</p>
<ul>
<li>采用SoftAdapt的平衡方法，利用连续训练步骤之间的变化率，并通过softmax函数进行归一化。</li>
<li>与学习率退火类似，为了利用过去不止一个训练步骤的损失统计数据，使用指数衰减来更新标量。</li>
<li>此外，在指数衰减中引入了一个随机回看(称为saudade $\rho$)，它决定是使用穿透步骤的损失统计来计算缩放，还是一直回看直到训练$\mathcal{L}_{i}^{(0)}$开始。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922184617.png" alt=""></p>
<h2 id="Hyperparameter-Tuning-and-Meta-Learning"><a href="#Hyperparameter-Tuning-and-Meta-Learning" class="headerlink" title="Hyperparameter Tuning and Meta Learning"></a>Hyperparameter Tuning and Meta Learning</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/微信截图_20220922184701.png" alt=""></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%80%BB%E7%BB%93/" rel="tag"># 总结</a>
              <a href="/tags/%E8%AE%BA%E6%96%87/" rel="tag"># 论文</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/10/09/%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E6%80%BB%E7%BB%93/" rel="prev" title="项目部署总结">
                  <i class="fa fa-chevron-left"></i> 项目部署总结
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/11/07/Python%E6%8B%BE%E9%81%97/" rel="next" title="Python知识点">
                  Python知识点 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">木霈玖</span>
</div>

    </div>
  </footer>

  
  <script src="//unpkg.com/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//unpkg.com/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
