<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//unpkg.com/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//unpkg.com/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="前言了解一下大语言模型的相关内容。 主要参考清华刘知远团队的课程。">
<meta property="og:type" content="article">
<meta property="og:title" content="大语言模型">
<meta property="og:url" content="http://example.com/2024/04/09/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="木霈玖的博客">
<meta property="og:description" content="前言了解一下大语言模型的相关内容。 主要参考清华刘知远团队的课程。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101742998.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101757934.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101804232.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101806486.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101809525.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101817772.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404102043123.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404102105511.png">
<meta property="article:published_time" content="2024-04-09T08:00:59.000Z">
<meta property="article:modified_time" content="2024-04-10T13:59:55.871Z">
<meta property="article:author" content="木霈玖">
<meta property="article:tag" content="总结">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="大语言模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101742998.png">


<link rel="canonical" href="http://example.com/2024/04/09/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>大语言模型 | 木霈玖的博客</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">木霈玖的博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NLP%E5%9F%BA%E7%A1%80"><span class="nav-number">2.</span> <span class="nav-text">NLP基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E4%BB%BB%E5%8A%A1"><span class="nav-number">2.1.</span> <span class="nav-text">基本任务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%8D%E8%A1%A8%E7%A4%BA"><span class="nav-number">2.2.</span> <span class="nav-text">词表示</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%8D%E8%A1%A8%E7%A4%BA%E7%9A%84%E7%9B%AE%E6%A0%87"><span class="nav-number">2.2.1.</span> <span class="nav-text">词表示的目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%8D%E8%A1%A8%E7%A4%BA%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">2.2.2.</span> <span class="nav-text">词表示的方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.3.</span> <span class="nav-text">语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BB%BB%E5%8A%A1"><span class="nav-number">2.3.1.</span> <span class="nav-text">语言模型的任务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-gram-model"><span class="nav-number">2.3.2.</span> <span class="nav-text">N-gram model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">存在的问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Neural-Language-Model"><span class="nav-number">2.3.3.</span> <span class="nav-text">Neural Language Model</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer%E5%92%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.</span> <span class="nav-text">Transformer和预训练语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">3.1.</span> <span class="nav-text">注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%89%B9%E7%82%B9"><span class="nav-number">3.1.1.</span> <span class="nav-text">注意力机制特点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer"><span class="nav-number">3.2.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA"><span class="nav-number">3.2.1.</span> <span class="nav-text">动机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E8%A7%88"><span class="nav-number">3.2.2.</span> <span class="nav-text">概览</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E7%BC%96%E7%A0%81"><span class="nav-number">3.2.3.</span> <span class="nav-text">输入编码</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Byte-Pair-Encoding-BPE"><span class="nav-number">3.2.3.1.</span> <span class="nav-text">Byte Pair Encoding(BPE)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="nav-number">3.2.3.2.</span> <span class="nav-text">位置编码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder"><span class="nav-number">3.2.4.</span> <span class="nav-text">Encoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decoder"><span class="nav-number">3.2.5.</span> <span class="nav-text">Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tricks"><span class="nav-number">3.2.6.</span> <span class="nav-text">Tricks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-PLMs"><span class="nav-number">3.3.</span> <span class="nav-text">预训练语言模型(PLMs)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%A4%E7%A7%8D%E4%B8%BB%E6%B5%81%E6%96%B9%E5%BC%8F"><span class="nav-number">3.3.1.</span> <span class="nav-text">两种主流方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GPT"><span class="nav-number">3.3.2.</span> <span class="nav-text">GPT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BERT"><span class="nav-number">3.3.3.</span> <span class="nav-text">BERT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MoE"><span class="nav-number">3.3.4.</span> <span class="nav-text">MoE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformers%E5%B7%A5%E5%85%B7"><span class="nav-number">3.3.5.</span> <span class="nav-text">Transformers工具</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Pipeline"><span class="nav-number">3.3.5.1.</span> <span class="nav-text">Pipeline</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tokenization"><span class="nav-number">3.3.5.2.</span> <span class="nav-text">Tokenization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pre-trained-Model"><span class="nav-number">3.3.5.3.</span> <span class="nav-text">Pre-trained Model</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Advanced-Adaptation"><span class="nav-number">4.</span> <span class="nav-text">Advanced Adaptation</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">木霈玖</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/04/09/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="木霈玖">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="木霈玖的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          大语言模型
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-04-09 16:00:59" itemprop="dateCreated datePublished" datetime="2024-04-09T16:00:59+08:00">2024-04-09</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2024-04-10 21:59:55" itemprop="dateModified" datetime="2024-04-10T21:59:55+08:00">2024-04-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">总结</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>了解一下大语言模型的相关内容。</p>
<p>主要参考清华刘知远团队的课程。</p>
<a id="more"></a>
<h1 id="NLP基础"><a href="#NLP基础" class="headerlink" title="NLP基础"></a>NLP基础</h1><p>自然语言处理：让计算机理解人类所说的语言。</p>
<p>相关综述：《Advances in Natural Language Processing》</p>
<h2 id="基本任务"><a href="#基本任务" class="headerlink" title="基本任务"></a>基本任务</h2><ul>
<li>词性标注：名词、动词、形容词的识别</li>
<li>命名实体的识别：人名、地名、机构名、日期的识别</li>
<li>共指消解：代词指向的是哪个实体</li>
<li>依赖关系：句子中成分的句法关系</li>
<li>中文的分词</li>
</ul>
<h2 id="词表示"><a href="#词表示" class="headerlink" title="词表示"></a>词表示</h2><h3 id="词表示的目标"><a href="#词表示的目标" class="headerlink" title="词表示的目标"></a>词表示的目标</h3><ul>
<li>词之间的相似度计算</li>
<li>词之间的语义关系</li>
</ul>
<h3 id="词表示的方法"><a href="#词表示的方法" class="headerlink" title="词表示的方法"></a>词表示的方法</h3><ul>
<li>相关词表示法：近义词、反义词、上位词（细微差异难以区分、新词义难以处理、主观性、数据吸收、大量人工）</li>
<li>独热表示法：每个词表示成独立的符号（表示词的时候假设不同的词之间是正交的）</li>
<li>上下文表示法：词用上下文出现的频度表示（需要大量存储空间、对于不经常出现的词表示变得稀疏）</li>
<li>词嵌入：将每个词学到低维稠密的向量空间当中</li>
</ul>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><h3 id="语言模型的任务"><a href="#语言模型的任务" class="headerlink" title="语言模型的任务"></a>语言模型的任务</h3><ul>
<li>计算一个词的序列成为一句合法的话的概率（Joint probability）</li>
<li>根据前面的词预测后面的词（conditional probability）</li>
</ul>
<h3 id="N-gram-model"><a href="#N-gram-model" class="headerlink" title="N-gram model"></a>N-gram model</h3><p>考虑连续出现N个词的概率（马尔可夫假设）：例如 $P(w_{j}|nerver \ too \ late \ to)=\frac{count(too \ late \ to \ w_{j})}{count(too \ late \ to)}$</p>
<h4 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h4><ul>
<li>N-gram model在实际使用时很少考虑比较长的上下文，因为前文越长，统计结果会越稀疏</li>
<li>N-gram model无法计算相似度</li>
</ul>
<h3 id="Neural-Language-Model"><a href="#Neural-Language-Model" class="headerlink" title="Neural Language Model"></a>Neural Language Model</h3><p>首先将上下文的每个词表示为低维的向量，然后将这些向量拼接成一个上下文向量，最后经过非线性变换预测下一个词。</p>
<h1 id="Transformer和预训练语言模型"><a href="#Transformer和预训练语言模型" class="headerlink" title="Transformer和预训练语言模型"></a>Transformer和预训练语言模型</h1><h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>通过求注意力分数来进行加权求和，得到一个与隐向量维度相同的输出向量，该向量包含decoder需要的所有encoder的信息。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101742998.png" alt=""></p>
<h3 id="注意力机制特点"><a href="#注意力机制特点" class="headerlink" title="注意力机制特点"></a>注意力机制特点</h3><ul>
<li>解决信息瓶颈问题</li>
<li>缓解梯度消失问题</li>
<li>提供了可解释性</li>
</ul>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><ul>
<li>RNN当中的顺序计算不利于GPU并行</li>
<li>RNN需要注意力机制来解决信息瓶颈等问题</li>
</ul>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101757934.png" alt=""></p>
<h3 id="输入编码"><a href="#输入编码" class="headerlink" title="输入编码"></a>输入编码</h3><h4 id="Byte-Pair-Encoding-BPE"><a href="#Byte-Pair-Encoding-BPE" class="headerlink" title="Byte Pair Encoding(BPE)"></a>Byte Pair Encoding(BPE)</h4><p>具体实现过程：</p>
<ol>
<li><p>首先统计每个单词的出现的频率；</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101804232.png" alt=""></p>
</li>
<li><p>将语料库当中的单词按照字母切分构成最初的词典；</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101806486.png" alt=""></p>
</li>
<li><p>统计语料库中每个Byte gram（连续两个相邻位置拼到一起，如lo、ow）出现的数量，把出现频度最高的Byte gram（es）抽象成新的词加入词表；</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101809525.png" alt=""></p>
</li>
<li><p>去除不再出现的单词（s）；</p>
</li>
<li><p>重复上述步骤</p>
</li>
</ol>
<h4 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101817772.png" alt=""></p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul>
<li>2个子层：多头注意力机制和前馈神经网络（2层的MLP）</li>
<li>2个技巧：残差连接和Layer normalization</li>
</ul>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul>
<li>Masked self-attention：使得第i个位置不参考第i+1个位置的信息</li>
<li>Encoder-decoder attention：k向量和v向量来自encoder最后一层的输出，q向量来自decoder（decoder的生成关注和整合encoder每个位置的信息）</li>
</ul>
<h3 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h3><ul>
<li>Checkpoint averaging</li>
<li>Adam optimizer</li>
<li>Dropout</li>
<li>Label smoothing</li>
<li>Auto-regressive decoding</li>
</ul>
<h2 id="预训练语言模型-PLMs"><a href="#预训练语言模型-PLMs" class="headerlink" title="预训练语言模型(PLMs)"></a>预训练语言模型(PLMs)</h2><h3 id="两种主流方式"><a href="#两种主流方式" class="headerlink" title="两种主流方式"></a>两种主流方式</h3><ul>
<li>Feature-based approaches：预训练语言模型的输出作为下游任务模型的输入</li>
<li>Fine-tuning approaches：预训练好的模型作为下游任务的模型</li>
</ul>
<h3 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h3><p>只使用Transformer的Decoder进行文本的生成</p>
<h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p>基于Transformer的Encoder，关注的任务是Masked LM和Next Sentence Prediction</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404102043123.png" alt=""></p>
<h3 id="MoE"><a href="#MoE" class="headerlink" title="MoE"></a>MoE</h3><p>MoE 应用于大模型，GPT-4并不是第一个。在2022年的时候，Google就提出了MoE大模型Switch Transformer，模型大小是1571B，Switch Transformer在预训练任务上显示出比 T5-XXL（11B）模型更高的样本效率。在相同的训练时间和计算资源下，Switch Transformer 能够达到更好的性能。</p>
<ul>
<li>稀疏MoE层：这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构。</li>
<li>门控网络或路由：这个部分用于决定哪些 token 被发送到哪个专家。例如，在下图中，“More”这个 token 可能被发送到第二个专家，而“Parameters”这个 token 被发送到第一个专家。有时，一个 token 甚至可以被发送到多个专家。token 的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404102105511.png" alt=""></p>
<h3 id="Transformers工具"><a href="#Transformers工具" class="headerlink" title="Transformers工具"></a>Transformers工具</h3><h4 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h4><p>Transformers中的pipeline函数自动使用一个预训练好的模型来执行下游任务</p>
<h4 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h4><p>不同预训练语言模型有不同的tokenization方式，Transformers中AutoTokenizer.from_pretrained函数可以自动进行tokenization</p>
<h4 id="Pre-trained-Model"><a href="#Pre-trained-Model" class="headerlink" title="Pre-trained Model"></a>Pre-trained Model</h4><p>AutoModelForSequenceClassification.from_pretrained函数就调用了一个做序列分类的预训练模型，随后我们就可以在这个模型基础之上进行微调</p>
<h1 id="Advanced-Adaptation"><a href="#Advanced-Adaptation" class="headerlink" title="Advanced Adaptation"></a>Advanced Adaptation</h1>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%80%BB%E7%BB%93/" rel="tag"># 总结</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" rel="tag"># 大语言模型</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/10/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="prev" title="强化学习">
                  <i class="fa fa-chevron-left"></i> 强化学习
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">木霈玖</span>
</div>

    </div>
  </footer>

  
  <script src="//unpkg.com/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//unpkg.com/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
