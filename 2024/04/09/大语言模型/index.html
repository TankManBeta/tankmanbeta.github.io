<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//unpkg.com/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//unpkg.com/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="前言了解一下大语言模型的相关内容。 主要参考清华刘知远团队的课程。">
<meta property="og:type" content="article">
<meta property="og:title" content="大语言模型">
<meta property="og:url" content="http://example.com/2024/04/09/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="木霈玖的博客">
<meta property="og:description" content="前言了解一下大语言模型的相关内容。 主要参考清华刘知远团队的课程。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101742998.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101757934.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101804232.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101806486.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101809525.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101817772.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404102043123.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404102105511.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111510028.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111514918.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111516250.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111521912.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111645534.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111650060.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111743187.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111749954.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111752088.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111758180.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111802035.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111825707.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111828103.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111857863.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111939534.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111942704.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111944309.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404211636816.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404211641993.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404211645462.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404211650387.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404211651675.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404211653204.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404211655500.png">
<meta property="article:published_time" content="2024-04-09T08:00:59.000Z">
<meta property="article:modified_time" content="2024-04-21T09:20:43.758Z">
<meta property="article:author" content="木霈玖">
<meta property="article:tag" content="总结">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="大语言模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101742998.png">


<link rel="canonical" href="http://example.com/2024/04/09/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>大语言模型 | 木霈玖的博客</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">木霈玖的博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NLP%E5%9F%BA%E7%A1%80"><span class="nav-number">2.</span> <span class="nav-text">NLP基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E4%BB%BB%E5%8A%A1"><span class="nav-number">2.1.</span> <span class="nav-text">基本任务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%8D%E8%A1%A8%E7%A4%BA"><span class="nav-number">2.2.</span> <span class="nav-text">词表示</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%8D%E8%A1%A8%E7%A4%BA%E7%9A%84%E7%9B%AE%E6%A0%87"><span class="nav-number">2.2.1.</span> <span class="nav-text">词表示的目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%8D%E8%A1%A8%E7%A4%BA%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">2.2.2.</span> <span class="nav-text">词表示的方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.3.</span> <span class="nav-text">语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BB%BB%E5%8A%A1"><span class="nav-number">2.3.1.</span> <span class="nav-text">语言模型的任务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-gram-model"><span class="nav-number">2.3.2.</span> <span class="nav-text">N-gram model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">存在的问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Neural-Language-Model"><span class="nav-number">2.3.3.</span> <span class="nav-text">Neural Language Model</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer%E5%92%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.</span> <span class="nav-text">Transformer和预训练语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">3.1.</span> <span class="nav-text">注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%89%B9%E7%82%B9"><span class="nav-number">3.1.1.</span> <span class="nav-text">注意力机制特点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer"><span class="nav-number">3.2.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA"><span class="nav-number">3.2.1.</span> <span class="nav-text">动机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E8%A7%88"><span class="nav-number">3.2.2.</span> <span class="nav-text">概览</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E7%BC%96%E7%A0%81"><span class="nav-number">3.2.3.</span> <span class="nav-text">输入编码</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Byte-Pair-Encoding-BPE"><span class="nav-number">3.2.3.1.</span> <span class="nav-text">Byte Pair Encoding(BPE)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="nav-number">3.2.3.2.</span> <span class="nav-text">位置编码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder"><span class="nav-number">3.2.4.</span> <span class="nav-text">Encoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decoder"><span class="nav-number">3.2.5.</span> <span class="nav-text">Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tricks"><span class="nav-number">3.2.6.</span> <span class="nav-text">Tricks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-PLMs"><span class="nav-number">3.3.</span> <span class="nav-text">预训练语言模型(PLMs)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%A4%E7%A7%8D%E4%B8%BB%E6%B5%81%E6%96%B9%E5%BC%8F"><span class="nav-number">3.3.1.</span> <span class="nav-text">两种主流方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GPT"><span class="nav-number">3.3.2.</span> <span class="nav-text">GPT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BERT"><span class="nav-number">3.3.3.</span> <span class="nav-text">BERT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MoE"><span class="nav-number">3.3.4.</span> <span class="nav-text">MoE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformers%E5%B7%A5%E5%85%B7"><span class="nav-number">3.3.5.</span> <span class="nav-text">Transformers工具</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Pipeline"><span class="nav-number">3.3.5.1.</span> <span class="nav-text">Pipeline</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tokenization"><span class="nav-number">3.3.5.2.</span> <span class="nav-text">Tokenization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pre-trained-Model"><span class="nav-number">3.3.5.3.</span> <span class="nav-text">Pre-trained Model</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Advanced-Adaptation"><span class="nav-number">4.</span> <span class="nav-text">Advanced Adaptation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Prompt-learning"><span class="nav-number">4.1.</span> <span class="nav-text">Prompt-learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Template"><span class="nav-number">4.1.1.</span> <span class="nav-text">Template</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Verbalizer"><span class="nav-number">4.1.2.</span> <span class="nav-text">Verbalizer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Delta-Tuning"><span class="nav-number">4.2.</span> <span class="nav-text">Delta Tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Addition-based"><span class="nav-number">4.2.1.</span> <span class="nav-text">Addition-based</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Adapter-Tuning"><span class="nav-number">4.2.1.1.</span> <span class="nav-text">Adapter-Tuning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Prefix-Tuning"><span class="nav-number">4.2.1.2.</span> <span class="nav-text">Prefix-Tuning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Specification-based"><span class="nav-number">4.2.2.</span> <span class="nav-text">Specification-based</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#BitFit"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">BitFit</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reparameterization-based"><span class="nav-number">4.2.3.</span> <span class="nav-text">Reparameterization-based</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Intrinstic-Prompt-Tuning"><span class="nav-number">4.2.3.1.</span> <span class="nav-text">Intrinstic Prompt Tuning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LoRA"><span class="nav-number">4.2.3.2.</span> <span class="nav-text">LoRA</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96"><span class="nav-number">5.</span> <span class="nav-text">训练优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-Parallel"><span class="nav-number">5.1.</span> <span class="nav-text">Data Parallel</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Parallel"><span class="nav-number">5.2.</span> <span class="nav-text">Model Parallel</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ZeRO"><span class="nav-number">5.3.</span> <span class="nav-text">ZeRO</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pipeline-Parallel"><span class="nav-number">5.4.</span> <span class="nav-text">Pipeline Parallel</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tricks-1"><span class="nav-number">5.5.</span> <span class="nav-text">Tricks</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9"><span class="nav-number">6.</span> <span class="nav-text">模型压缩</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Knowledge-Distillation"><span class="nav-number">6.1.</span> <span class="nav-text">Knowledge Distillation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Pruning"><span class="nav-number">6.2.</span> <span class="nav-text">Model Pruning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Quantization"><span class="nav-number">6.3.</span> <span class="nav-text">Model Quantization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Weight-Sharing"><span class="nav-number">6.4.</span> <span class="nav-text">Weight Sharing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Low-rank-Approximation"><span class="nav-number">6.5.</span> <span class="nav-text">Low-rank Approximation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Architecture-Search"><span class="nav-number">6.6.</span> <span class="nav-text">Architecture Search</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86"><span class="nav-number">7.</span> <span class="nav-text">模型推理</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8"><span class="nav-number">8.</span> <span class="nav-text">大模型应用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2"><span class="nav-number">8.1.</span> <span class="nav-text">信息检索</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89"><span class="nav-number">8.1.1.</span> <span class="nav-text">问题定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E8%AF%84%E6%B5%8B"><span class="nav-number">8.1.2.</span> <span class="nav-text">性能评测</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94"><span class="nav-number">8.2.</span> <span class="nav-text">机器问答</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3"><span class="nav-number">8.2.1.</span> <span class="nav-text">阅读理解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%80%E6%94%BE%E5%9F%9FQA"><span class="nav-number">8.2.2.</span> <span class="nav-text">开放域QA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90"><span class="nav-number">8.3.</span> <span class="nav-text">文本生成</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1"><span class="nav-number">8.3.1.</span> <span class="nav-text">文本生成任务</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E7%89%A9%E5%8C%BB%E5%AD%A6"><span class="nav-number">8.4.</span> <span class="nav-text">生物医学</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%95%E5%BE%8B%E6%99%BA%E8%83%BD"><span class="nav-number">8.5.</span> <span class="nav-text">法律智能</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BE%AE%E8%B0%83%E6%96%B9%E5%BC%8F"><span class="nav-number">9.</span> <span class="nav-text">微调方式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Adapter-Tuning-1"><span class="nav-number">9.1.</span> <span class="nav-text">Adapter Tuning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Prefix-Tuning-1"><span class="nav-number">9.2.</span> <span class="nav-text">Prefix Tuning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Prompt-Tuning"><span class="nav-number">9.3.</span> <span class="nav-text">Prompt Tuning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#P-Tuning"><span class="nav-number">9.4.</span> <span class="nav-text">P-Tuning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#P-Tuning-v2"><span class="nav-number">9.5.</span> <span class="nav-text">P-Tuning v2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LoRA-1"><span class="nav-number">9.6.</span> <span class="nav-text">LoRA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#QLoRA"><span class="nav-number">9.7.</span> <span class="nav-text">QLoRA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LoRA-MoE"><span class="nav-number">9.8.</span> <span class="nav-text">LoRA+MoE</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">木霈玖</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/04/09/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="木霈玖">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="木霈玖的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          大语言模型
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-04-09 16:00:59" itemprop="dateCreated datePublished" datetime="2024-04-09T16:00:59+08:00">2024-04-09</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2024-04-21 17:20:43" itemprop="dateModified" datetime="2024-04-21T17:20:43+08:00">2024-04-21</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">总结</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>了解一下大语言模型的相关内容。</p>
<p>主要参考清华刘知远团队的课程。</p>
<a id="more"></a>
<h1 id="NLP基础"><a href="#NLP基础" class="headerlink" title="NLP基础"></a>NLP基础</h1><p>自然语言处理：让计算机理解人类所说的语言。</p>
<p>相关综述：《Advances in Natural Language Processing》</p>
<h2 id="基本任务"><a href="#基本任务" class="headerlink" title="基本任务"></a>基本任务</h2><ul>
<li>词性标注：名词、动词、形容词的识别</li>
<li>命名实体的识别：人名、地名、机构名、日期的识别</li>
<li>共指消解：代词指向的是哪个实体</li>
<li>依赖关系：句子中成分的句法关系</li>
<li>中文的分词</li>
</ul>
<h2 id="词表示"><a href="#词表示" class="headerlink" title="词表示"></a>词表示</h2><h3 id="词表示的目标"><a href="#词表示的目标" class="headerlink" title="词表示的目标"></a>词表示的目标</h3><ul>
<li>词之间的相似度计算</li>
<li>词之间的语义关系</li>
</ul>
<h3 id="词表示的方法"><a href="#词表示的方法" class="headerlink" title="词表示的方法"></a>词表示的方法</h3><ul>
<li>相关词表示法：近义词、反义词、上位词（细微差异难以区分、新词义难以处理、主观性、数据吸收、大量人工）</li>
<li>独热表示法：每个词表示成独立的符号（表示词的时候假设不同的词之间是正交的）</li>
<li>上下文表示法：词用上下文出现的频度表示（需要大量存储空间、对于不经常出现的词表示变得稀疏）</li>
<li>词嵌入：将每个词学到低维稠密的向量空间当中</li>
</ul>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><h3 id="语言模型的任务"><a href="#语言模型的任务" class="headerlink" title="语言模型的任务"></a>语言模型的任务</h3><ul>
<li>计算一个词的序列成为一句合法的话的概率（Joint probability）</li>
<li>根据前面的词预测后面的词（conditional probability）</li>
</ul>
<h3 id="N-gram-model"><a href="#N-gram-model" class="headerlink" title="N-gram model"></a>N-gram model</h3><p>考虑连续出现N个词的概率（马尔可夫假设）：例如 $P(w_{j}|nerver \ too \ late \ to)=\frac{count(too \ late \ to \ w_{j})}{count(too \ late \ to)}$</p>
<h4 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h4><ul>
<li>N-gram model在实际使用时很少考虑比较长的上下文，因为前文越长，统计结果会越稀疏</li>
<li>N-gram model无法计算相似度</li>
</ul>
<h3 id="Neural-Language-Model"><a href="#Neural-Language-Model" class="headerlink" title="Neural Language Model"></a>Neural Language Model</h3><p>首先将上下文的每个词表示为低维的向量，然后将这些向量拼接成一个上下文向量，最后经过非线性变换预测下一个词。</p>
<h1 id="Transformer和预训练语言模型"><a href="#Transformer和预训练语言模型" class="headerlink" title="Transformer和预训练语言模型"></a>Transformer和预训练语言模型</h1><h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>通过求注意力分数来进行加权求和，得到一个与隐向量维度相同的输出向量，该向量包含decoder需要的所有encoder的信息。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101742998.png" alt=""></p>
<h3 id="注意力机制特点"><a href="#注意力机制特点" class="headerlink" title="注意力机制特点"></a>注意力机制特点</h3><ul>
<li>解决信息瓶颈问题</li>
<li>缓解梯度消失问题</li>
<li>提供了可解释性</li>
</ul>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><ul>
<li>RNN当中的顺序计算不利于GPU并行</li>
<li>RNN需要注意力机制来解决信息瓶颈等问题</li>
</ul>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101757934.png" alt=""></p>
<h3 id="输入编码"><a href="#输入编码" class="headerlink" title="输入编码"></a>输入编码</h3><h4 id="Byte-Pair-Encoding-BPE"><a href="#Byte-Pair-Encoding-BPE" class="headerlink" title="Byte Pair Encoding(BPE)"></a>Byte Pair Encoding(BPE)</h4><p>具体实现过程：</p>
<ol>
<li><p>首先统计每个单词的出现的频率；</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101804232.png" alt=""></p>
</li>
<li><p>将语料库当中的单词按照字母切分构成最初的词典；</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101806486.png" alt=""></p>
</li>
<li><p>统计语料库中每个Byte gram（连续两个相邻位置拼到一起，如lo、ow）出现的数量，把出现频度最高的Byte gram（es）抽象成新的词加入词表；</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101809525.png" alt=""></p>
</li>
<li><p>去除不再出现的单词（s）；</p>
</li>
<li><p>重复上述步骤</p>
</li>
</ol>
<h4 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h4><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404101817772.png" alt=""></p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul>
<li>2个子层：多头注意力机制和前馈神经网络（2层的MLP）</li>
<li>2个技巧：残差连接和Layer normalization</li>
</ul>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul>
<li>Masked self-attention：使得第i个位置不参考第i+1个位置的信息</li>
<li>Encoder-decoder attention：k向量和v向量来自encoder最后一层的输出，q向量来自decoder（decoder的生成关注和整合encoder每个位置的信息）</li>
</ul>
<h3 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h3><ul>
<li>Checkpoint averaging</li>
<li>Adam optimizer</li>
<li>Dropout</li>
<li>Label smoothing</li>
<li>Auto-regressive decoding</li>
</ul>
<h2 id="预训练语言模型-PLMs"><a href="#预训练语言模型-PLMs" class="headerlink" title="预训练语言模型(PLMs)"></a>预训练语言模型(PLMs)</h2><h3 id="两种主流方式"><a href="#两种主流方式" class="headerlink" title="两种主流方式"></a>两种主流方式</h3><ul>
<li>Feature-based approaches：预训练语言模型的输出作为下游任务模型的输入</li>
<li>Fine-tuning approaches：预训练好的模型作为下游任务的模型</li>
</ul>
<h3 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h3><p>只使用Transformer的Decoder进行文本的生成</p>
<h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p>基于Transformer的Encoder，关注的任务是Masked LM和Next Sentence Prediction</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404102043123.png" alt=""></p>
<h3 id="MoE"><a href="#MoE" class="headerlink" title="MoE"></a>MoE</h3><p>MoE 应用于大模型，GPT-4并不是第一个。在2022年的时候，Google就提出了MoE大模型Switch Transformer，模型大小是1571B，Switch Transformer在预训练任务上显示出比 T5-XXL（11B）模型更高的样本效率。在相同的训练时间和计算资源下，Switch Transformer 能够达到更好的性能。</p>
<ul>
<li>稀疏MoE层：这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构。</li>
<li>门控网络或路由：这个部分用于决定哪些 token 被发送到哪个专家。例如，在下图中，“More”这个 token 可能被发送到第二个专家，而“Parameters”这个 token 被发送到第一个专家。有时，一个 token 甚至可以被发送到多个专家。token 的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404102105511.png" alt=""></p>
<h3 id="Transformers工具"><a href="#Transformers工具" class="headerlink" title="Transformers工具"></a>Transformers工具</h3><h4 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h4><p>Transformers中的pipeline函数自动使用一个预训练好的模型来执行下游任务</p>
<h4 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h4><p>不同预训练语言模型有不同的tokenization方式，Transformers中AutoTokenizer.from_pretrained函数可以自动进行tokenization</p>
<h4 id="Pre-trained-Model"><a href="#Pre-trained-Model" class="headerlink" title="Pre-trained Model"></a>Pre-trained Model</h4><p>AutoModelForSequenceClassification.from_pretrained函数就调用了一个做序列分类的预训练模型，随后我们就可以在这个模型基础之上进行微调</p>
<h1 id="Advanced-Adaptation"><a href="#Advanced-Adaptation" class="headerlink" title="Advanced Adaptation"></a>Advanced Adaptation</h1><h2 id="Prompt-learning"><a href="#Prompt-learning" class="headerlink" title="Prompt-learning"></a>Prompt-learning</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111510028.png" alt=""></p>
<p>对于生成式模型而言，prompt需要mask的地方可能在句子最后</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111514918.png" alt=""></p>
<p>而对于理解类的任务，prompt需要mask的地方可能在句子中间</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111516250.png" alt=""></p>
<h3 id="Template"><a href="#Template" class="headerlink" title="Template"></a>Template</h3><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111521912.png" alt=""></p>
<h3 id="Verbalizer"><a href="#Verbalizer" class="headerlink" title="Verbalizer"></a>Verbalizer</h3><p>verbalizer的本质上是将回答映射到不固定的标签</p>
<ul>
<li>文本构建</li>
<li>虚拟token构建</li>
</ul>
<h2 id="Delta-Tuning"><a href="#Delta-Tuning" class="headerlink" title="Delta Tuning"></a>Delta Tuning</h2><p>模型本身不变，优化其中一小部分的参数，从而驱动大模型</p>
<ul>
<li>Addition-based：引入额外可训练的神经模块</li>
<li>Specification-based：指定模型中的某些参数是可训练的</li>
<li>Reparameterization-based：假设模型的微调可以在一个低维子空间或者通过一个低秩的矩阵完成</li>
</ul>
<h3 id="Addition-based"><a href="#Addition-based" class="headerlink" title="Addition-based"></a>Addition-based</h3><h4 id="Adapter-Tuning"><a href="#Adapter-Tuning" class="headerlink" title="Adapter-Tuning"></a>Adapter-Tuning</h4><p>主要是在原有的attention当中加入了Adapter，并且只训练这些Adapter</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111645534.png" alt=""></p>
<h4 id="Prefix-Tuning"><a href="#Prefix-Tuning" class="headerlink" title="Prefix-Tuning"></a>Prefix-Tuning</h4><p>在hidden state前面加入soft token，并且只训练这些soft token</p>
<h3 id="Specification-based"><a href="#Specification-based" class="headerlink" title="Specification-based"></a>Specification-based</h3><h4 id="BitFit"><a href="#BitFit" class="headerlink" title="BitFit"></a>BitFit</h4><p>只微调所有的bias项</p>
<h3 id="Reparameterization-based"><a href="#Reparameterization-based" class="headerlink" title="Reparameterization-based"></a>Reparameterization-based</h3><h4 id="Intrinstic-Prompt-Tuning"><a href="#Intrinstic-Prompt-Tuning" class="headerlink" title="Intrinstic Prompt Tuning"></a>Intrinstic Prompt Tuning</h4><p>模型的优化被映射到一个低维的子空间</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111650060.png" alt=""></p>
<h4 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h4><p>不认为模型的优化是低维的，而是低秩的，因此将模型参数进行一个低秩分解</p>
<h1 id="训练优化"><a href="#训练优化" class="headerlink" title="训练优化"></a>训练优化</h1><h2 id="Data-Parallel"><a href="#Data-Parallel" class="headerlink" title="Data Parallel"></a>Data Parallel</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111743187.png" alt=""></p>
<h2 id="Model-Parallel"><a href="#Model-Parallel" class="headerlink" title="Model Parallel"></a>Model Parallel</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111749954.png" alt=""></p>
<h2 id="ZeRO"><a href="#ZeRO" class="headerlink" title="ZeRO"></a>ZeRO</h2><ul>
<li><p>Stage 1：每张卡只计算部分梯度</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111752088.png" alt=""></p>
</li>
<li><p>Stage 2：反向传播的过程中及时移除gradient</p>
</li>
<li><p>Stage 3：每张卡只保留部分模型参数</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111758180.png" alt=""></p>
</li>
</ul>
<h2 id="Pipeline-Parallel"><a href="#Pipeline-Parallel" class="headerlink" title="Pipeline Parallel"></a>Pipeline Parallel</h2><p>模型的不同层分给不同的显卡</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111802035.png" alt=""></p>
<h2 id="Tricks-1"><a href="#Tricks-1" class="headerlink" title="Tricks"></a>Tricks</h2><ul>
<li>Mixed precision：混合精度训练</li>
<li>Offloading：把optimizer的参数传给CPU进行计算</li>
<li>Overlapping：提前获取参数</li>
<li>Checkpointing：设置检查点</li>
</ul>
<h1 id="模型压缩"><a href="#模型压缩" class="headerlink" title="模型压缩"></a>模型压缩</h1><h2 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111825707.png" alt=""></p>
<p>让学生模型和教师模型层与层之间的hidden states尽可能接近</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111828103.png" alt=""></p>
<h2 id="Model-Pruning"><a href="#Model-Pruning" class="headerlink" title="Model Pruning"></a>Model Pruning</h2><ul>
<li>Weight pruning（unstructured）：随机丢弃参数</li>
<li>Attention head pruning（structured）：丢弃某个head</li>
<li>Layer pruning（structured）：丢弃神经网络的某一层</li>
</ul>
<h2 id="Model-Quantization"><a href="#Model-Quantization" class="headerlink" title="Model Quantization"></a>Model Quantization</h2><p>浮点计算位数多，将浮点的表示转换为定精度的表示，有一个系数，有一个int值，每次将系数和int值相乘还原成浮点型</p>
<h2 id="Weight-Sharing"><a href="#Weight-Sharing" class="headerlink" title="Weight Sharing"></a>Weight Sharing</h2><p>不同层用同样的参数计算</p>
<h2 id="Low-rank-Approximation"><a href="#Low-rank-Approximation" class="headerlink" title="Low-rank Approximation"></a>Low-rank Approximation</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111857863.png" alt=""></p>
<h2 id="Architecture-Search"><a href="#Architecture-Search" class="headerlink" title="Architecture Search"></a>Architecture Search</h2><p>找到最优的神经网络模型</p>
<h1 id="模型推理"><a href="#模型推理" class="headerlink" title="模型推理"></a>模型推理</h1><ul>
<li>高显存占有</li>
<li>高计算能力要求</li>
</ul>
<h1 id="大模型应用"><a href="#大模型应用" class="headerlink" title="大模型应用"></a>大模型应用</h1><h2 id="信息检索"><a href="#信息检索" class="headerlink" title="信息检索"></a>信息检索</h2><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><ol>
<li>定义一个query</li>
<li>给定文档的集合</li>
<li>IR系统计算相关性分数并根据分数进行排序</li>
</ol>
<h3 id="性能评测"><a href="#性能评测" class="headerlink" title="性能评测"></a>性能评测</h3><ul>
<li><p>MRR：考虑每个查询排名最靠前的第一个相关文档的位置</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111939534.png" alt=""></p>
</li>
<li><p>MAP：一组查询的平均准确率均值</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111942704.png" alt=""></p>
</li>
<li><p>NDCG：归一化的折损累计增益</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404111944309.png" alt=""></p>
</li>
</ul>
<h2 id="机器问答"><a href="#机器问答" class="headerlink" title="机器问答"></a>机器问答</h2><h3 id="阅读理解"><a href="#阅读理解" class="headerlink" title="阅读理解"></a>阅读理解</h3><p>类型：Multiple choice、Cloze test、Extractive</p>
<h3 id="开放域QA"><a href="#开放域QA" class="headerlink" title="开放域QA"></a>开放域QA</h3><p>类型：Task Definition、Generation-based Methods、Retrieval-based Methods</p>
<h2 id="文本生成"><a href="#文本生成" class="headerlink" title="文本生成"></a>文本生成</h2><h3 id="文本生成任务"><a href="#文本生成任务" class="headerlink" title="文本生成任务"></a>文本生成任务</h3><ul>
<li>Data-to-Text：图片、表格等生成文本</li>
<li>Dialogue：根据用户输入生成一些满足需求的对话</li>
<li>Machine Translation</li>
<li>Poetry Generation</li>
<li>Style Transfer</li>
<li>Storytelling</li>
<li>Summarization</li>
</ul>
<h2 id="生物医学"><a href="#生物医学" class="headerlink" title="生物医学"></a>生物医学</h2><ul>
<li>domain corpus：Sci-BERT，BioBERT，clinical BERT</li>
<li>special pretraining task：MC-BERT，KeBioLM</li>
</ul>
<h2 id="法律智能"><a href="#法律智能" class="headerlink" title="法律智能"></a>法律智能</h2><ul>
<li>Legal Judgement Prediction</li>
<li>Similar Cases Retrieval</li>
<li>Legal Documents Generation</li>
<li>Legal Information Recommendation</li>
<li>Legal Documents Translation</li>
<li>Legal Question Answering</li>
<li>Court View Generation</li>
<li>Risk Warming</li>
<li>Legal Text Mining</li>
<li>Compliance Review</li>
</ul>
<h1 id="微调方式"><a href="#微调方式" class="headerlink" title="微调方式"></a>微调方式</h1><h2 id="Adapter-Tuning-1"><a href="#Adapter-Tuning-1" class="headerlink" title="Adapter Tuning"></a>Adapter Tuning</h2><p>2019年谷歌的研究人员首次在论文《Parameter-Efficient Transfer Learning for NLP》提出针对 BERT 的 PEFT微调方式，拉开了 PEFT 研究的序幕。他们指出，在面对特定的下游任务时，如果进行 Full-Fintuning（即预训练模型中的所有参数都进行微调），太过低效；而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果。</p>
<p>于是他们设计了Adapter 结构，将其嵌入 Transformer 的结构里面，在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构进行微调。同时为了保证训练的高效性（也就是尽可能少的引入更多参数），他们将 Adapter 设计为这样的结构：</p>
<ul>
<li>首先是一个 down-project 层将高维度特征映射到低维特征</li>
<li>然后过一个非线形层之后，再用一个 up-project 结构将低维特征映射回原来的高维特征</li>
<li>同时也设计了 skip-connection 结构，确保了在最差的情况下能够退化为identity（类似残差结构）</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404211636816.png" alt=""></p>
<h2 id="Prefix-Tuning-1"><a href="#Prefix-Tuning-1" class="headerlink" title="Prefix Tuning"></a>Prefix Tuning</h2><p>2021年斯坦福的研究人员在论文《Prefix-Tuning: Optimizing Continuous Prompts for Generation》中提出了 Prefix Tuning 方法。与Full-finetuning 更新所有参数的方式不同，该方法是在输入 token 之前（所有层的Transformer的输出）构造一段任务相关的 virtual tokens 作为 Prefix，然后训练的时候只更新 Prefix 部分的参数，而 Transformer 中的其他部分参数固定。该方法其实和构造 Prompt 类似，只是 Prompt 是人为构造的“显式”的提示，并且无法更新参数，而Prefix 则是可以学习的“隐式”的提示。</p>
<p>同时，为了防止直接更新 Prefix 的参数导致训练不稳定的情况，他们在 Prefix 层前面加了 MLP 结构(相当于将Prefix 分解为更小维度的 Input 与 MLP 的组合后输出的结果)，训练完成后，只保留 Prefix 的参数。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404211641993.png" alt=""></p>
<h2 id="Prompt-Tuning"><a href="#Prompt-Tuning" class="headerlink" title="Prompt Tuning"></a>Prompt Tuning</h2><p>Prompt Tuning 是2021年谷歌在论文《The Power of Scale for Parameter-Efficient Prompt Tuning》中提出的微调方法。</p>
<p>该方法可以看作是 Prefix Tuning 的简化版本，<strong>只在输入层加入 prompt tokens</strong>，并不需要加入 MLP 进行调整来解决难训练的问题，主要在 T5 预训练模型上做实验。似乎只要预训练模型足够强大，其他的一切都不是问题。作者也做实验说明随着预训练模型参数量的增加，Prompt Tuning的方法会逼近 Fine-tune 的结果。</p>
<p>固定预训练参数，为每一个任务额外添加一个或多个 embedding，之后拼接 query 正常输入 LLM，并只训练这些 embedding。左图为单任务全参数微调，右图为 Prompt tuning。<br><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404211645462.png" alt=""></p>
<h2 id="P-Tuning"><a href="#P-Tuning" class="headerlink" title="P-Tuning"></a>P-Tuning</h2><p>P-Tuning 方法的提出主要是为了解决这样一个问题：大模型的 Prompt 构造方式严重影响下游任务的效果。</p>
<p>P-Tuning 提出将 Prompt 转换为可以学习的 Embedding 层，只是考虑到直接对 Embedding 参数进行优化会存在这样两个挑战：</p>
<ul>
<li>Discretenes： 对输入正常语料的 Embedding 层已经经过预训练，而如果直接对输入的 prompt embedding进行随机初始化训练，容易陷入局部最优。</li>
<li>Association：没法捕捉到 prompt embedding 之间的相关关系。</li>
</ul>
<p>P-Tuning 和 Prefix-Tuning 差不多同时提出，做法其实也有一些相似之处，主要区别在：</p>
<ul>
<li>Prefix Tuning 是将额外的 embedding 加在开头，看起来更像是模仿 Instruction 指令；而 P-Tuning 的位置则不固定。</li>
<li>Prefix Tuning 通过在每个 Attention 层都加入 Prefix Embedding 来增加额外的参数，通过 MLP 来初始化；而 P-Tuning 只是在输入的时候加入 Embedding，并通过 LSTM+MLP 来初始化。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404211650387.png" alt=""></p>
<h2 id="P-Tuning-v2"><a href="#P-Tuning-v2" class="headerlink" title="P-Tuning v2"></a>P-Tuning v2</h2><p>P-Tuning 的问题是在小参数量模型上表现差。于是就有了v2版本：《P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks》。</p>
<p>从标题就可以看出，P-Tuning v2 的目标就是要让 Prompt Tuning 能够在不同参数规模的预训练模型、针对不同下游任务的结果上都达到匹敌 Fine-tuning 的结果。</p>
<p>那也就是说当前 Prompt Tuning 方法在这两个方面都存在局限性。</p>
<ul>
<li>不同模型规模：Prompt Tuning 和 P-tuning 这两种方法都是在预训练模型参数规模够足够大时，才能达到和Fine-tuning 类似的效果，而参数规模较小时效果则很差。</li>
<li>不同任务类型：Prompt Tuning 和 P-tuning 这两种方法在 sequence tagging 任务上表现都很差。</li>
</ul>
<p>相比 Prompt Tuning 和 P-tuning 的方法， P-tuning v2 方法在多层加入了 Prompts tokens 作为输入，带来两个方面的好处：</p>
<ul>
<li>带来更多可学习的参数（从 P-tuning 和 Prompt Tuning 的0.1%增加到0.1%-3%），同时也足够 parameter-efficient。</li>
<li>加入到更深层结构中的 Prompt 能给模型预测带来更直接的影响。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404211651675.png" alt=""></p>
<h2 id="LoRA-1"><a href="#LoRA-1" class="headerlink" title="LoRA"></a>LoRA</h2><p>LoRA，英文全称Low-RankAdaptation of Large Language Models，直译为大语言模型的低阶适应，是一种PEFT（Parameter-Efficient Tuning，简称PEFT），这是微软的研究人员为了解决大语言模型微调而开发的一项技术。</p>
<p>LoRA的基本原理是冻结预训练好的模型权重参数，在冻结原模型参数的情况下，通过往模型中加入额外的网络层，并只训练这些新增的网络层参数。由于这些新增参数数量较少，这样不仅 finetune 的成本显著下降，还能获得和全模型微调类似的效果。<br><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404211653204.png" alt=""></p>
<h2 id="QLoRA"><a href="#QLoRA" class="headerlink" title="QLoRA"></a>QLoRA</h2><p>QLoRA 是由 Tim Dettmers 等人提出的量化 LoRA 的缩写。QLoRA 是一种在微调过程中进一步减少内存占用的技术。在反向传播过程中，QLoRA 将预训练的权重量化为 4-bit，并使用分页优化器来处理内存峰值。</p>
<p>使用LoRA时可以节省33%的GPU内存。然而，由于QLoRA中预训练模型权重的额外量化和去量化，训练时间增加了39%。</p>
<p>首先分析下LoRA微调中的痛点：</p>
<ul>
<li>参数空间小：LoRA中参与训练的参数量较少，解空间较小，效果相比全量微调有一定的差距。</li>
<li>微调大模型成本高：对于上百亿参数量的模型，LoRA微调的成本还是很高。</li>
<li>精度损失：针对第二点，可以采用int8或int4量化，进一步对模型基座的参数进行压缩。但是又会引发精度损失的问题，降低模型性能。</li>
</ul>
<p>QLoRA优点：</p>
<ul>
<li>4-bit NormalFloat：提出一种理论最优的4-bit的量化数据类型，优于当前普遍使用的FP4与Int4。对于正态分布权重而言，一种信息理论上最优的新数据类型，该数据类型对正态分布数据产生比 4 bit整数和 4bit 浮点数更好的实证结果。QLORA包含一种低精度存储数据类型（通常为4-bit）和一种计算数据类型（通常为BFloat16）。在实践中，QLORA权重张量使用时，需要将将张量去量化为BFloat16，然后在16位计算精度下进行矩阵乘法运算。模型本身用4bit加载，训练时把数值反量化到bf16后进行训练。</li>
<li>Double Quantization：对第一次量化后的那些常量再进行一次量化，减少存储空间。相比于当前的模型量化方法，更加节省显存空间。每个参数平均节省0.37bit，对于65B的LLaMA模型，大约能节省3GB显存空间。</li>
<li>Paged Optimizers：使用NVIDIA统一内存特性，该特性可以在在GPU偶尔OOM的情况下，进行CPU和GPU之间自动分页到分页的传输，以实现无错误的 GPU 处理。该功能的工作方式类似于 CPU 内存和磁盘之间的常规内存分页。使用此功能为优化器状态（Optimizer）分配分页内存，然后在 GPU 内存不足时将其自动卸载到 CPU 内存，并在优化器更新步骤需要时将其加载回 GPU 内存。</li>
<li>增加Adapter：4-bit的NormalFloat与Double Quantization，节省了很多空间，但带来了性能损失，作者通过插入更多adapter来弥补这种性能损失。在LoRA中，一般会选择在query和value的全连接层处插入adapter。而QLoRA则在所有全连接层处都插入了adapter，增加了训练参数，弥补精度带来的性能损失。</li>
</ul>
<h2 id="LoRA-MoE"><a href="#LoRA-MoE" class="headerlink" title="LoRA+MoE"></a>LoRA+MoE</h2><p>由于大模型全量微调时的显存占用过大，LoRA、Adapter、IA 3 这些参数高效微调方法便成为了资源有限的机构和研究者微调大模型的标配。PEFT方法的总体思路是冻结住大模型的主干参数，引入一小部分可训练的参数作为适配模块进行训练，以节省模型微调时的显存和参数存储开销。</p>
<p>传统上，LoRA这类适配模块的参数和主干参数一样是稠密的，每个样本上的推理过程都需要用到所有的参数。近来，大模型研究者们为了克服稠密模型的参数效率瓶颈，开始关注以Mistral、DeepSeek MoE为代表的混合专家（Mixure of Experts，简称MoE）模型框架。在该框架下，模型的某个模块（如Transformer的某个FFN层）会存在多组形状相同的权重（称为专家），另外有一个路由模块（Router）接受原始输入、输出各专家的激活权重。<br><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202404211655500.png" alt=""></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%80%BB%E7%BB%93/" rel="tag"># 总结</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" rel="tag"># 大语言模型</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/10/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="prev" title="强化学习">
                  <i class="fa fa-chevron-left"></i> 强化学习
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/05/11/Docker%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/" rel="next" title="Docker使用总结">
                  Docker使用总结 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">木霈玖</span>
</div>

    </div>
  </footer>

  
  <script src="//unpkg.com/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//unpkg.com/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
