<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//unpkg.com/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//unpkg.com/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="前言学习一下多智能体强化学习相关算法。">
<meta property="og:type" content="article">
<meta property="og:title" content="多智能体强化学习">
<meta property="og:url" content="http://example.com/2024/11/17/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="木霈玖的博客">
<meta property="og:description" content="前言学习一下多智能体强化学习相关算法。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171915509.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172038329.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171918940.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171923497.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172041620.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172046802.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172048239.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172102666.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182259010.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182302432.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182308208.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182329900.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182341968.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182351374.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202503141754204.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411190001341.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411190007735.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411190011682.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281152539.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272022809.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272022775.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272023962.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272023998.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272024417.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272024548.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272024944.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272025725.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281839165.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281839619.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281840590.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281840019.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281841265.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281841239.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281842146.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281842375.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281842527.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281843228.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291711199.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291714241.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291714436.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291718255.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291719700.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291719547.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291720847.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291714436.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291730999.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291731261.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291732902.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291733671.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291734267.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291735536.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192146348.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192159569.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192206736.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192208564.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411202322720.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411202312830.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411202320613.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181709884.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181706862.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181710393.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181720106.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181726323.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181732197.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181741355.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181745351.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171817653.png">
<meta property="article:published_time" content="2024-11-17T08:54:28.000Z">
<meta property="article:modified_time" content="2025-12-18T09:51:40.076Z">
<meta property="article:author" content="木霈玖">
<meta property="article:tag" content="总结">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171915509.png">


<link rel="canonical" href="http://example.com/2024/11/17/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>多智能体强化学习 | 木霈玖的博客</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">木霈玖的博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#VDN-AAMAS2018"><span class="nav-number">2.</span> <span class="nav-text">VDN-AAMAS2018</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#QMIX-ICML2018"><span class="nav-number">3.</span> <span class="nav-text">QMIX-ICML2018</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#QTRAN-ICML2019"><span class="nav-number">4.</span> <span class="nav-text">QTRAN-ICML2019</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#QTRAN%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="nav-number">4.1.</span> <span class="nav-text">QTRAN直观理解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%AF%E5%88%86%E8%A7%A3%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E5%85%85%E5%88%86%E6%9D%A1%E4%BB%B6"><span class="nav-number">4.2.</span> <span class="nav-text">可分解值函数的充分条件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%AF%E5%88%86%E8%A7%A3%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E5%BF%85%E8%A6%81%E6%9D%A1%E4%BB%B6"><span class="nav-number">4.3.</span> <span class="nav-text">可分解值函数的必要条件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-number">4.4.</span> <span class="nav-text">网络结构</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Qatten"><span class="nav-number">5.</span> <span class="nav-text">Qatten</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%90%86%E8%AE%BA%E8%AF%81%E6%98%8E"><span class="nav-number">5.1.</span> <span class="nav-text">理论证明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%8A%8A%E4%B9%98%E7%A7%AF%E9%A1%B9%E8%83%BD%E9%99%8D%E9%98%B6%E4%B8%BA%E7%BA%BF%E6%80%A7%E9%A1%B9"><span class="nav-number">5.2.</span> <span class="nav-text">为什么把乘积项能降阶为线性项</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E7%BB%93%E6%9E%84"><span class="nav-number">5.3.</span> <span class="nav-text">算法结构</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#QPLEX-ICLR2021"><span class="nav-number">6.</span> <span class="nav-text">QPLEX-ICLR2021</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E5%8A%A8%E6%9C%BA"><span class="nav-number">6.1.</span> <span class="nav-text">研究动机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80"><span class="nav-number">6.2.</span> <span class="nav-text">理论基础</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-1"><span class="nav-number">6.3.</span> <span class="nav-text">网络结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%9B%B4%E6%96%B0%E6%B5%81%E7%A8%8B"><span class="nav-number">6.4.</span> <span class="nav-text">算法更新流程</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#WQMIX-NeurIPS2020"><span class="nav-number">7.</span> <span class="nav-text">WQMIX-NeurIPS2020</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E8%83%8C%E6%99%AF"><span class="nav-number">7.1.</span> <span class="nav-text">问题背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E5%AD%90%E5%AE%9A%E4%B9%89%E5%8F%8A%E6%80%A7%E8%B4%A8"><span class="nav-number">7.2.</span> <span class="nav-text">算子定义及性质</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3"><span class="nav-number">7.3.</span> <span class="nav-text">算法详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9D%83%E9%87%8D%E8%AE%BE%E8%AE%A1"><span class="nav-number">7.3.1.</span> <span class="nav-text">权重设计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#COMA-AAAI2018"><span class="nav-number">8.</span> <span class="nav-text">COMA-AAAI2018</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E8%83%8C%E6%99%AF%E5%92%8C%E6%80%9D%E6%83%B3"><span class="nav-number">8.1.</span> <span class="nav-text">算法背景和思想</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="nav-number">8.2.</span> <span class="nav-text">公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-number">8.2.1.</span> <span class="nav-text">全局策略梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E4%BA%8B%E5%AE%9E%E5%9F%BA%E7%BA%BF"><span class="nav-number">8.2.2.</span> <span class="nav-text">反事实基线</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%9B%B4%E6%96%B0"><span class="nav-number">8.2.3.</span> <span class="nav-text">策略更新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">8.2.4.</span> <span class="nav-text">全局值函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span class="nav-number">8.3.</span> <span class="nav-text">算法步骤</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MADDPG-NeurIPS2017"><span class="nav-number">9.</span> <span class="nav-text">MADDPG-NeurIPS2017</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E5%8A%A8%E6%9C%BA"><span class="nav-number">9.1.</span> <span class="nav-text">背景与动机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E7%BB%86%E8%8A%82"><span class="nav-number">9.2.</span> <span class="nav-text">算法细节</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AD%E5%BF%83%E5%8C%96%E8%AE%AD%E7%BB%83"><span class="nav-number">9.2.1.</span> <span class="nav-text">中心化训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E6%89%A7%E8%A1%8C"><span class="nav-number">9.2.2.</span> <span class="nav-text">分布式执行</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MAAC-ICML2019"><span class="nav-number">10.</span> <span class="nav-text">MAAC-ICML2019</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">10.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3-1"><span class="nav-number">10.2.</span> <span class="nav-text">算法详解</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ROMA-ICML2020"><span class="nav-number">11.</span> <span class="nav-text">ROMA-ICML2020</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MAPPO-NeurIPS2022"><span class="nav-number">12.</span> <span class="nav-text">MAPPO-NeurIPS2022</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-2"><span class="nav-number">12.1.</span> <span class="nav-text">网络结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">12.2.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%87%87%E6%A0%B7%E5%92%8C%E6%9B%B4%E6%96%B0%E6%96%B9%E5%BC%8F"><span class="nav-number">12.3.</span> <span class="nav-text">采样和更新方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%87%E6%A0%B7"><span class="nav-number">12.3.1.</span> <span class="nav-text">采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0"><span class="nav-number">12.3.2.</span> <span class="nav-text">更新</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">木霈玖</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">26</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/11/17/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="木霈玖">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="木霈玖的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          多智能体强化学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-11-17 16:54:28" itemprop="dateCreated datePublished" datetime="2024-11-17T16:54:28+08:00">2024-11-17</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2025-12-18 17:51:40" itemprop="dateModified" datetime="2025-12-18T17:51:40+08:00">2025-12-18</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>16k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>15 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>学习一下多智能体强化学习相关算法。</p>
<a id="more"></a>
<h1 id="VDN-AAMAS2018"><a href="#VDN-AAMAS2018" class="headerlink" title="VDN-AAMAS2018"></a><code>VDN-AAMAS2018</code></h1><p><code>VDN</code>中提出一种通过反向传播将团队的奖励信号分解到各个智能体上的这样一种方式。其网络结构如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171915509.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172038329.png" alt=""></p>
<p>先看上图中的图<code>1</code>，画的是两个独立的智能体，因为对每个智能体来说，观测都是部分可观测的，所以<code>Q</code>函数是被定义成基于观测历史数据所得到的$Q(h_{t},a_{t})$，实际操作的时候直接用<code>RNN</code>来做就可以。图<code>2</code>说的就是联合动作值函数由各个智能体的值函数累加得到的：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171918940.png" alt=""></p>
<p>其中$d$表示$d$个智能体，$\tilde{Q}_{i}$由每个智能体的局部观测信息得到，$\tilde{Q}_{i}$是通过联合奖励信号反向传播到各个智能体的$\tilde{Q}_{i}$上进行更新的。这样各个智能体通过贪婪策略选取动作的话，也就会使得联合动作值函数最大。</p>
<p>总结来说：值分解网络旨在学习一个联合动作值函数$Q_{tot}(\tau,\mathbf{u})$，其中$\tau \in \mathbf{T} \equiv \mathcal{T}^{n}$是一个联合动作-观测的历史轨迹，$\mathbf{u}$是一个联合动作。它是由每个智能体$a$独立计算其值函数$Q_{a}\left(\tau^{a},u^{a};\theta^{a}\right)$，之后累加求和得到的。其关系如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171923497.png" alt=""></p>
<h1 id="QMIX-ICML2018"><a href="#QMIX-ICML2018" class="headerlink" title="QMIX-ICML2018"></a><code>QMIX-ICML2018</code></h1><p>在之前的值分解网络中，拿到了联合动作的$Q$值之后，我们就可以直接取能够获取最大的$Q_{tot}(\tau, \mathbf{u})$所对应的联合动作。这种方式就能够实现集中式学习，但是得到分布式策略。并且对全局值函数做<code>argmax</code>与对单个智能体地值函数做<code>argmax</code>能够得到相同的结果：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172041620.png" alt=""></p>
<p>这样的话，每个智能体$a$都可以基于$Q_{a}$以贪婪策略选择动作。由于每个子智能体都采用贪婪策略，因此这个算法必定会是<code>off-policy</code>的算法，这一点是很容易实现的，并且样本的利用率会比较高。在<code>VDN</code>中采用的是线性加权，因此上述等式会成立，而如果是采用一个神经网络来学习融合各个智能体的$Q$函数的话，上述等式就未必会成立了。</p>
<p>在<code>QMIX</code>中也是需要一个联合动作值函数的，但是与值分解网络的不同之处在于，这个联合动作值函数并不是简单地由各个智能体的值函数线性相加得到的。为了保证值函数的单调性来使得上式能够成立，作者对联合动作值函数$Q_{tot}$和单个智能体动作值函数$Q_{a}$之间做了一个约束：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172046802.png" alt=""></p>
<p>设计思想如上所示，具体的网络结构如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172048239.png" alt=""></p>
<p>上述这个复杂的网络结构可以拆分为如下三部分：</p>
<ul>
<li><code>agent networks</code>：对于每个智能体都要学一个独立的值函数$Q_{a}\left(\tau^{a}, u^{a}\right)$，单个智能体的网络结构采用<code>DRQN</code>的网络结构，在每个时间步，接收当前的独立观测$O_{t}^{a}$和上一个动作$u_{t-1}^{a}$，如上图<code>(c)</code>所示。</li>
<li><code>mixing network</code>：是一个全连接网络，接收每个智能体的输出$Q_{n}\left(\tau^{n}, u_{t}^{n}\right)$作为输入，输出联合动作值函数$Q_{t o t}(\boldsymbol{\tau}, \boldsymbol{u})$，对每个子智能体的动作值函数做非线性映射，并且要保证单调性约束。想要保证公式<code>(2)</code>的单调性约束的话，我们只需要保证<code>mixing network</code>的权重非负即可。</li>
<li><code>hypernetworks</code>：<code>hypernetworks</code>网络去产生<code>mixing network</code>的权重，超参数网络输入状态$s$，输出<code>Mixing</code>网络的每一层的超参数向量，激活函数来使得输出非负，对于<code>Mixing</code>网络参数的偏置并没有非负的要求。<code>Hypernetworks</code>表示用于产生较大规模网络参数的小规模网络，在这一过程中，主网络的作用与其他任意神经网络一样，将输入样本映射到对应的目标值，而超网络的作用则是接收一系列包含主网络参数结构信息的值作为输入然后产生主网络某一层的参数。超网络的思想源于<code>evolutionary computing</code>（进化计算）：在一个包含成千上万参数的空间中搜索参数是十分困难的，一种更加高效的方式是用较小的网络产生较大网络的参数，这样搜索空间就会被限制在一个小得多的空间中，超网络的参数和主网络通过端到端的方式进行训练。</li>
</ul>
<p><code>QMIX</code>的训练方式是端到端的训练：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172102666.png" alt=""></p>
<h1 id="QTRAN-ICML2019"><a href="#QTRAN-ICML2019" class="headerlink" title="QTRAN-ICML2019"></a><code>QTRAN-ICML2019</code></h1><p>前面介绍的<code>VDN</code>，<code>QMIX</code>算法，都是基于值方法并且用于协作式<code>MARL</code>任务中。这些算法本质上都在找分布式最优策略，并且满足下式中描述的关系：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182259010.png" alt=""></p>
<p>其中，$Q_{jt}$表示整体的联合$Q$函数；$Q_{i}$表示智能体$i$的值函数。在本文，作者将<code>(1)</code>式中描述的关系定义为<code>IGM(Individual-Global-Max)</code>条件。</p>
<p><code>VDN</code>为了满足<code>IGM</code>条件，直接将值函数分解成“加和形式<code>(Additivity)</code>”：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182302432.png" alt=""></p>
<p>从<code>(2)</code>式可以得出$\frac{\partial Q_{jt}(\tau,u)}{\partial Q_{i}(\tau_{i},u_{i})}\equiv1,\forall i\in\mathcal{N}$，可见只要满足<code>(2)</code>式就能满足<code>IGM</code>条件。</p>
<p>但是<code>QMIX</code>觉得<code>VDN</code>这样做不至于，并且会导致很多复杂的函数无法很好地拟合出来，因此<code>QMIX</code>提出了更一般的<strong>“单调性条件<code>(Monotonicity)</code>”</strong>：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182308208.png" alt=""></p>
<p><code>QMIX</code>认为只要满足<code>(3)</code>式，就能满足<code>IGM</code>条件。<code>(2)</code>式是<code>(3)</code>式的充分条件，但是同时，<code>(3)</code>式也是<code>(1)</code>式的充分条件。换句话说，<strong>它们对<code>IGM</code>条件来说，都不是必要的！</strong>因此，针对一些具有非单调收益的合作问题，<code>VDN</code>和<code>QMIX</code>的函数拟合能力就会受到限制。</p>
<p>因此，基于这样的问题，作者提出<code>QTRAN</code>算法，并且声称该算法能够分解任何可分解的任务，而不需要受<code>(2)</code>式和<code>(3)</code>式的约束。</p>
<h2 id="QTRAN直观理解"><a href="#QTRAN直观理解" class="headerlink" title="QTRAN直观理解"></a><code>QTRAN</code>直观理解</h2><p>在<code>VDN</code>和<code>QMIX</code>中是将$Q_{\mathrm{jt}}$通过累加求和和保证单调性的方式来分解的，作者这里提出一种更加鲁棒的分解方式，将原始的$Q_{\mathrm{jt}}$映射成$Q_{\mathrm{jt}}^{\prime}$，通过$Q_{\mathrm{jt}}^{\prime}$去分解值函数到各个子智能体上，来保证学到的$Q_{\mathrm{jt}}^{\prime}$与真实的动作值函数$Q^{*}$非常接近。这样在学习真实的动作值函数的时候，没有像<code>VDN</code>和<code>QMIX</code>那样对其加上一些累加求和和保证单调性的限制，所以它能学地更好。</p>
<p>但是由于部分可观测的限制，这个$Q_{\mathrm{jt}}^{\prime}$是没有办法用来进行具体地决策的，所以我们需要去找到$Q_{\mathrm{jt}}$、$Q_{\mathrm{jt}}^{\prime}$和$\left[Q_{i}\right]$三者之间的关系。</p>
<h2 id="可分解值函数的充分条件"><a href="#可分解值函数的充分条件" class="headerlink" title="可分解值函数的充分条件"></a>可分解值函数的充分条件</h2><p>由于不提供累加求和和单调性来保证可分解，<code>QTRAN</code>提出了一个满足<code>IGM</code>定义的充分条件：当动作值函数$Q_{\mathrm{jt}}(\boldsymbol{\tau}, \boldsymbol{u})$和$\left[Q_{i}\left(\tau_{i}, u_{i}\right)\right]$满足下面这个关系式时，我们认为它是可分解的，其中$V_{\mathrm{jt}}(\tau)=\max _{\boldsymbol{u}} Q_{\mathrm{jt}}(\boldsymbol{\tau}, \boldsymbol{u})-\sum_{i=1}^{N} Q_{i}\left(\tau_{i}, \bar{u}_{i}\right)$：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182329900.png" alt=""></p>
<p>想要通过上述条件证明出满足<code>IGM</code>条件，也就是$\arg \max _{\boldsymbol{u}} Q_{\mathrm{jt}}(\boldsymbol{\tau}, \boldsymbol{u})=\overline{\boldsymbol{u}}$时（其中$\bar{u}_{i}=\arg \max _{u_{i}} Q_{i}\left(\tau_{i}, u_{i}\right)$）才能取到全局最大的动作值函数。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182341968.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182351374.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202503141754204.png" alt=""></p>
<h2 id="可分解值函数的必要条件"><a href="#可分解值函数的必要条件" class="headerlink" title="可分解值函数的必要条件"></a>可分解值函数的必要条件</h2><p>必要条件说的是，当满足<code>IGM</code>条件时，能够把充分条件给推出来。作者在论文中说到，存在一个仿射函数$\phi(\boldsymbol{Q})=A \cdot \boldsymbol{Q}+B$对$Q$进行一个映射，其中$\left[a_{i i}\right] \in \mathbb{R}_{+}^{N \times N}$为一个对角矩阵，并且$a_{i i}&gt;0$。相较<code>QMIX</code>还多一个$B$，$B=\left[b_{i}\right] \in \mathbb{R}^{N}$。作者将$Q_{i}$进行了一个缩放，$a_{i i} Q_{i}+b_{i}$。然后定义：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411190001341.png" alt=""></p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411190007735.png" alt=""></p>
<p>网络结构中主要有三部分：</p>
<ul>
<li>独立的动作值网络：$f_{\mathrm{q}}:\left(\tau_{i}, u_{i}\right) \mapsto Q_{i}$。对于每个动作值网络，输入是他自己的动作观测历史$\tau_{i}$，输出动作值函数$Q_{i}\left(\tau_{i}, \cdot\right)$。$Q_{\mathrm{jt}}^{\prime}$由各个子智能体的动作值函数累加得到。</li>
<li>联合动作值网络：$f_{\mathrm{r}}:(\tau, \boldsymbol{u}) \mapsto Q_{\mathrm{jt}}$。如上图所示，网络的前面几层参数是共享的，用所有的独立智能体的动作值函数向量来采样样本，更新联合动作值函数。</li>
<li>状态值网络：$f_{\mathrm{v}}: \tau \mapsto V_{\mathrm{jt}}$。状态值函数类似<code>dueling</code>网络，并且这里可以引入全局的状态信息，它是独立于动作轨迹的，但是可以用来辅助动作值函数的训练。</li>
</ul>
<p>此时损失函数可以表达成如下形式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411190011682.png" alt=""></p>
<p>第一个$\mathcal{L}_{\mathrm{td}}$使用标准的<code>TD-error</code>来更新最优的联合动作值函数，$\mathcal{L}_{\mathrm{opt}}$和$\mathcal{L}_{\mathrm{nopt}}$是用来去满足可分解值函数地充分必要条件的，$\mathcal{L}_{\mathrm{opt}}$对应条件<code>(a)</code>，$\mathcal{L}_{\mathrm{nopt}}$对应条件<code>(b)</code>，$\hat{Q}_{\mathrm{total}}$表示将$Q_{\mathrm{total}}$固定，也就是说$\mathcal{L}_{\mathrm{opt}}$和$\mathcal{L}_{\mathrm{nopt}}$不更新$Q_{\mathrm{total}}$，$Q^{\prime}_{\mathrm{jt}}$是真实的，$Q_{\mathrm{jt}}$是网络学习到的，用$V$去弥补两者之间的差异。</p>
<p><code>QTRAN-alt</code>：与<code>QTRAN-base</code>不同的是其采用了<code>counterfactual network</code>。作者发现条件<code>(4b)</code>太宽松，导致由$Q_{tot}^{\prime}+V_{jt}$构建出的<code>joint Q-value</code>不准确，其中的一种可能性是非最优与最优动作下的$Q_{tot}^{\prime}$之间误差较小，但代入到$Q_{tot}$时就会导致两者的误差很大，即$Q_{tot}^{\prime}(\tau,\bar{u})-Q_{tot}^{\prime}(\tau,u)\lt Q_{tot}(\tau,\bar{u})-Q_{tot}(\tau,u)$。该条件能够扩大$Q_{tot}^{\prime}(\tau,\bar{u})$与$Q_{tot}^{\prime}(\tau,u)$的差距，从而保证算法更稳定。<code>QTRAN-alt</code>的损失函数仍然是三项，只是最后一项$L_{opt}$与之前的<code>QTRAN-base</code>不同：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281152539.png" alt=""></p>
<h1 id="Qatten"><a href="#Qatten" class="headerlink" title="Qatten"></a><code>Qatten</code></h1><p><code>Qatten</code>首次从理论上推导出了将联合$Q$值（$Q_{tot}$）分解为局部$Q$值（$Q^{i}$）的过程，提出了一种叫做<code>Qatten</code>的算法。<code>QMIX</code>和<code>VDN</code>等算法对联合$Q$值的分解没有理论支撑，而<code>Qatten</code>算法弥补了这一缺陷。<code>Qatten</code>算法引入了多头注意力机制来分解联合$Q$值，并给出了理论解释。</p>
<p><code>Qatten</code>算法在不引入附加假设和约束条件的情况下，首次从理论上推导出任意数量的智能体的$Q_{tot}$和$Q^{i}$的广义形式化，提出了一种实用的基于多头注意力机制的$Q$值混合网络来近似联合$Q$值和分解单个$Q$值。<code>Qatten</code>利用<code>key</code>值存储操作度量每个<code>agent</code>对全局系统的重要程度，利用多头注意力机制来捕获分解时$Q_{tot}$对$Q^{i}$的不同高阶偏导数。此外，加权头$Q$值和非线性机制可以提高<code>Qatten</code>的逼近能力。算法在星际争霸<code>SMAC</code>环境中进行实验，取得了良好的效果。同时，注意力分析表明，<code>Qatten</code>抓住了每个<code>agent</code>对于近似$Q_{tot}$的重要性，并在一定程度上揭示了$Q^{i}$近似$Q_{tot}$的内部工作流程。</p>
<h2 id="理论证明"><a href="#理论证明" class="headerlink" title="理论证明"></a>理论证明</h2><p>首先联合动作值函数$Q_{tot}(s, a)$，应用隐函数定理，也就将其看做了关于$Q^{i}$的函数。作者假设没有独立的<code>agent</code>与整个<code>group</code>不相关，因为独立的<code>agent</code>不应该是<code>group</code>的成员，应该当做独立的<code>agent</code>去优化其策略。也就是说$Q^{i}$变化会影响$Q_{tot}$，所以偏导不为$0$：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272022809.png" alt=""></p>
<p>作者分析接近动作空间中最大值点时，$Q_{tot}$和$Q^{i}$的局部行为，假设状态是固定的，由于最值点$a_{o}$处梯度会消失，所以：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272022775.png" alt=""></p>
<p>因此可以得到以下结论：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272023962.png" alt=""></p>
<p>然后进行泰勒二元展开：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272023998.png" alt=""></p>
<p>当联合$Q$值$Q_{tot}$在动作空间中的最大点附近时，$Q_{tot}$对单个$Q^{i}$的依赖性近似于线性。$Q_{tot}$与$Q^{i}$之间的函数关系在动作空间上呈线性，但包含了所有的非线性信息。线性系数的详细结构与$Q_{tot}$和$Q^{i}$的所有非线性依赖有关。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272024417.png" alt=""></p>
<p>证明如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272024548.png" alt=""></p>
<h2 id="为什么把乘积项能降阶为线性项"><a href="#为什么把乘积项能降阶为线性项" class="headerlink" title="为什么把乘积项能降阶为线性项"></a>为什么把乘积项能降阶为线性项</h2><ul>
<li>关键点是我们是在 “近极值点” 做局部展开：在该点$Q_{i}$的第一阶项为$0$，主导变化为$(a_{i}-a_{i}^{0})^{2}$（即$Q_{i}-\alpha_{i}$的主要成分与二阶量相关）。因此任意两个$Q_{i}Q_{j}$的主要变化可以分解为常数 + 与单个$Q_{i}$（或$Q_{j}$）线性相关的主导项 + 更小的高阶残差。</li>
<li>递归地对高阶乘积进行类似处理，可以把任意阶的乘积主导项“映射”为一组以单个$Q_{i}$为基的线性组合（加常数和小残差）。这就是为什么能用“线性加权的$Q_{i}$”去近似完整的非线性$Q_{tot}$。</li>
</ul>
<h2 id="算法结构"><a href="#算法结构" class="headerlink" title="算法结构"></a>算法结构</h2><p>$Q_{tot}$的分解公式可以表示如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272024944.png" alt=""></p>
<p>先用单注意力机制计算出联合$Q$值（这里称作$Q^{h}$），再将这个过程重复$H$次，把结果拼在一起（直接加和或者加权平均），得到真正的$Q_{tot}$。值得注意的是，为了增加$Q_{tot}$和$Q^{i}$的非线性关系，加入了一个自注意力模块，没有直接用$o_{i}$作为输入，而是用$(o_{i}, Q^{i})$作为输入。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508272025725.png" alt=""></p>
<h1 id="QPLEX-ICLR2021"><a href="#QPLEX-ICLR2021" class="headerlink" title="QPLEX-ICLR2021"></a><code>QPLEX-ICLR2021</code></h1><h2 id="研究动机"><a href="#研究动机" class="headerlink" title="研究动机"></a>研究动机</h2><ul>
<li>对于<code>CTDE</code>框架下的多智能体值方法，<code>joint greedy action</code>应该等于每个个体的<code>greedy action</code>的集合，即<code>IGM</code>原则。</li>
<li><code>VDN</code>和<code>QMIX</code>提出的联合效用函数与单体效用函数的相加性和单调性。</li>
</ul>
<p>为了解决这一挑战，提出了<code>QPLEX</code>，称为<code>duPLEX dueling multi-agent Q-learning(QPLEX)</code>，该方法采用<code>duplex dueling network</code>结构将联合动作值函数分解为个体的动作值函数。<code>QPLEX</code>引入了<code>dueling structure</code>：$Q=V+A$，用于表示联合和个体动作值函数，然后将<code>IGM</code>原则重新形式化为基于优势的<code>IGM</code>。这种重新表述将<code>IGM</code>的一致性转换为对优势函数值范围的约束，从而促进了具有线性分解结构的动作-值函数的学习。<code>QPLEX</code>利用<code>advantage of a duplex dueling</code>架构将<code>IGM</code>约束条件编码到神经网络结构中，保证<code>IGM</code>的一致性。</p>
<p><code>QPLEX</code>的主要亮点：分别对联合$Q$值$Q_{tot}$和各个<code>agent</code>的<code>Q</code>值$Q_{i}$使用<code>Dueling structure</code>：$Q=V+A$进行分解，将<code>IGM</code>一致性转化为易于实现的优势函数取值范围约束，从而方便了具有线性分解结构的值函数的学习，这种分解让$Q$值的获得更为具体，这样可以进一步判断$Q$值的获得是由于状态还是由于采取的动作的优势。</p>
<h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><p>联合动作价值函数的<code>dueling</code>分解：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281839165.png" alt=""></p>
<p>个体动作价值函数的<code>dueling</code>分解：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281839619.png" alt=""></p>
<p>约束条件：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281840590.png" alt=""></p>
<p>当执行最佳动作时，对于联合动作优势函数有：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281840019.png" alt=""></p>
<h2 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281841265.png" alt=""></p>
<p>框架主要分三两部分：</p>
<ul>
<li><code>Dueling Mixing</code>网络</li>
<li>整体的<code>Duplex Dueling</code>架构</li>
<li>智能体网络结构<code>Transformation</code>网络</li>
</ul>
<p><code>Agent network</code>的输入为$t$时刻智能体$a$的观测值$o_{i}^{t}$和$t-1$时刻智能体$a$的动作$a_{i}^{t-1}$，输出为$t$时刻智能体$a$的值函数$Q_{i}(\tau_{i}, a_{i})$。<code>Agent network</code>由<code>DRQN</code>网络实现，根据不同的任务需求，不同智能体的网络可以进行单独训练，也可进行参数共享，<code>DRQN</code>是将<code>DQN</code>中的全连接层替换为<code>LSTM</code>网络，循环网络在观测质量变化的情况下，具有更强的适应性。如图所示，其网络一共包含$3$层，输入层（<code>MLP</code>多层神经网络)、 中间层（<code>GRU</code>门控循环神经网络）、输出层（<code>MLP</code>多层神经网络）。<br><code>Transformation network</code>的输入为智能体$i$的状态价值函数$V_{i}(\tau_{i})$、优势函数$A_{i}(\tau_{i}, a_{i})$和全局状态$s_{t}$，输出为基于全局状态$s$，智能体$i$的状态价值函数$V_{i}(\tau)$、优势函数$A_{i}(\tau, a_{i})$。具体实现方式是：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281841239.png" alt=""></p>
<p><code>Dueling Mixing network</code>的输入为智能体$i$的状态价值函数$V_{i}(\tau)$、优势函数$A_{i}(\tau, a_{i})$和全局状态$s_{t}$，输出为联合动作价值函数$Q_{tot}(\tau, a)$。因为$V$仅与状态$s$（或联合观测历史$\tau$）有关，$V_{tot}(\tau)$表示为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281842146.png" alt=""></p>
<p>$A_{tot}(\tau, a)$表示为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281842375.png" alt=""></p>
<p>其中，$\lambda_{i}(\tau, a)&gt;0$，保证贪婪动作选择与策略一致：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281842527.png" alt=""></p>
<p>最后，我们可以得到最终的$Q_{tot}(\tau, a)$如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508281843228.png" alt=""></p>
<h2 id="算法更新流程"><a href="#算法更新流程" class="headerlink" title="算法更新流程"></a>算法更新流程</h2><p>损失函数类似<code>QMIX</code>，$\mathcal{L}(\theta)=\sum_{i=1}^{b}[y_{i}^{tot}-Q_{tot}(\tau, u, s;\theta)]^{2}$。其中，$y_{i}^{tot}=r+\gamma \max_{u^{\prime}}Q_{tot}(\tau^{\prime}, u^{\prime}, s^{\prime}; \theta^{-})$。</p>
<h1 id="WQMIX-NeurIPS2020"><a href="#WQMIX-NeurIPS2020" class="headerlink" title="WQMIX-NeurIPS2020"></a><code>WQMIX-NeurIPS2020</code></h1><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>在协作任务<code>MARL</code>环境中，多个智能体共享同一个奖励值，因此常常存在<code>lazy agent</code>和<code>credit assignment</code>问题。为了解决该问题，<code>VDN</code>算法考虑以一种最简单的求和方式来分解系统的整体值函数：$Q_{tot}(\tau, u)=\sum_{i=1}^{n}Q_{i}(\tau_{i}, u_{i})$ 。这种值函数分解的方式比较简单，由于它过于满足“单调性”条件，大大限制了其非线性函数逼近能力。因此有研究者提出了<code>QMIX</code>算法，通过设计<code>Mixing network</code>（混合网络）来拟合$Q_{tot}^{\ast}$，在满足分布式策略“单调性”的前提下，大大提高了网络的非线性函数逼近能力。</p>
<p>但是<code>QMIX</code>算法满足的单调性约束其实是充分但不必要条件，这就说明还存在一些场景的值函数是<code>QMIX</code>无法精确拟合的。例如，当智能体和智能体之间在执行动作的时候互相影响，某个智能体的决策需要考虑其它智能体的动作时，<code>QMIX</code>算法是没有考虑的。因此，<code>QMIX</code>的函数表达能力还是受到了一定的限制。因此，如何进一步摆脱<code>QMIX</code>函数表达能力上的限制，是本文研究的主要目标。</p>
<h2 id="算子定义及性质"><a href="#算子定义及性质" class="headerlink" title="算子定义及性质"></a>算子定义及性质</h2><p>贝尔曼最优算子的定义如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291711199.png" alt=""></p>
<p>$Q_{tot}$所在的函数空间如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291714241.png" alt=""></p>
<p><code>QMIX</code>算法可以看作以下优化问题：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291714436.png" alt=""></p>
<p>作者将上式定义为<code>QMIX</code>算子，记作$\mathcal{T}^{\ast}_{\text{Qmix}}$，它可看作是两个算子的复合：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291718255.png" alt=""></p>
<p>其中$\Pi_{\text{Qmix}}$的定义如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291719700.png" alt=""></p>
<ul>
<li>$\mathcal{T}^{\ast}_{\text{Qmix}}$不是一个压缩映射。假设下表中左边的矩阵为$Q$函数的收益矩阵 ，该矩阵无法用$\mathcal{Q}^{mix}$中的值函数来表达。利用$\Pi_{\text{Qmix}}Q^{\ast}$，可能会得到下表中中间或者右边的$Q_{tot}$，它们都能够使智能体获得最多的奖励$r=1$。因此，通过$\mathcal{T}^{\ast}_{\text{Qmix}}$算子计算出的$Q_{tot}$可能不是唯一的，所以它不具备压缩映射不动点唯一的特征，也就不是压缩映射。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291719547.png" alt=""></p>
<ul>
<li><code>QMIX</code>中最大化$Q_{tot}$得到的联合动作不总是对的。例如，下表中的$2$智能体+$3$动作任务，左边是真实的收益矩阵$Q^{\ast}$，右边是$\Pi_{\text{Qmix}}$算子返回的一个$Q_{tot}$。根据$Q^{\ast}$，智能体采取$r=8$对应的动作组合会获得最大奖励；但是为了满足“单调性约束”，$\Pi_{\text{Qmix}}Q^{\ast}$返回的$Q_{tot}$可能会把原来$r=8$处的奖励拟合成了$-12$，这样，最多只能获得$r=0$的奖励。通俗解释就是，$A$在$B=0$时应该选$0$，$B=1$时也应该选$0$，依此类推。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291720847.png" alt=""></p>
<ul>
<li><code>QMIX</code>可能低估了某些联合动作的价值。这个性质和上一个性质描述的内容差不多。错误的$\arg\max$计算结果，必然导致对值函数进一步的错误估计。例如，上表中$r=8$处的收益就被错误估计成了$-12$。</li>
</ul>
<h2 id="算法详解"><a href="#算法详解" class="headerlink" title="算法详解"></a>算法详解</h2><p>论文认为，<code>QMIX</code>算子的不足主要源于更新网络的这一步中：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291714436.png" alt=""></p>
<p>这里的$\arg\min$是指找到一个新的$q$，使得对当前所有动作的误差之和最小。但是，当改进多个次优动作带来的误差减小好于改进单个最优动作带来的误差减小时，算子则有优先学习次优动作，而低估了最优动作。进一步地，此时智能体会更倾向于选择次优动作，导致缓存池中次优动作的样本量比最优动作的样本量大，所以每次更新时次优动作误差占比就越大，对最优联合动作的$Q$值评估就越不准确。</p>
<p>因此，这篇论文考虑的<code>QMIX</code>算子的不足之处就是在更新过程中<code>QMIX</code>算子同等对待所有样本。论文为了优化<code>QMIX</code>更新算子，提出<code>Weighted QMIX</code>更新算子，为不同样本赋予不同权重，即：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291730999.png" alt=""></p>
<p>在<code>QMIX</code>算法更新过程中，如果要为不同样本赋予不同权重，则首先需要准确评估样本，也就是拟合目标$r+\gamma \cdot \max Q$。然而，这里的$Q$并不能使用回当前正在学习的$Q^{mix}$，因为这个函数对最优联合动作的评估是不准确的。其次，如果再新增一个与$Q^{mix}$结构相同的网络，这个网络又会由于单调性约束而表达能力不足。</p>
<p>因此论文新增一个无需满足单调性约束的新的全局$Q$网络$\hat{Q}^{\ast}$，这个$Q$网络与$Q^{mix}$结构相似，不同之处在于混合网络不使用超网络，而是直接用<code>MLP</code>代替：各智能体的局部$Q$值以及当前全局状态拼接后直接输入混合网络，得到全局$Q$值。由于不使用超网络，所以<code>MLP</code>的混合网络中的权重也无需取绝对值。$\hat{Q}^{\ast}$就比$Q^{mix}$具有更强的表达能力，能更好逼近真实全局$Q$函数，更适合用来准确评估样本中的拟合目标：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291731261.png" alt=""></p>
<p>这里需要注意的是，$\hat{Q}^{\ast}$无需满足单调性约束，提高了表达能力，但是代价就是找出下一个状态的最优联合动作只能直接在联合动作动作中找，不能再像<code>QMIX</code>一样各个局部$Q$函数找出最优局部动作即为全局最优联合动作，因为此时$\hat{Q}^{\ast}$不满足<code>IGM</code>。所以在这一步中，下一个状态的最优联合动作还是用回当前正在学习的<code>QMIX</code>的全局$Q$网络$Q^{mix}$来获取。因此论文还需要使用新的贝尔曼最优算子。至此，论文提出的<code>Weighted QMIX</code>算子可表示为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291732902.png" alt=""></p>
<p>算法的整体框架如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291733671.png" alt=""></p>
<h3 id="权重设计"><a href="#权重设计" class="headerlink" title="权重设计"></a>权重设计</h3><p>论文提供了两种权重设计方案。第一种是<code>Idealised Central Weighting</code>（理想中心加权），这种方案直接提高最优动作的权重，降低次优动作的权重。然而，这样就需要遍历$\hat{Q}^{\ast}$的联合动作空间来找出最优联合动作，才能判断是否需要为当前样本提高权重，这样并不现实。所以论文使用了近似方法，只要$Q$值比近似最优大，则判断当前样本为最优联合动作：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291734267.png" alt=""></p>
<p>第二种方案是<code>Optimistic Weighting</code>（乐观加权），这种方案直接提高低估样本的权重：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202508291735536.png" alt=""></p>
<h1 id="COMA-AAAI2018"><a href="#COMA-AAAI2018" class="headerlink" title="COMA-AAAI2018"></a><code>COMA-AAAI2018</code></h1><p><code>COMA</code>是一种<code>policy-based</code>的多智能体算法，其核心思想是在于“独立回报分配”。当多个智能体在执行任务时，获得的奖励分数是由所有智能体的行为共同决定的，那么怎么把一个奖励分数合理的分配给每一个智能体呢？这就是<code>COMA</code>算法要解决的关键问题。在网络结构上，<code>COMA</code>沿用了<code>Actor-Critic</code>架构，其中<code>Actor</code>使用基于<code>policy-based</code>的<code>RNN</code>网络。</p>
<h2 id="算法背景和思想"><a href="#算法背景和思想" class="headerlink" title="算法背景和思想"></a>算法背景和思想</h2><p>在多智能体强化学习场景中，每个智能体在某一时刻只掌握局部的信息，无法全局观测环境状态。为了促进合作，各个智能体的动作对全局奖励有不同的贡献，因此需要一种有效的方法来分配奖励。<code>COMA</code>引入了“反事实基线”<code>(Counterfactual Baseline)</code>的概念，专门用于降低多智能体策略梯度方法中的方差。</p>
<p><code>COMA</code>的核心思想是通过引入一个基线，该基线模拟在固定其他智能体动作的前提下，某个智能体选择不同动作时对全局奖励的影响，从而更精确地衡量当前动作的贡献，减少策略梯度更新中的方差。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192146348.png" alt=""></p>
<h2 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h2><h3 id="全局策略梯度"><a href="#全局策略梯度" class="headerlink" title="全局策略梯度"></a>全局策略梯度</h3><p>对于多智能体问题，每个智能体$(i)$的策略梯度可以表示为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192159569.png" alt=""></p>
<p>其中，$(\pi_{\theta_{i}}(a_{i} \vert s))$是智能体$(i)$在状态$(s)$下选择动作$(a_{i})$的概率，$(Q_{i}(s,a))$是该智能体在执行动作$(a_{i})$时的动作价值函数。</p>
<h3 id="反事实基线"><a href="#反事实基线" class="headerlink" title="反事实基线"></a>反事实基线</h3><p>为了减小方差，<code>COMA</code>提出了反事实基线$(b(s,a_{-i}))$，该基线衡量在保持其他智能体动作$(a_{-i})$不变的情况下，智能体$(i)$选择其他动作时的期望收益。具体公式为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192206736.png" alt=""></p>
<p>这里$(a_{-i})$的表示除智能体$(i)$之外，其他智能体的动作组合。</p>
<h3 id="策略更新"><a href="#策略更新" class="headerlink" title="策略更新"></a>策略更新</h3><p>有了反事实基线之后，<code>COMA</code>中智能体$(i)$的策略更新公式变为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192208564.png" alt=""></p>
<p>其中，$(Q_{i}(s,a))$是在当前状态下，所有智能体执行一组动作$(a)$时的全局奖励，而$(b(s,a_{-i}))$是该动作的反事实基线。</p>
<h3 id="全局值函数"><a href="#全局值函数" class="headerlink" title="全局值函数"></a>全局值函数</h3><p><code>COMA</code>中的值函数$(Q(s,a))$和基线$(b(s,a_{-i}))$都是通过集中化的学习进行优化的，虽然决策是去中心化的，但值函数和基线都依赖于全局的状态和动作信息。</p>
<h2 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h2><ol>
<li>初始化智能体策略和集中式的全局值函数。</li>
<li>智能体与环境交互，收集经验数据。</li>
<li>使用经验数据更新全局值函数$(Q(s,a))$。</li>
<li>计算反事实基线$(b(s,a_{-i}))$。</li>
<li>计算每个智能体的策略梯度，并更新策略参数。</li>
<li>重复上述过程，直至智能体策略收敛。</li>
</ol>
<h1 id="MADDPG-NeurIPS2017"><a href="#MADDPG-NeurIPS2017" class="headerlink" title="MADDPG-NeurIPS2017"></a><code>MADDPG-NeurIPS2017</code></h1><h2 id="背景与动机"><a href="#背景与动机" class="headerlink" title="背景与动机"></a>背景与动机</h2><p> 在多智能体系统中，多个智能体同时作用于同一个环境，相互之间可能是竞争的、协作的，或者二者兼有。这类环境下，单智能体算法如<code>DDPG</code>往往无法取得较好的效果，因为每个智能体的行为都会影响其他智能体的状态和奖励。为了解决这一问题，<code>MADDPG</code>采用了一种集中式训练，分布式执行的架构。</p>
<ul>
<li>集中式训练：训练期间，每个智能体可以观察所有其他智能体的动作和状态，从而学到更有效的策略。</li>
<li>分布式执行：在执行阶段，智能体只依赖其自身的观测来做出决策，保持分布式控制的特性。</li>
</ul>
<h2 id="算法细节"><a href="#算法细节" class="headerlink" title="算法细节"></a>算法细节</h2><ul>
<li><code>Actor-Critic</code>架构：每个智能体都有一个<code>Actor</code>网络用于输出动作，以及一个<code>Critic</code>网络用于评估当前策略的好坏。<code>Actor</code>直接学习确定性策略，而<code>Critic</code>负责估算状态-动作对的$Q$值。</li>
<li>集中式训练，分布式执行：在训练阶段，Critic网络可以访问所有智能体的信息，包括状态和动作，这允许它准确评估每个动作的期望回报。然而，在执行阶段，每个智能体的<code>Actor</code>网络只能基于自己的局部观察来做出决策。</li>
<li>经验回放：为了提高训练的稳定性和效率，<code>MADDPG</code>使用了经验回放机制。智能体的每次交互会被存储在一个回放缓冲区中，训练时会从这个缓冲区中随机抽取一批经验来更新网络。</li>
<li>目标网络：为了进一步稳定训练过程，<code>MADDPG</code>为每个<code>Actor</code>和<code>Critic</code>网络维护了一个目标网络。这些目标网络的参数会缓慢跟踪对应网络的参数，用于计算期望回报的稳定目标。</li>
<li>奖励和惩罚：<code>MADDPG</code>允许设计复杂的奖励机制，包括对合作行为的奖励和对对立行为的惩罚，来引导智能体学习如何在多种交互场景中作出最优决策。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411202322720.png" alt=""></p>
<h3 id="中心化训练"><a href="#中心化训练" class="headerlink" title="中心化训练"></a>中心化训练</h3><p>与<code>DDPG</code>不同之处在于，<code>MADDPG</code>中的每个智能体在计算自身的<code>Critic</code>的前向传播时，把包括自身在内的所有智能体的观测拼接成观测向量$s=\{s_{1},s_{2},\cdots,s_{n}\}$，把所有智能体动作拼接成动作向量$\{a_{1},a_{2},\cdots,a_{n}\}$，$(s,a)$作为<code>Online Critic Net</code>的输入，输出一维的$Q$值，即$Q_{\phi_{i}}(s,a)$，换句话说就是利用环境中智能体的全局信息（全局观测和动作）来“中心化”训练自身的<code>Critic Net</code>。现在，有了$Q_{\phi_{i}}(s,a)$，同时有了回放样本可以计算出$Q_{\phi_{i}}(s^{\prime},a^{\prime})$，接下来要做的和<code>DDPG</code>相同，以时序差分误差构建二者的<code>MSE</code>损失函数，然后利用梯度下降更新参数$\phi_{i}$。具体的损失函数及梯度如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411202312830.png" alt=""></p>
<p>其中，$\boldsymbol{a}^{\prime}=\{\mu_{\tilde{\theta}_{1}}(s_{1}^{\prime}),\mu_{\tilde{\theta}_{1}}(s_{2}^{\prime}),\ldots,\mu_{\tilde{\theta}_{1}}(s_{N}^{\prime})\}$这里用<code>Target Policy Net</code>计算下一状态智能体所采取的动作。需要注意，每个智能体的<code>Target Policy Net</code>输入是自身局部观测$s_{i}$。</p>
<h3 id="分布式执行"><a href="#分布式执行" class="headerlink" title="分布式执行"></a>分布式执行</h3><p>在计算自身的<code>Actor</code>的前向传播时，每个智能体只将自身的局部观测向量$s=\{s_{1},s_{2},\ldots,s_{n}\}$作为<code>Online Actor Net</code>的输入，输出一个确定性动作$a_{i}$，即$\mu_{\theta_{i}}(s_{i})$。接下来和<code>DDPG</code>一样，计算时序差分误差的<code>MSE</code>损失函数并计算关于参数$\theta_{i}$的梯度，然后利用梯度下降更新参数。损失函数及梯度如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411202320613.png" alt=""></p>
<h1 id="MAAC-ICML2019"><a href="#MAAC-ICML2019" class="headerlink" title="MAAC-ICML2019"></a><code>MAAC-ICML2019</code></h1><h2 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h2><p>多智能体强化学习算法主要有三种：</p>
<ul>
<li>第一种方法是独立地训练环境中的每个<code>agent</code>，以最大化个人的收益，将其他<code>agent</code>视为环境的一部分。该方法违反了强化学习的基本假设，即环境的平稳性和马尔科夫性。在这种方法中，任何<code>agent</code>都是动态的、非平稳的。</li>
<li>第二种方法是将所有<code>agent</code>集成训练为一个<code>agent</code>。但此方法不易扩展，动作空间会随着<code>agent</code>的数量增加而呈指数型增加。同时在执行过程中，还需要<code>agent</code>之间的高度通信。在现实中，这种要求较难满足。</li>
<li>上述两种方法过于极端，第三种方法是上述两种的结合，集中式训练分散式执行。<code>Critic</code>通过所有<code>agent</code>的信息集中学习，<code>Actor</code>只从各自对应的<code>agent</code>处接收信息。因此在测试期间，<code>agent</code>执行策略并不需要知道其他<code>agent</code>的信息。但即便如此，该方法也不易扩展，并且普遍不适用于合作、竞争和混合的环境和任务。</li>
</ul>
<p>本文在几个方向上扩展了这些之前的工作。主要思想是学习一个具有注意机制的集中式<code>critic</code>。本文背后的直觉来自于这样一个事实：在许多现实环境中，<code>agent</code>知道它应该注意的其他<code>agent</code>是有益的。然而在多智能体强化学习的典型集中式方法没有考虑这些动态，而是简单地考虑所有时间点的所有<code>agent</code>。注意力<code>critic</code>能够动态地选择在训练过程中的每个时间点都要关注哪些<code>agent</code>，从而提高具有复杂交互的多智能体领域的性能。</p>
<h2 id="算法详解-1"><a href="#算法详解-1" class="headerlink" title="算法详解"></a>算法详解</h2><p>在该算法中，注意力机制的作用主要在 每个<code>agent</code>查询其他<code>agent</code>关于他们的观测和动作的信息，并将这些信息合并到其值函数的估计中。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181709884.png" alt=""></p>
<p>对于所有的<code>agent</code>，<code>critic</code>根据观测$o=\left(o_{1}, \ldots, o_{N}\right)$和动作$a=\left(a_{1}, \ldots, a_{N}\right)$来计算<code>Q-value function</code>$Q_{i}^{\psi}(o, a)$。我们表示除了$i$之外的<code>agent</code>的集合为$\backslash i$，并使用$j$对其进行索引。$Q_{i}^{\psi}(o, a)$是<code>agent</code>的观测、动作以及其他<code>agent</code>的影响的函数：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181706862.png" alt=""></p>
<p>其中，$f_i$是一个二层的<code>MLP</code>，$g_i$是一个单层的<code>MLP</code>嵌入函数。$x_i$为其他<code>agent</code>的影响，通过每个<code>agent</code>的值加权和计算得来：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181710393.png" alt=""></p>
<p>其中，值$v_j$是智能体$j$的<code>embedding</code>函数。该值用嵌入函数编码，然后由共享矩阵$V$进行线性变换。$h$是一个元素级的非线性函数，本文使用的是<code>ReLU</code>。</p>
<p>注意力权重$\alpha_j$将嵌入$e_{j}$与$e_i = g_i(o_i, a_i)$进行比较，使用双线性映射（即<code>query-key system</code>），并将这两个嵌入的相似性值传入<code>softmax</code>函数中：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181720106.png" alt=""></p>
<p>其中，$W_q$将$e_i$转化为一个<code>query</code>，$W_k$将$e_j$转化为一个<code>key</code>，然后根据这两个矩阵的维数进行匹配缩放，以防止梯度消失。</p>
<p>在本文中，作者使用多头注意力机制。每个头使用一组单独的参数$W_k,W_q,V$产生来自其他所有<code>agent</code>的聚合相似性，然后拼接所有头的输出（聚合相似性）作为一个单一向量。作用在于每个头可以专注于不同的<code>agent</code>的加权混合。</p>
<p>需要注意的是，用于提取选择器、键和值的权重在所有<code>agent</code>之间共享，这鼓励了一个公共的嵌入空间。即使在对抗的环境中，<code>critic</code>参数的共享也是可能的，因为多智能体的值函数近似本质上是一个多任务回归问题。这种参数共享使我们的方法能够在对单个<code>agent</code>的奖励不同但共享共同特征的环境中有效地学习。在训练时，这种方法可以很容易地扩展到包括除了局部观测和动作之外的其他信息。</p>
<p><code>Learning with Attentive Critics</code>由于参数共享，所有的<code>critic</code>一起更新，以最小化一个联合回归损失函数：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181726323.png" alt=""></p>
<p>其中，$\bar{\psi}$和$\bar{\theta}$分别是<code>targent critics</code>和<code>target policies</code>的参数。智能体$i$的动作值估计$Q_{i}^{\psi}$接收所有智能体的观测和动作。$\alpha$是决定最大化熵和奖励之间的平衡的温度参数。各个策略通过梯度上升更新：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181732197.png" alt=""></p>
<p>其中，$b(o,a_{\backslash i})$是用于计算优势函数的多智能体<code>baseline</code>。需要注意的是，我们根据所有<code>agent</code>的当前策略抽样所有的动作来计算智能体$i$的梯度估计（这点与<code>MADDPG</code>不同，<code>MADDPG</code>中，其他<code>agent</code>的动作是从经验重放缓冲区抽样，这会导致过度泛化，因为<code>agent</code>无法根据当前策略进行学习）。</p>
<p><code>Multi-Agent Advantage Function</code>使用一个仅从$Q^\psi_i(o,a)$中边缘化给定<code>agent</code>动作的<code>baseline</code>的优势函数，可以帮助解决多代理的信用分配问题。换而言之，将所有其他<code>agent</code>固定，通过比较一个<code>agent</code>的特殊动作的价值的与平均动作的价值，我们可以了解到，上述动作是否会导致预期收益的增加，或者任何奖励的增加是否归因于其他<code>agent</code>的行为。该优势函数如下：<br><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181741355.png" alt=""></p>
<p>在离散策略的情况下，我们可以通过输出智能体<code>i</code>的每个可能的动作$a_i∈A_i$的预期回报$Q_i(o,(a_i,a_{\backslash i}))$来计算<code>baseline</code>。然后，我们就可以精确地计算出期望值：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202512181745351.png" alt=""></p>
<p>为了做到这一点，我们必须从$Q_i$的输入中删除$a_i$，并为每个动作输出一个值。我们为每个<code>agent</code>添加了一个观测编码器，$e_i =g_i^o(o_i)$，使用这个编码器来替代上述的$e_i =g_i(o_i,a_i)$，同时修改$f_i$，使其为每个可能的动作输出一个值。</p>
<p>在连续策略的情况下，我们可以通过从智能体$i$的策略中采样来估计上述期望，或者通过学习一个单独的只输入其他<code>agent</code>的动作的值来估计上述期望。</p>
<h1 id="ROMA-ICML2020"><a href="#ROMA-ICML2020" class="headerlink" title="ROMA-ICML2020"></a><code>ROMA-ICML2020</code></h1><h1 id="MAPPO-NeurIPS2022"><a href="#MAPPO-NeurIPS2022" class="headerlink" title="MAPPO-NeurIPS2022"></a><code>MAPPO-NeurIPS2022</code></h1><h2 id="网络结构-2"><a href="#网络结构-2" class="headerlink" title="网络结构"></a>网络结构</h2><p>和单智能体<code>PPO</code>算法一样，<code>MAPPO</code>算法中每个智能体都有各自的<code>Actor</code>网络和<code>Critic</code>网络（如果所有智能体的状态空间和动作空间也相同，即同构，也可以所有智能体共享一套<code>Actor</code>和<code>Critic</code>网络）。与单智能体<code>PPO</code>不同的是，<code>MAPPO</code>的<code>Critic</code>网络可以接收有关全局状态的信息，这个全局状态可以是由所有智能体的观察拼接而成，也可以是环境直接提供。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171817653.png" alt=""></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>和单智能体<code>PPO</code>算法一样，损失函数由<code>Actor Loss</code>和<code>Critic Loss</code>组成：</p>
<ul>
<li><code>Actor Loss</code>为最小化负的智能体在当前策略下的预期累积奖励$-E[\frac{\pi(a|S_t;\theta)}{\pi(a|S_t;\theta_k)} A_t]$</li>
<li><code>Critic Loss</code>为回报和状态价值函数的均方差$[(G_t-V(s,w))]^{2}$</li>
</ul>
<h2 id="采样和更新方式"><a href="#采样和更新方式" class="headerlink" title="采样和更新方式"></a>采样和更新方式</h2><h3 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h3><p>如智能体间不共享参数，即每个智能体有各自的<code>Actor</code>和<code>Critic</code>网络，则给每个智能体建立一个<code>replay buffer</code>，将该智能体交互中获得的$s_{t}, a_{t}, r，s_{t+1}$存入对应的<code>replay buffer</code>中。另在<code>replay buffer</code>中增加<code>mask</code>，记录每一时刻智能体是否存活，以便后续死亡的智能体后续数据不用于更新网络。一般情况下，不同智能体间不共享奖励。</p>
<h3 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h3><p>如果智能体间不共享参数，则针对每一个智能体分别从<code>replay buffer</code>中抽样，训练其网络，其更新函数与<code>PPO</code>更新函数整体一致，并且增加了<code>GAE</code>、<code>value normlization</code>等技巧。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%80%BB%E7%BB%93/" rel="tag"># 总结</a>
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># 强化学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/11/03/CloseAirCombat%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/" rel="prev" title="CloseAirCombat使用总结">
                  <i class="fa fa-chevron-left"></i> CloseAirCombat使用总结
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/11/29/ROS/" rel="next" title="ROS">
                  ROS <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">木霈玖</span>
</div>

    </div>
  </footer>

  
  <script src="//unpkg.com/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//unpkg.com/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
