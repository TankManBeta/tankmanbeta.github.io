<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//unpkg.com/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//unpkg.com/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="前言学习一下多智能体强化学习相关算法。">
<meta property="og:type" content="article">
<meta property="og:title" content="多智能体强化学习">
<meta property="og:url" content="http://example.com/2024/11/17/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="木霈玖的博客">
<meta property="og:description" content="前言学习一下多智能体强化学习相关算法。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171915509.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172038329.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171918940.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171923497.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172041620.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172046802.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172048239.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172102666.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182259010.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182302432.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182308208.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182329900.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182341968.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182351374.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411190001341.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411190007735.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411190011682.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192146348.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192159569.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192206736.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192208564.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411202322720.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411202312830.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411202320613.png">
<meta property="og:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171817653.png">
<meta property="article:published_time" content="2024-11-17T08:54:28.000Z">
<meta property="article:modified_time" content="2024-11-30T13:31:51.757Z">
<meta property="article:author" content="木霈玖">
<meta property="article:tag" content="总结">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171915509.png">


<link rel="canonical" href="http://example.com/2024/11/17/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>多智能体强化学习 | 木霈玖的博客</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">木霈玖的博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#VDN"><span class="nav-number">2.</span> <span class="nav-text">VDN</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#QMIX"><span class="nav-number">3.</span> <span class="nav-text">QMIX</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#QTRAN"><span class="nav-number">4.</span> <span class="nav-text">QTRAN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#QTRAN%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="nav-number">4.1.</span> <span class="nav-text">QTRAN直观理解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%AF%E5%88%86%E8%A7%A3%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E5%85%85%E5%88%86%E6%9D%A1%E4%BB%B6"><span class="nav-number">4.2.</span> <span class="nav-text">可分解值函数的充分条件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%AF%E5%88%86%E8%A7%A3%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E5%BF%85%E8%A6%81%E6%9D%A1%E4%BB%B6"><span class="nav-number">4.3.</span> <span class="nav-text">可分解值函数的必要条件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-number">4.4.</span> <span class="nav-text">网络结构</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#COMA"><span class="nav-number">5.</span> <span class="nav-text">COMA</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E8%83%8C%E6%99%AF%E5%92%8C%E6%80%9D%E6%83%B3"><span class="nav-number">5.1.</span> <span class="nav-text">算法背景和思想</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="nav-number">5.2.</span> <span class="nav-text">公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-number">5.2.1.</span> <span class="nav-text">全局策略梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E4%BA%8B%E5%AE%9E%E5%9F%BA%E7%BA%BF"><span class="nav-number">5.2.2.</span> <span class="nav-text">反事实基线</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%9B%B4%E6%96%B0"><span class="nav-number">5.2.3.</span> <span class="nav-text">策略更新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">5.2.4.</span> <span class="nav-text">全局值函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span class="nav-number">5.3.</span> <span class="nav-text">算法步骤</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MADDPG"><span class="nav-number">6.</span> <span class="nav-text">MADDPG</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E5%8A%A8%E6%9C%BA"><span class="nav-number">6.1.</span> <span class="nav-text">背景与动机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E7%BB%86%E8%8A%82"><span class="nav-number">6.2.</span> <span class="nav-text">算法细节</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AD%E5%BF%83%E5%8C%96%E8%AE%AD%E7%BB%83"><span class="nav-number">6.2.1.</span> <span class="nav-text">中心化训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E6%89%A7%E8%A1%8C"><span class="nav-number">6.2.2.</span> <span class="nav-text">分布式执行</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MAPPO"><span class="nav-number">7.</span> <span class="nav-text">MAPPO</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-1"><span class="nav-number">7.1.</span> <span class="nav-text">网络结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">7.2.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%87%87%E6%A0%B7%E5%92%8C%E6%9B%B4%E6%96%B0%E6%96%B9%E5%BC%8F"><span class="nav-number">7.3.</span> <span class="nav-text">采样和更新方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%87%E6%A0%B7"><span class="nav-number">7.3.1.</span> <span class="nav-text">采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0"><span class="nav-number">7.3.2.</span> <span class="nav-text">更新</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">木霈玖</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/11/17/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="木霈玖">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="木霈玖的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          多智能体强化学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-11-17 16:54:28" itemprop="dateCreated datePublished" datetime="2024-11-17T16:54:28+08:00">2024-11-17</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2024-11-30 21:31:51" itemprop="dateModified" datetime="2024-11-30T21:31:51+08:00">2024-11-30</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>学习一下多智能体强化学习相关算法。</p>
<a id="more"></a>
<h1 id="VDN"><a href="#VDN" class="headerlink" title="VDN"></a><code>VDN</code></h1><p><code>VDN</code>中提出一种通过反向传播将团队的奖励信号分解到各个智能体上的这样一种方式。其网络结构如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171915509.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172038329.png" alt=""></p>
<p>先看上图中的图<code>1</code>，画的是两个独立的智能体，因为对每个智能体来说，观测都是部分可观测的，所以<code>Q</code>函数是被定义成基于观测历史数据所得到的$Q(h_{t},a_{t})$，实际操作的时候直接用<code>RNN</code>来做就可以。图<code>2</code>说的就是联合动作值函数由各个智能体的值函数累加得到的：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171918940.png" alt=""></p>
<p>其中$d$表示$d$个智能体，$\tilde{Q}_{i}$由每个智能体的局部观测信息得到，$\tilde{Q}_{i}$是通过联合奖励信号反向传播到各个智能体的$\tilde{Q}_{i}$上进行更新的。这样各个智能体通过贪婪策略选取动作的话，也就会使得联合动作值函数最大。</p>
<p>总结来说：值分解网络旨在学习一个联合动作值函数$Q_{tot}(\tau,\mathbf{u})$，其中$\tau \in \mathbf{T} \equiv \mathcal{T}^{n}$是一个联合动作-观测的历史轨迹，$\mathbf{u}$是一个联合动作。它是由每个智能体$a$独立计算其值函数$Q_{a}\left(\tau^{a},u^{a};\theta^{a}\right)$，之后累加求和得到的。其关系如下所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171923497.png" alt=""></p>
<h1 id="QMIX"><a href="#QMIX" class="headerlink" title="QMIX"></a><code>QMIX</code></h1><p>在之前的值分解网络中，拿到了联合动作的$Q$值之后，我们就可以直接取能够获取最大的$Q_{tot}(\tau, \mathbf{u})$所对应的联合动作。这种方式就能够实现集中式学习，但是得到分布式策略。并且对全局值函数做<code>argmax</code>与对单个智能体地值函数做<code>argmax</code>能够得到相同的结果：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172041620.png" alt=""></p>
<p>这样的话，每个智能体$a$都可以基于$Q_{a}$以贪婪策略选择动作。由于每个子智能体都采用贪婪策略，因此这个算法必定会是<code>off-policy</code>的算法，这一点是很容易实现的，并且样本的利用率会比较高。在<code>VDN</code>中采用的是线性加权，因此上述等式会成立，而如果是采用一个神经网络来学习融合各个智能体的$Q$函数的话，上述等式就未必会成立了。</p>
<p>在<code>QMIX</code>中也是需要一个联合动作值函数的，但是与值分解网络的不同之处在于，这个联合动作值函数并不是简单地由各个智能体的值函数线性相加得到的。为了保证值函数的单调性来使得上式能够成立，作者对联合动作值函数$Q_{tot}$和单个智能体动作值函数$Q_{a}$之间做了一个约束：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172046802.png" alt=""></p>
<p>设计思想如上所示，具体的网络结构如下图所示：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172048239.png" alt=""></p>
<p>上述这个复杂的网络结构可以拆分为如下三部分：</p>
<ul>
<li><code>agent networks</code>：对于每个智能体都要学一个独立的值函数$Q_{a}\left(\tau^{a}, u^{a}\right)$，单个智能体的网络结构采用<code>DRQN</code>的网络结构，在每个时间步，接收当前的独立观测$O_{t}^{a}$和上一个动作$\mu_{t-1}^{a}$，如上图<code>(c)</code>所示。</li>
<li><code>mixing network</code>：是一个全连接网络，接收每个智能体的输出$Q_{n}\left(\tau^{n}, u_{t}^{n}\right)$作为输入，输出联合动作值函数$Q_{t o t}(\boldsymbol{\tau}, \boldsymbol{u})$，对每个子智能体的动作值函数做非线性映射，并且要保证单调性约束。想要保证公式<code>(2)</code>的单调性约束的话，我们只需要保证<code>mixing network</code>的权重非负即可。</li>
<li><code>hypernetworks</code>：<code>hypernetworks</code>网络去产生<code>mixing network</code>的权重，超参数网络输入状态$s$，输出<code>Mixing</code>网络的每一层的超参数向量，激活函数来使得输出非负，对于<code>Mixing</code>网络参数的偏置并没有非负的要求。</li>
</ul>
<p><code>QMIX</code>的训练方式是端到端的训练：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411172102666.png" alt=""></p>
<h1 id="QTRAN"><a href="#QTRAN" class="headerlink" title="QTRAN"></a><code>QTRAN</code></h1><p>前面介绍的<code>VDN</code>，<code>QMIX</code>算法，都是基于值方法并且用于协作式<code>MARL</code>任务中。这些算法本质上都在找分布式最优策略，并且满足下式中描述的关系：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182259010.png" alt=""></p>
<p>其中，$Q_{jt}$表示整体的联合$Q$函数；$Q_{i}$表示智能体$i$的值函数。在本文，作者将<code>(1)</code>式中描述的关系定义为<code>IGM(Individual-Global-Max)</code>条件。</p>
<p><code>VDN</code>为了满足<code>IGM</code>条件，直接将值函数分解成“加和形式<code>(Additivity)</code>”：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182302432.png" alt=""></p>
<p>从<code>(2)</code>式可以得出$\frac{\partial Q_{jt}(\tau,u)}{\partial Q_{i}(\tau_{i},u_{i})}\equiv1,\forall i\in\mathcal{N}$，可见只要满足<code>(2)</code>式就能满足<code>IGM</code>条件。</p>
<p>但是<code>QMIX</code>觉得<code>VDN</code>这样做不至于，并且会导致很多复杂的函数无法很好地拟合出来，因此<code>QMIX</code>提出了更一般的<strong>“单调性条件<code>(Monotonicity)</code>”</strong>：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182308208.png" alt=""></p>
<p><code>QMIX</code>认为只要满足<code>(3)</code>式，就能满足<code>IGM</code>条件。<code>(2)</code>式是<code>(3)</code>式的充分条件，但是同时，<code>(3)</code>式也是<code>(1)</code>式的充分条件。换句话说，<strong>它们对<code>IGM</code>条件来说，都不是必要的！</strong>因此，针对一些具有非单调收益的合作问题，<code>VDN</code>和<code>QMIX</code>的函数拟合能力就会受到限制。</p>
<p>因此，基于这样的问题，作者提出<code>QTRAN</code>算法，并且声称该算法能够分解任何可分解的任务，而不需要受<code>(2)</code>式和<code>(3)</code>式的约束。</p>
<h2 id="QTRAN直观理解"><a href="#QTRAN直观理解" class="headerlink" title="QTRAN直观理解"></a><code>QTRAN</code>直观理解</h2><p>在<code>VDN</code>和<code>QMIX</code>中是将$Q_{\mathrm{jt}}$通过累加求和和保证单调性的方式来分解的，作者这里提出一种更加鲁棒的分解方式，将原始的$Q_{\mathrm{jt}}$映射成$Q_{\mathrm{jt}}^{\prime}$，通过$Q_{\mathrm{jt}}^{\prime}$去分解值函数到各个子智能体上，来保证学到的$Q_{\mathrm{jt}}^{\prime}$与真实的动作值函数$Q^{*}$非常接近。这样在学习真实的动作值函数的时候，没有像<code>VDN</code>和<code>QMIX</code>那样对其加上一些累加求和和保证单调性的限制，所以它能学地更好。</p>
<p>但是由于部分可观测地限制，这个$Q_{\mathrm{jt}}^{\prime}$是没有办法用来进行具体地决策的，所以我们需要去找到$Q_{\mathrm{jt}}$、$Q_{\mathrm{jt}}^{\prime}$和$\left[Q_{i}\right]$三者之间的关系。</p>
<h2 id="可分解值函数的充分条件"><a href="#可分解值函数的充分条件" class="headerlink" title="可分解值函数的充分条件"></a>可分解值函数的充分条件</h2><p>由于不提供累加求和和单调性来保证可分解，<code>QTRAN</code>提出了一个满足<code>IGM</code>定义的充分条件：当动作值函数$Q_{\mathrm{jt}}(\boldsymbol{\tau}, \boldsymbol{u})$和$\left[Q_{i}\left(\tau_{i}, u_{i}\right)\right]$满足下面这个关系式时，我们认为它是可分解的，其中$V_{\mathrm{jt}}(\tau)=\max _{\boldsymbol{u}} Q_{\mathrm{jt}}(\boldsymbol{\tau}, \boldsymbol{u})-\sum_{i=1}^{N} Q_{i}\left(\tau_{i}, \bar{u}_{i}\right)$：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182329900.png" alt=""></p>
<p>想要通过上述条件证明出满足<code>IGM</code>条件，也就是$\arg \max _{\boldsymbol{u}} Q_{\mathrm{jt}}(\boldsymbol{\tau}, \boldsymbol{u})=\overline{\boldsymbol{u}}$时（其中$\bar{u}_{i}=\arg \max _{u_{i}} Q_{i}\left(\tau_{i}, u_{i}\right)$）才能取到全局最大的动作值函数。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182341968.png" alt=""></p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411182351374.png" alt=""></p>
<h2 id="可分解值函数的必要条件"><a href="#可分解值函数的必要条件" class="headerlink" title="可分解值函数的必要条件"></a>可分解值函数的必要条件</h2><p>必要条件说的是，当满足<code>IGM</code>条件时，能够把充分条件给推出来。作者在论文中说到，存在一个仿射函数$\phi(\boldsymbol{Q})=A \cdot \boldsymbol{Q}+B$对$Q$进行一个映射，其中$\left[a_{i i}\right] \in \mathbb{R}_{+}^{N \times N}$为一个对角矩阵，并且$a_{i i}&gt;0$。相较<code>QMIX</code>还多一个$B$，$B=\left[b_{i}\right] \in \mathbb{R}^{N}$。作者将$Q_{i}$进行了一个缩放，$a_{i i} Q_{i}+b_{i}$。然后定义：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411190001341.png" alt=""></p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411190007735.png" alt=""></p>
<p>网络结构中主要有三部分：</p>
<ul>
<li>独立的动作值网络：$f_{\mathrm{q}}:\left(\tau_{i}, u_{i}\right) \mapsto Q_{i}$。对于每个动作值网络，输入是他自己的动作观测历史$\tau_{i}$，输出动作值函数$Q_{i}\left(\tau_{i}, \cdot\right)$。$Q_{\mathrm{jt}}^{\prime}$由各个子智能体的动作值函数累加得到。</li>
<li>联合动作值网络：$f_{\mathrm{r}}:(\tau, \boldsymbol{u}) \mapsto Q_{\mathrm{jt}}$。如上图所示，网络的前面几层参数是共享的，用所有的独立智能体的动作值函数向量来采样样本，更新联合动作值函数。</li>
<li>状态值网络：$f_{\mathrm{v}}: \tau \mapsto V_{\mathrm{jt}}$。状态值函数类似<code>dueling</code>网络，并且这里可以引入全局的状态信息，它是独立于动作轨迹的，但是可以用来辅助动作值函数的训练。</li>
</ul>
<p>此时损失函数可以表达成如下形式：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411190011682.png" alt=""></p>
<p>第一个$L_{\mathrm{td}}$可以无忧无虑地去近似最优的联合动作值函数，下面两个就是用来去满足可分解值函数地充分必要条件的，$Q^{\prime}_{jt}$是真实的，$Q_{jt}$是网络学习到的，用$V$去弥补两者之间的差异。</p>
<h1 id="COMA"><a href="#COMA" class="headerlink" title="COMA"></a><code>COMA</code></h1><p><code>COMA</code>是一种<code>policy-based</code>的多智能体算法，其核心思想是在于“独立回报分配”。当多个智能体在执行任务时，获得的奖励分数是由所有智能体的行为共同决定的，那么怎么把一个奖励分数合理的分配给每一个智能体呢？这就是<code>COMA</code>算法要解决的关键问题。在网络结构上，<code>COMA</code>沿用了<code>Actor-Critic</code>架构，其中<code>Actor</code>使用基于<code>policy-based</code>的<code>RNN</code>网络。</p>
<h2 id="算法背景和思想"><a href="#算法背景和思想" class="headerlink" title="算法背景和思想"></a>算法背景和思想</h2><p>在多智能体强化学习场景中，每个智能体在某一时刻只掌握局部的信息，无法全局观测环境状态。为了促进合作，各个智能体的动作对全局奖励有不同的贡献，因此需要一种有效的方法来分配奖励。<code>COMA</code>引入了“反事实基线”<code>(Counterfactual Baseline)</code>的概念，专门用于降低多智能体策略梯度方法中的方差。</p>
<p><code>COMA</code>的核心思想是通过引入一个基线，该基线模拟在固定其他智能体动作的前提下，某个智能体选择不同动作时对全局奖励的影响，从而更精确地衡量当前动作的贡献，减少策略梯度更新中的方差。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192146348.png" alt=""></p>
<h2 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h2><h3 id="全局策略梯度"><a href="#全局策略梯度" class="headerlink" title="全局策略梯度"></a>全局策略梯度</h3><p>对于多智能体问题，每个智能体$(i)$的策略梯度可以表示为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192159569.png" alt=""></p>
<p>其中，$(\pi_{\theta_{i}}(a_{i} \vert s))$是智能体$(i)$在状态$(s)$下选择动作$(a_{i})$的概率，$(Q_{i}(s,a))$是该智能体在执行动作$(a_{i})$时的动作价值函数。</p>
<h3 id="反事实基线"><a href="#反事实基线" class="headerlink" title="反事实基线"></a>反事实基线</h3><p>为了减小方差，<code>COMA</code>提出了反事实基线$(b(s,a_{-i}))$，该基线衡量在保持其他智能体动作$(a_{-i})$不变的情况下，智能体$(i)$选择其他动作时的期望收益。具体公式为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192206736.png" alt=""></p>
<p>这里$(a_{-i})$的表示除智能体$(i)$之外，其他智能体的动作组合。</p>
<h3 id="策略更新"><a href="#策略更新" class="headerlink" title="策略更新"></a>策略更新</h3><p>有了反事实基线之后，<code>COMA</code>中智能体$(i)$的策略更新公式变为：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411192208564.png" alt=""></p>
<p>其中，$(Q_{i}(s,a))$是在当前状态下，所有智能体执行一组动作$(a)$时的全局奖励，而$(b(s,a_{-i}))$是该动作的反事实基线。</p>
<h3 id="全局值函数"><a href="#全局值函数" class="headerlink" title="全局值函数"></a>全局值函数</h3><p><code>COMA</code>中的值函数$(Q(s,a))$和基线$(b(s,a_{-i}))$都是通过集中化的学习进行优化的，虽然决策是去中心化的，但值函数和基线都依赖于全局的状态和动作信息。</p>
<h2 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h2><ol>
<li>初始化智能体策略和集中式的全局值函数。</li>
<li>智能体与环境交互，收集经验数据。</li>
<li>使用经验数据更新全局值函数$(Q(s,a))$。</li>
<li>计算反事实基线$(b(s,a_{-i}))$。</li>
<li>计算每个智能体的策略梯度，并更新策略参数。</li>
<li>重复上述过程，直至智能体策略收敛。</li>
</ol>
<h1 id="MADDPG"><a href="#MADDPG" class="headerlink" title="MADDPG"></a><code>MADDPG</code></h1><h2 id="背景与动机"><a href="#背景与动机" class="headerlink" title="背景与动机"></a>背景与动机</h2><p> 在多智能体系统中，多个智能体同时作用于同一个环境，相互之间可能是竞争的、协作的，或者二者兼有。这类环境下，单智能体算法如<code>DDPG</code>往往无法取得较好的效果，因为每个智能体的行为都会影响其他智能体的状态和奖励。为了解决这一问题，<code>MADDPG</code>采用了一种集中式训练，分布式执行的架构。</p>
<ul>
<li>集中式训练：训练期间，每个智能体可以观察所有其他智能体的动作和状态，从而学到更有效的策略。</li>
<li>分布式执行：在执行阶段，智能体只依赖其自身的观测来做出决策，保持分布式控制的特性。</li>
</ul>
<h2 id="算法细节"><a href="#算法细节" class="headerlink" title="算法细节"></a>算法细节</h2><ul>
<li><code>Actor-Critic</code>架构：每个智能体都有一个<code>Actor</code>网络用于输出动作，以及一个<code>Critic</code>网络用于评估当前策略的好坏。<code>Actor</code>直接学习确定性策略，而<code>Critic</code>负责估算状态-动作对的$Q$值。</li>
<li>集中式训练，分布式执行：在训练阶段，Critic网络可以访问所有智能体的信息，包括状态和动作，这允许它准确评估每个动作的期望回报。然而，在执行阶段，每个智能体的<code>Actor</code>网络只能基于自己的局部观察来做出决策。</li>
<li>经验回放：为了提高训练的稳定性和效率，<code>MADDPG</code>使用了经验回放机制。智能体的每次交互会被存储在一个回放缓冲区中，训练时会从这个缓冲区中随机抽取一批经验来更新网络。</li>
<li>目标网络：为了进一步稳定训练过程，<code>MADDPG</code>为每个<code>Actor</code>和<code>Critic</code>网络维护了一个目标网络。这些目标网络的参数会缓慢跟踪对应网络的参数，用于计算期望回报的稳定目标。</li>
<li>奖励和惩罚：<code>MADDPG</code>允许设计复杂的奖励机制，包括对合作行为的奖励和对对立行为的惩罚，来引导智能体学习如何在多种交互场景中作出最优决策。</li>
</ul>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411202322720.png" alt=""></p>
<h3 id="中心化训练"><a href="#中心化训练" class="headerlink" title="中心化训练"></a>中心化训练</h3><p>与<code>DDPG</code>不同之处在于，<code>MADDPG</code>中的每个智能体在计算自身的<code>Critic</code>的前向传播时，把包括自身在内的所有智能体的观测拼接成观测向量$s=\{s_{1},s_{2},\cdots,s_{n}\}$，把所有智能体动作拼接成动作向量$\{a_{1},a_{2},\cdots,a_{n}\}$，$(s,a)$作为<code>Online Critic Net</code>的输入，输出一维的$Q$值，即$Q_{\phi_{i}}(s,a)$，换句话说就是利用环境中智能体的全局信息（全局观测和动作）来“中心化”训练自身的<code>Critic Net</code>。现在，有了$Q_{\phi_{i}}(s,a)$，同时有了回放样本可以计算出$Q_{\phi_{i}}(s^{\prime},a^{\prime})$，接下来要做的和<code>DDPG</code>相同，以时序差分误差构建二者的<code>MSE</code>损失函数，然后利用梯度下降更新参数$\phi_{i}$。具体的损失函数及梯度如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411202312830.png" alt=""></p>
<p>其中，$\boldsymbol{a}^{\prime}=\{\mu_{\tilde{\theta}_{1}}(s_{1}^{\prime}),\mu_{\tilde{\theta}_{1}}(s_{2}^{\prime}),\ldots,\mu_{\tilde{\theta}_{1}}(s_{N}^{\prime})\}$这里用<code>Target Policy Net</code>计算下一状态智能体所采取的动作。需要注意，每个智能体的<code>Target Policy Net</code>输入是自身局部观测$s_{i}$。</p>
<h3 id="分布式执行"><a href="#分布式执行" class="headerlink" title="分布式执行"></a>分布式执行</h3><p>在计算自身的<code>Actor</code>的前向传播时，每个智能体只将自身的局部观测向量$s=\{s_{1},s_{2},\ldots,s_{n}\}$作为<code>Online Actor Net</code>的输入，输出一个确定性动作$a_{i}$，即$\mu_{\theta_{i}}(s_{i})$。接下来和<code>DDPG</code>一样，计算时序差分误差的<code>MSE</code>损失函数并计算关于参数$\theta_{i}$的梯度，然后利用梯度下降更新参数。损失函数及梯度如下：</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411202320613.png" alt=""></p>
<h1 id="MAPPO"><a href="#MAPPO" class="headerlink" title="MAPPO"></a><code>MAPPO</code></h1><h2 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h2><p>和单智能体<code>PPO</code>算法一样，<code>MAPPO</code>算法中每个智能体都有各自的<code>Actor</code>网络和<code>Critic</code>网络（如果所有智能体的状态空间和动作空间也相同，即同构，也可以所有智能体共享一套<code>Actor</code>和<code>Critic</code>网络）。与单智能体<code>PPO</code>不同的是，<code>MAPPO</code>的<code>Critic</code>网络可以接收有关全局状态的信息，这个全局状态可以是由所有智能体的观察拼接而成，也可以是环境直接提供。</p>
<p><img src="https://my-pic-storage-1305445540.cos.ap-nanjing.myqcloud.com/202411171817653.png" alt=""></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>和单智能体<code>PPO</code>算法一样，损失函数由<code>Actor Loss</code>和<code>Critic Loss</code>组成：</p>
<ul>
<li><code>Actor Loss</code>为最小化负的智能体在当前策略下的预期累积奖励$-E[\frac{\pi(a|S_t;\theta)}{\pi(a|S_t;\theta_k)} A_t]$</li>
<li><code>Critic Loss</code>为回报和状态价值函数的均方差$[(G_t-V(s,w))]^{2}$</li>
</ul>
<h2 id="采样和更新方式"><a href="#采样和更新方式" class="headerlink" title="采样和更新方式"></a>采样和更新方式</h2><h3 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h3><p>如智能体间不共享参数，即每个智能体有各自的<code>Actor</code>和<code>Critic</code>网络，则给每个智能体建立一个<code>replay buffer</code>，将该智能体交互中获得的$s_{t}, a_{t}, r，s_{t+1}$存入对应的<code>replay buffer</code>中。另在<code>replay buffer</code>中增加<code>mask</code>，记录每一时刻智能体是否存活，以便后续死亡的智能体后续数据不用于更新网络。一般情况下，不同智能体间不共享奖励。</p>
<h3 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h3><p>如果智能体间不共享参数，则针对每一个智能体分别从<code>replay buffer</code>中抽样，训练其网络，其更新函数与<code>PPO</code>更新函数整体一致，并且增加了<code>GAE</code>、<code>value normlization</code>等技巧。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%80%BB%E7%BB%93/" rel="tag"># 总结</a>
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># 强化学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/11/03/CloseAirCombat%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/" rel="prev" title="CloseAirCombat使用总结">
                  <i class="fa fa-chevron-left"></i> CloseAirCombat使用总结
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/11/29/ROS/" rel="next" title="ROS">
                  ROS <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">木霈玖</span>
</div>

    </div>
  </footer>

  
  <script src="//unpkg.com/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//unpkg.com/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
